{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368d0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d144fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca16b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432dd875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ecaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "### 五张图片一组\n",
    "step=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2333b0c4",
   "metadata": {},
   "source": [
    "### read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9a0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "##path = r'C:\\Users\\liuya\\Downloads\\3d_printing_research\\clipped_samples'\n",
    "path = r'C:\\Users\\liuya\\Downloads\\3d_printing_research\\clipped_samples_otsu'\n",
    "image_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b383cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 730, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_path = os.path.join(path, image_list[350])\n",
    "np.array(Image.open(full_path).convert('RGB')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f889b4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c5002e95094d0588490dd8eab64fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_ls = []\n",
    "\n",
    "for i in tqdm(image_list):\n",
    "    full_path = os.path.join(path, i)\n",
    "    img = Image.open(full_path).convert('L')\n",
    "    img_array = np.asarray(img)\n",
    "    image_ls.append(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27a4cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalize\n",
    "data_input = np.array(image_ls)\n",
    "### np.divide((data_input-np.min(data_input)), (np.max(data_input)-np.min(data_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2cd6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = np.divide((data_input-np.min(data_input)), (np.max(data_input)-np.min(data_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f7ce9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = torch.tensor(data_input[:,None,:,:], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c2c2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4046, 1, 250, 730])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4732dc",
   "metadata": {},
   "source": [
    "### build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13885208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_NN_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d1 = nn.Conv2d(1, 16, kernel_size=(6,6))\n",
    "        self.maxpool2d1 = nn.MaxPool2d((3,3), return_indices=True)\n",
    "        self.conv2d2 = nn.Conv2d(16, 8, kernel_size=(6,6))\n",
    "        self.maxpool2d2 = nn.MaxPool2d((3,3), return_indices=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(in_features=15600, out_features=2560)\n",
    "        self.linear2 = nn.Linear(in_features=2560, out_features=128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        ## n_tau = x.shape[2]\n",
    "        ## 1 means 1 channel, multiply sample size with tau to create this many of 1*W vectors\n",
    "        ## x = x.view((-1, 1, x.shape[3], x.shape[4]))\n",
    "        output = F.relu(self.conv2d1(x))\n",
    "        output, indices1 = self.maxpool2d1(output)\n",
    "        output = F.relu(self.conv2d2(output))\n",
    "        output, indices2 = self.maxpool2d2(output)\n",
    "        output = self.flatten(output)\n",
    "        output = F.relu(self.linear1(output))\n",
    "        output = self.linear2(output)\n",
    "        #back to the previous dimension (sample_size, tau)\n",
    "        #output = output.view((batch_size, n_tau, -1))\n",
    "        return output, indices1, indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b19a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_NN_decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(128, 2560)\n",
    "        self.linear2 = nn.Linear(2560, 15600)\n",
    "        self.maxunpool2d1 = nn.MaxUnpool2d((3,3))\n",
    "        ## self.conv2d1 = nn.Conv2d(8, 16, kernel_size=(6,6), padding=(5,5))\n",
    "        self.conv2d1 = nn.ConvTranspose2d(8, 16, kernel_size=(6,6))\n",
    "        self.maxunpool2d2 = nn.MaxUnpool2d((3,3))\n",
    "        ## self.conv2d2 = nn.Conv2d(16, 1, kernel_size=(6,6), padding=(5,5))\n",
    "        self.conv2d2 = nn.ConvTranspose2d(16, 1, kernel_size=(6,6))\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        #self.pad = nn.ConstantPad2d((9,9,58,0), 0)\n",
    "\n",
    "        \n",
    "    def forward(self, x, indices1, indices2):\n",
    "        ## x = x.contiguous().view((-1, x.shape[2]))\n",
    "        output = F.relu(self.linear1(x))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = output.view((output.shape[0], 8, 25, 78))\n",
    "        output = self.maxunpool2d1(output, indices2, output_size=torch.Size([output.shape[0], 8, 76, 236]))\n",
    "        output = F.relu(self.conv2d1(output))\n",
    "        output = self.maxunpool2d2(output, indices1, output_size=torch.Size([output.shape[0], 16, 245, 725]))\n",
    "        output = self.conv2d2(output)\n",
    "        output = self.sigmoid1(output)\n",
    "        #output = self.pad(output)\n",
    "        #back to original size\n",
    "        #output = output.view((-1, 1, 5, output.shape[2], output.shape[3]))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b64b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.Conv_NN_encoder = args[0]\n",
    "        self.Conv_NN_decoder = args[1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, indc1, indc2 = self.Conv_NN_encoder(x)\n",
    "        output = self.Conv_NN_decoder(output, indc1, indc2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8df0e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net(Conv_NN_encoder(), Conv_NN_decoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0c13b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "net                                      --                        --\n",
       "├─Conv_NN_encoder: 1-1                   [10, 128]                 --\n",
       "│    └─Conv2d: 2-1                       [10, 16, 245, 725]        592\n",
       "│    └─MaxPool2d: 2-2                    [10, 16, 81, 241]         --\n",
       "│    └─Conv2d: 2-3                       [10, 8, 76, 236]          4,616\n",
       "│    └─MaxPool2d: 2-4                    [10, 8, 25, 78]           --\n",
       "│    └─Flatten: 2-5                      [10, 15600]               --\n",
       "│    └─Linear: 2-6                       [10, 2560]                39,938,560\n",
       "│    └─Linear: 2-7                       [10, 128]                 327,808\n",
       "├─Conv_NN_decoder: 1-2                   [10, 1, 250, 730]         --\n",
       "│    └─Linear: 2-8                       [10, 2560]                330,240\n",
       "│    └─Linear: 2-9                       [10, 15600]               39,951,600\n",
       "│    └─MaxUnpool2d: 2-10                 [10, 8, 76, 236]          --\n",
       "│    └─ConvTranspose2d: 2-11             [10, 16, 81, 241]         4,624\n",
       "│    └─MaxUnpool2d: 2-12                 [10, 16, 245, 725]        --\n",
       "│    └─ConvTranspose2d: 2-13             [10, 1, 250, 730]         577\n",
       "│    └─Sigmoid: 2-14                     [10, 1, 250, 730]         --\n",
       "==========================================================================================\n",
       "Total params: 80,558,617\n",
       "Trainable params: 80,558,617\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.64\n",
       "==========================================================================================\n",
       "Input size (MB): 7.30\n",
       "Forward/backward pass size (MB): 280.09\n",
       "Params size (MB): 322.23\n",
       "Estimated Total Size (MB): 609.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=(10,1,250,730))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92844adb",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f37de440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \n",
    "    model.train() #trian model\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        ##calculate loss\n",
    "        loss = 0\n",
    "        for i in range(data.shape[0]):\n",
    "            loss += F.mse_loss(output[i], data[i], reduction='sum')\n",
    "        loss /= data.shape[0]\n",
    "        #loss = F.mse_loss(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print result every 10 batch\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} ... Batch: {} ... Loss: {:.8f}'.format(epoch, batch_idx, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d2f1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval() #evaluate model\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            #calculate sum loss\n",
    "            test_loss += F.mse_loss(output, data, reduction='sum').item()\n",
    "    \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('------------------- Test set: Average loss: {:.4f} ... Samples: {}'.format(test_loss, len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338dba7",
   "metadata": {},
   "source": [
    "### train and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "562122f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_, val_window_ = train_test_split(data_input, test_size=0.2, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf5998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_window_, batch_size=16,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(val_window_, batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdeb1bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04dc5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2fe4058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6101d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "091cdcd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 ... Batch: 0 ... Loss: 1753.03442383\n",
      "Train Epoch: 1 ... Batch: 10 ... Loss: 1667.08435059\n",
      "Train Epoch: 1 ... Batch: 20 ... Loss: 1728.84570312\n",
      "Train Epoch: 1 ... Batch: 30 ... Loss: 1686.25573730\n",
      "Train Epoch: 1 ... Batch: 40 ... Loss: 1634.41589355\n",
      "Train Epoch: 1 ... Batch: 50 ... Loss: 1754.08618164\n",
      "Train Epoch: 1 ... Batch: 60 ... Loss: 1765.34667969\n",
      "Train Epoch: 1 ... Batch: 70 ... Loss: 1723.13916016\n",
      "Train Epoch: 1 ... Batch: 80 ... Loss: 1702.73107910\n",
      "Train Epoch: 1 ... Batch: 90 ... Loss: 1709.17993164\n",
      "Train Epoch: 1 ... Batch: 100 ... Loss: 1645.11547852\n",
      "Train Epoch: 1 ... Batch: 110 ... Loss: 1756.88720703\n",
      "Train Epoch: 1 ... Batch: 120 ... Loss: 1701.55493164\n",
      "Train Epoch: 1 ... Batch: 130 ... Loss: 1722.98291016\n",
      "Train Epoch: 1 ... Batch: 140 ... Loss: 1725.19555664\n",
      "Train Epoch: 1 ... Batch: 150 ... Loss: 1619.71154785\n",
      "Train Epoch: 1 ... Batch: 160 ... Loss: 1717.83093262\n",
      "Train Epoch: 1 ... Batch: 170 ... Loss: 1667.53735352\n",
      "Train Epoch: 1 ... Batch: 180 ... Loss: 1684.76049805\n",
      "Train Epoch: 1 ... Batch: 190 ... Loss: 1702.24255371\n",
      "Train Epoch: 1 ... Batch: 200 ... Loss: 1694.39147949\n",
      "------------------- Test set: Average loss: 1715.3875 ... Samples: 810\n",
      "Train Epoch: 2 ... Batch: 0 ... Loss: 1636.54797363\n",
      "Train Epoch: 2 ... Batch: 10 ... Loss: 1663.78649902\n",
      "Train Epoch: 2 ... Batch: 20 ... Loss: 1713.44860840\n",
      "Train Epoch: 2 ... Batch: 30 ... Loss: 1692.44335938\n",
      "Train Epoch: 2 ... Batch: 40 ... Loss: 1674.91223145\n",
      "Train Epoch: 2 ... Batch: 50 ... Loss: 1749.75708008\n",
      "Train Epoch: 2 ... Batch: 60 ... Loss: 1758.57824707\n",
      "Train Epoch: 2 ... Batch: 70 ... Loss: 1718.15063477\n",
      "Train Epoch: 2 ... Batch: 80 ... Loss: 1702.24719238\n",
      "Train Epoch: 2 ... Batch: 90 ... Loss: 1613.81408691\n",
      "Train Epoch: 2 ... Batch: 100 ... Loss: 1672.01623535\n",
      "Train Epoch: 2 ... Batch: 110 ... Loss: 1725.22534180\n",
      "Train Epoch: 2 ... Batch: 120 ... Loss: 1729.58447266\n",
      "Train Epoch: 2 ... Batch: 130 ... Loss: 1682.59997559\n",
      "Train Epoch: 2 ... Batch: 140 ... Loss: 1706.69360352\n",
      "Train Epoch: 2 ... Batch: 150 ... Loss: 1641.54907227\n",
      "Train Epoch: 2 ... Batch: 160 ... Loss: 1701.70532227\n",
      "Train Epoch: 2 ... Batch: 170 ... Loss: 1681.52233887\n",
      "Train Epoch: 2 ... Batch: 180 ... Loss: 1721.68884277\n",
      "Train Epoch: 2 ... Batch: 190 ... Loss: 1767.30139160\n",
      "Train Epoch: 2 ... Batch: 200 ... Loss: 1709.79980469\n",
      "------------------- Test set: Average loss: 1715.4542 ... Samples: 810\n",
      "Train Epoch: 3 ... Batch: 0 ... Loss: 1779.24548340\n",
      "Train Epoch: 3 ... Batch: 10 ... Loss: 1757.96557617\n",
      "Train Epoch: 3 ... Batch: 20 ... Loss: 1732.80102539\n",
      "Train Epoch: 3 ... Batch: 30 ... Loss: 1670.02587891\n",
      "Train Epoch: 3 ... Batch: 40 ... Loss: 1676.81408691\n",
      "Train Epoch: 3 ... Batch: 50 ... Loss: 1697.37646484\n",
      "Train Epoch: 3 ... Batch: 60 ... Loss: 1687.53540039\n",
      "Train Epoch: 3 ... Batch: 70 ... Loss: 1678.76232910\n",
      "Train Epoch: 3 ... Batch: 80 ... Loss: 1738.07812500\n",
      "Train Epoch: 3 ... Batch: 90 ... Loss: 1682.29870605\n",
      "Train Epoch: 3 ... Batch: 100 ... Loss: 1711.21423340\n",
      "Train Epoch: 3 ... Batch: 110 ... Loss: 1702.01428223\n",
      "Train Epoch: 3 ... Batch: 120 ... Loss: 1649.84082031\n",
      "Train Epoch: 3 ... Batch: 130 ... Loss: 1745.23388672\n",
      "Train Epoch: 3 ... Batch: 140 ... Loss: 1700.18701172\n",
      "Train Epoch: 3 ... Batch: 150 ... Loss: 1643.10217285\n",
      "Train Epoch: 3 ... Batch: 160 ... Loss: 1639.32946777\n",
      "Train Epoch: 3 ... Batch: 170 ... Loss: 1658.02600098\n",
      "Train Epoch: 3 ... Batch: 180 ... Loss: 1632.66772461\n",
      "Train Epoch: 3 ... Batch: 190 ... Loss: 1692.38647461\n",
      "Train Epoch: 3 ... Batch: 200 ... Loss: 1684.19519043\n",
      "------------------- Test set: Average loss: 1715.4625 ... Samples: 810\n",
      "Train Epoch: 4 ... Batch: 0 ... Loss: 1682.81372070\n",
      "Train Epoch: 4 ... Batch: 10 ... Loss: 1784.72534180\n",
      "Train Epoch: 4 ... Batch: 20 ... Loss: 1621.56396484\n",
      "Train Epoch: 4 ... Batch: 30 ... Loss: 1640.75366211\n",
      "Train Epoch: 4 ... Batch: 40 ... Loss: 1754.65478516\n",
      "Train Epoch: 4 ... Batch: 50 ... Loss: 1766.10266113\n",
      "Train Epoch: 4 ... Batch: 60 ... Loss: 1708.67016602\n",
      "Train Epoch: 4 ... Batch: 70 ... Loss: 1641.61425781\n",
      "Train Epoch: 4 ... Batch: 80 ... Loss: 1672.44189453\n",
      "Train Epoch: 4 ... Batch: 90 ... Loss: 1700.19238281\n",
      "Train Epoch: 4 ... Batch: 100 ... Loss: 1687.47558594\n",
      "Train Epoch: 4 ... Batch: 110 ... Loss: 1767.59655762\n",
      "Train Epoch: 4 ... Batch: 120 ... Loss: 1723.41882324\n",
      "Train Epoch: 4 ... Batch: 130 ... Loss: 1702.08862305\n",
      "Train Epoch: 4 ... Batch: 140 ... Loss: 1755.17883301\n",
      "Train Epoch: 4 ... Batch: 150 ... Loss: 1668.84741211\n",
      "Train Epoch: 4 ... Batch: 160 ... Loss: 1738.64025879\n",
      "Train Epoch: 4 ... Batch: 170 ... Loss: 1808.51013184\n",
      "Train Epoch: 4 ... Batch: 180 ... Loss: 1750.74023438\n",
      "Train Epoch: 4 ... Batch: 190 ... Loss: 1798.29504395\n",
      "Train Epoch: 4 ... Batch: 200 ... Loss: 1708.22705078\n",
      "------------------- Test set: Average loss: 1715.8471 ... Samples: 810\n",
      "Train Epoch: 5 ... Batch: 0 ... Loss: 1698.54602051\n",
      "Train Epoch: 5 ... Batch: 10 ... Loss: 1762.12316895\n",
      "Train Epoch: 5 ... Batch: 20 ... Loss: 1655.48425293\n",
      "Train Epoch: 5 ... Batch: 30 ... Loss: 1650.42834473\n",
      "Train Epoch: 5 ... Batch: 40 ... Loss: 1681.22216797\n",
      "Train Epoch: 5 ... Batch: 50 ... Loss: 1688.16137695\n",
      "Train Epoch: 5 ... Batch: 60 ... Loss: 1683.71557617\n",
      "Train Epoch: 5 ... Batch: 70 ... Loss: 1649.55261230\n",
      "Train Epoch: 5 ... Batch: 80 ... Loss: 1797.58593750\n",
      "Train Epoch: 5 ... Batch: 90 ... Loss: 1698.16003418\n",
      "Train Epoch: 5 ... Batch: 100 ... Loss: 1665.04772949\n",
      "Train Epoch: 5 ... Batch: 110 ... Loss: 1709.85644531\n",
      "Train Epoch: 5 ... Batch: 120 ... Loss: 1753.56140137\n",
      "Train Epoch: 5 ... Batch: 130 ... Loss: 1752.16259766\n",
      "Train Epoch: 5 ... Batch: 140 ... Loss: 1773.43762207\n",
      "Train Epoch: 5 ... Batch: 150 ... Loss: 1731.49853516\n",
      "Train Epoch: 5 ... Batch: 160 ... Loss: 1663.85913086\n",
      "Train Epoch: 5 ... Batch: 170 ... Loss: 1749.87451172\n",
      "Train Epoch: 5 ... Batch: 180 ... Loss: 1687.07385254\n",
      "Train Epoch: 5 ... Batch: 190 ... Loss: 1694.05419922\n",
      "Train Epoch: 5 ... Batch: 200 ... Loss: 1596.27209473\n",
      "------------------- Test set: Average loss: 1715.9212 ... Samples: 810\n",
      "Train Epoch: 6 ... Batch: 0 ... Loss: 1783.97802734\n",
      "Train Epoch: 6 ... Batch: 10 ... Loss: 1723.35266113\n",
      "Train Epoch: 6 ... Batch: 20 ... Loss: 1707.45141602\n",
      "Train Epoch: 6 ... Batch: 30 ... Loss: 1756.43652344\n",
      "Train Epoch: 6 ... Batch: 40 ... Loss: 1688.30798340\n",
      "Train Epoch: 6 ... Batch: 50 ... Loss: 1642.67333984\n",
      "Train Epoch: 6 ... Batch: 60 ... Loss: 1727.21154785\n",
      "Train Epoch: 6 ... Batch: 70 ... Loss: 1710.40234375\n",
      "Train Epoch: 6 ... Batch: 80 ... Loss: 1700.14990234\n",
      "Train Epoch: 6 ... Batch: 90 ... Loss: 1706.88012695\n",
      "Train Epoch: 6 ... Batch: 100 ... Loss: 1741.70898438\n",
      "Train Epoch: 6 ... Batch: 110 ... Loss: 1697.81616211\n",
      "Train Epoch: 6 ... Batch: 120 ... Loss: 1671.23852539\n",
      "Train Epoch: 6 ... Batch: 130 ... Loss: 1671.87084961\n",
      "Train Epoch: 6 ... Batch: 140 ... Loss: 1650.15625000\n",
      "Train Epoch: 6 ... Batch: 150 ... Loss: 1748.97753906\n",
      "Train Epoch: 6 ... Batch: 160 ... Loss: 1696.84057617\n",
      "Train Epoch: 6 ... Batch: 170 ... Loss: 1730.66650391\n",
      "Train Epoch: 6 ... Batch: 180 ... Loss: 1702.04772949\n",
      "Train Epoch: 6 ... Batch: 190 ... Loss: 1716.87145996\n",
      "Train Epoch: 6 ... Batch: 200 ... Loss: 1655.64367676\n",
      "------------------- Test set: Average loss: 1715.7400 ... Samples: 810\n",
      "Train Epoch: 7 ... Batch: 0 ... Loss: 1721.55273438\n",
      "Train Epoch: 7 ... Batch: 10 ... Loss: 1679.18591309\n",
      "Train Epoch: 7 ... Batch: 20 ... Loss: 1669.88940430\n",
      "Train Epoch: 7 ... Batch: 30 ... Loss: 1644.68530273\n",
      "Train Epoch: 7 ... Batch: 40 ... Loss: 1749.79174805\n",
      "Train Epoch: 7 ... Batch: 50 ... Loss: 1693.82836914\n",
      "Train Epoch: 7 ... Batch: 60 ... Loss: 1820.03479004\n",
      "Train Epoch: 7 ... Batch: 70 ... Loss: 1717.56494141\n",
      "Train Epoch: 7 ... Batch: 80 ... Loss: 1639.52526855\n",
      "Train Epoch: 7 ... Batch: 90 ... Loss: 1652.55065918\n",
      "Train Epoch: 7 ... Batch: 100 ... Loss: 1678.22717285\n",
      "Train Epoch: 7 ... Batch: 110 ... Loss: 1675.50683594\n",
      "Train Epoch: 7 ... Batch: 120 ... Loss: 1699.68200684\n",
      "Train Epoch: 7 ... Batch: 130 ... Loss: 1656.48107910\n",
      "Train Epoch: 7 ... Batch: 140 ... Loss: 1708.64587402\n",
      "Train Epoch: 7 ... Batch: 150 ... Loss: 1696.24060059\n",
      "Train Epoch: 7 ... Batch: 160 ... Loss: 1723.48974609\n",
      "Train Epoch: 7 ... Batch: 170 ... Loss: 1704.77368164\n",
      "Train Epoch: 7 ... Batch: 180 ... Loss: 1650.96057129\n",
      "Train Epoch: 7 ... Batch: 190 ... Loss: 1704.74658203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 ... Batch: 200 ... Loss: 1721.70812988\n",
      "------------------- Test set: Average loss: 1716.5343 ... Samples: 810\n",
      "Train Epoch: 8 ... Batch: 0 ... Loss: 1759.33081055\n",
      "Train Epoch: 8 ... Batch: 10 ... Loss: 1644.55676270\n",
      "Train Epoch: 8 ... Batch: 20 ... Loss: 1696.66088867\n",
      "Train Epoch: 8 ... Batch: 30 ... Loss: 1741.96325684\n",
      "Train Epoch: 8 ... Batch: 40 ... Loss: 1734.99584961\n",
      "Train Epoch: 8 ... Batch: 50 ... Loss: 1736.33813477\n",
      "Train Epoch: 8 ... Batch: 60 ... Loss: 1708.69873047\n",
      "Train Epoch: 8 ... Batch: 70 ... Loss: 1710.64221191\n",
      "Train Epoch: 8 ... Batch: 80 ... Loss: 1665.86389160\n",
      "Train Epoch: 8 ... Batch: 90 ... Loss: 1747.97253418\n",
      "Train Epoch: 8 ... Batch: 100 ... Loss: 1727.11022949\n",
      "Train Epoch: 8 ... Batch: 110 ... Loss: 1699.62243652\n",
      "Train Epoch: 8 ... Batch: 120 ... Loss: 1691.85229492\n",
      "Train Epoch: 8 ... Batch: 130 ... Loss: 1700.88415527\n",
      "Train Epoch: 8 ... Batch: 140 ... Loss: 1713.51391602\n",
      "Train Epoch: 8 ... Batch: 150 ... Loss: 1750.08947754\n",
      "Train Epoch: 8 ... Batch: 160 ... Loss: 1756.63928223\n",
      "Train Epoch: 8 ... Batch: 170 ... Loss: 1692.35278320\n",
      "Train Epoch: 8 ... Batch: 180 ... Loss: 1678.22326660\n",
      "Train Epoch: 8 ... Batch: 190 ... Loss: 1745.36767578\n",
      "Train Epoch: 8 ... Batch: 200 ... Loss: 1728.17504883\n",
      "------------------- Test set: Average loss: 1716.5191 ... Samples: 810\n",
      "Train Epoch: 9 ... Batch: 0 ... Loss: 1773.81884766\n",
      "Train Epoch: 9 ... Batch: 10 ... Loss: 1737.18139648\n",
      "Train Epoch: 9 ... Batch: 20 ... Loss: 1693.00036621\n",
      "Train Epoch: 9 ... Batch: 30 ... Loss: 1741.20275879\n",
      "Train Epoch: 9 ... Batch: 40 ... Loss: 1708.80493164\n",
      "Train Epoch: 9 ... Batch: 50 ... Loss: 1768.65429688\n",
      "Train Epoch: 9 ... Batch: 60 ... Loss: 1699.98962402\n",
      "Train Epoch: 9 ... Batch: 70 ... Loss: 1695.44995117\n",
      "Train Epoch: 9 ... Batch: 80 ... Loss: 1719.05102539\n",
      "Train Epoch: 9 ... Batch: 90 ... Loss: 1658.70776367\n",
      "Train Epoch: 9 ... Batch: 100 ... Loss: 1716.03906250\n",
      "Train Epoch: 9 ... Batch: 110 ... Loss: 1631.82714844\n",
      "Train Epoch: 9 ... Batch: 120 ... Loss: 1668.67236328\n",
      "Train Epoch: 9 ... Batch: 130 ... Loss: 1700.63354492\n",
      "Train Epoch: 9 ... Batch: 140 ... Loss: 1719.96630859\n",
      "Train Epoch: 9 ... Batch: 150 ... Loss: 1714.66564941\n",
      "Train Epoch: 9 ... Batch: 160 ... Loss: 1648.29626465\n",
      "Train Epoch: 9 ... Batch: 170 ... Loss: 1668.75048828\n",
      "Train Epoch: 9 ... Batch: 180 ... Loss: 1712.77355957\n",
      "Train Epoch: 9 ... Batch: 190 ... Loss: 1713.87658691\n",
      "Train Epoch: 9 ... Batch: 200 ... Loss: 1639.95239258\n",
      "------------------- Test set: Average loss: 1715.8707 ... Samples: 810\n",
      "Train Epoch: 10 ... Batch: 0 ... Loss: 1628.66467285\n",
      "Train Epoch: 10 ... Batch: 10 ... Loss: 1727.77453613\n",
      "Train Epoch: 10 ... Batch: 20 ... Loss: 1717.40075684\n",
      "Train Epoch: 10 ... Batch: 30 ... Loss: 1643.36596680\n",
      "Train Epoch: 10 ... Batch: 40 ... Loss: 1749.23803711\n",
      "Train Epoch: 10 ... Batch: 50 ... Loss: 1671.93542480\n",
      "Train Epoch: 10 ... Batch: 60 ... Loss: 1764.52954102\n",
      "Train Epoch: 10 ... Batch: 70 ... Loss: 1711.12756348\n",
      "Train Epoch: 10 ... Batch: 80 ... Loss: 1774.98669434\n",
      "Train Epoch: 10 ... Batch: 90 ... Loss: 1671.58557129\n",
      "Train Epoch: 10 ... Batch: 100 ... Loss: 1650.21203613\n",
      "Train Epoch: 10 ... Batch: 110 ... Loss: 1672.63269043\n",
      "Train Epoch: 10 ... Batch: 120 ... Loss: 1705.18872070\n",
      "Train Epoch: 10 ... Batch: 130 ... Loss: 1678.06701660\n",
      "Train Epoch: 10 ... Batch: 140 ... Loss: 1716.42126465\n",
      "Train Epoch: 10 ... Batch: 150 ... Loss: 1752.94616699\n",
      "Train Epoch: 10 ... Batch: 160 ... Loss: 1687.55310059\n",
      "Train Epoch: 10 ... Batch: 170 ... Loss: 1727.98413086\n",
      "Train Epoch: 10 ... Batch: 180 ... Loss: 1748.59497070\n",
      "Train Epoch: 10 ... Batch: 190 ... Loss: 1676.94738770\n",
      "Train Epoch: 10 ... Batch: 200 ... Loss: 1736.60400391\n",
      "------------------- Test set: Average loss: 1716.3849 ... Samples: 810\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d32dc702",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the model\n",
    "torch.save(model.state_dict(), 'conv_small_kernel_without_timeaxis.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eae268",
   "metadata": {},
   "source": [
    "### test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0b07f209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 250, 730])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "382430c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 250, 730])\n"
     ]
    }
   ],
   "source": [
    "model.eval() #evaluate model\n",
    "with torch.no_grad():\n",
    "    data = data_input[750]\n",
    "    data = data[None,:,:,:]\n",
    "    print(data.shape)\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    output = output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "39848b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAD6CAAAAAC6tr/wAAAFhElEQVR4nO3d23KjOhAFUEid//9lzoOTGBI74wsgafdaT+PUzJSRNh1ZbmCaAAAAAAAAAAAAAAAAAAAAAAAAAAAAqptbv4H2ltUgLNNjQ/Lo36OdyvOzXA5/+Xo9r/78PTLL+sXnT+YbP6U3NWdn2Wb6Ydt/VXPshlFrej7r9AuhvqfW+A3lv9Zv4CTLdoFBvvyq8+ri41Hrz6D5ozmQ5Kq9nFKoP/93qe5N8oy0WHt8jeeyfkELoaPfeEV93R20Ad5M4LAv0zT38WFx3pxigUPdtcDx7iPW9wQOeKfCRrrvWK+EjXuPYoZ4mFBvxAx/hyLGtp/V9YsiZqE3Yw/qZf9h6FSvjD0X3Rl3OJd51FXIP4w7JX0ZdxwjY70y7sx0YswBTI/11Zjz04WP1m/gaUudXE/TVO94dzNie1TVudZZ+JSxBqtqqC+ztGxf8rdBqnbVTN/mysxHDDA+Yv3nZW8DzGAbAwyMaD9igIk8WfcjItiP6n4qT9bxWtuFKryj4+io1y/oeD7P1utQyPWLep3Q8/W5IBFs3tbbSX70XUNq6G1Wm+hnEIR6V/P1PixF9XP0Qr2na5noZ4ZP1suBC/ZRepnh0/XQ1CrWRyrbFNvBOV116M/WwVSfqvnxCvaJms/2mZouSMSa4zQ9j0X7dIXqdrNDFesWCiW7ix0SOECbHhIlm8O1+A0l2A3VWZKce6T6RHpRIOFnHaJQdyc93eettcWaU51z6op1p5Irt2izvTtVTNpPWJDI9RDipungaMeNV76Ya3MOPI7Yxxbki0j3kVVbrmnooPNTqgcXULe1R3FDQLJ3Poblj5vlMpTxw73zWluqx5bzEM59FyRlr51ONP5c7vl7Z/zRKO7HI3MGX5TstSARazqzT7QFO9IydOG2+ceXn+uRwb19VsaMBLcMXLXfeutiXcKg8bYgIdQbZ6SaXceIhful93x57J1oFzJgtp/c/Fs9y1Gy6dpfZ+N1W/MzxvcLtRJewVil++vdbnfnL68ej6t7jJQwVrRXC5Kv0rx5Bd/GumxyVbXf/l+cDUWMkfB99rW1s5YyxnTvdynCEIdLHX0+o53OLVP/yxIfGnnWvPp2o2N6SHjaGBd3q9o8bfMFXbfVW7TZxXfCu1mGizZ72tztoW3CRZujrRN+4heavkfkJJsPn5u471Deb5wyok1Dm766bdPHNauPXFh/4/TYp4cEDnJr8b5t5Lv3yVW0Gdt8L9q+aGdsd4uybyOJ8LsbUbQJ9RltS23SqNqEEm0y/NoiEW1CiTahRJtQok2oS7Tt/RFH1SaUaBPhd3uUaBNKtAkl2kS41/k3d3F1PexI5x+hLEjIoD2KKkSbUKJNKNEmlPYoQqnahBJtImiPogzRJoK7R1GGaBPqY5rs/ZFI1SaUaJNB5x9VfEyTi2xIpGoTSrQJpfOPUKo2oUSbCDr/KEO0iaDzjzJEm1A6/wilapNBexRViDahRJtQok2oS79263cBu1O1CSXaRNAeRRmiTSjRJpRoE2qeJu1RJPj5QVLVJpRok0HnH1WINqFEm1CiTSidf4RStQkl2kTQ+UcZok0o0SaUaBNK5x8hdP6RSXsUVYg2oUSbUKJNqI/JBgmRVG1CiTYRbrdHzfq1yaNqE0q0CSXahPLcSEKp2oQSbRLc2OQTbRIsvxfVok0o0SaUaBNK5x+hVG1CiTYR3BiNMkSbUPq1CaVqE0rnH6FUbSLoIaEM0SaUaBNKtAmlPYpQqjahRJsI2qMoQ7QJJdqEEm1Cfdj7I9OHdm0yWZAQQecfZYg2oUSbUKJNKJ1/hFK1CSXaRND5RxmiTSjRJpRoE0rnH6HmaRJuhue5kYTy3EjqEG1CiTahtEcRStUmlGgTQXsUZYg2oUSbUP8Dn8DBhISnLmYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=730x250 at 0x28948BF4F70>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## cut the image,0].numpy()*255\n",
    "original = data_input[750].numpy().reshape((250,730))*255\n",
    "Image.fromarray(original.astype('uint8'), 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ebbe2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_input[0,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc48ae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAD6CAAAAAC6tr/wAAAI2UlEQVR4nO3du4tkRRTH8VPdvTPOKuooiigKJiqCgYqgiCIYaChmggYiiKH/hBiYGGhiZmRsLIKgIIoYGhj4wkBZd9d9uK/Z7mPQ030fc6end/o+qn71/VAw0+POeKf7dPWvz61bEwyQ8Mz08i3Ty7dML1+4+9QnZjYa+oCAdmyfmMyHnZmamU2GPiCgHePxZD4un6C0oeQDn058OvHpK9eCGaUNHWF/BDczsjZ0uO+PMDKjtCFkf9p2czMCCXQQSCCKQAJVBBJoIpBAFIEEqggk0EQggSgCCVQRSKCJQAJRBBKoIpBAE4EEoggkUEUggSYCCUQRSKCKQAJNBBKIIpBAFYEEmggkEFULJMza5hZKn1vp1qrvWe/foU8EkgVflOf8BWxRq023QuXbQuUjIlELJHmWttfKuOHG/JYfLHevFz8ikXUg8frUfJRa9R4sfso7InkGkuZ5GkqyCyReDdQd/fT65+hfRoHEe5moCd7RyCiQ9Bk+5u86lxXuZpR7z/IIJO4DlVWlV+iBCu+RfiBxszBwOa1qlKMzOQSSSNogDY1yCrw72oEkkppegSm8M6qBJP6irqDCOyAYSDzZGqHCWyQVSOanut3MZkMfykaI4W2QCSTVEzIKZcEUviGZQJJYul4bFX5MEoFksDMyPdJ5NepL8pf9emZrSV311al91UCSYGnn+mBfyvT3XlsRSBLM2vk9tm5mbmE2MjvpxPCV0uyQ5FfTVdUXV2J4s+Q6JLmXtRWNzuoX5x+o8AWJDkl2Vjy9qfCF1AIJc/Z6uHotoUDCMn7ckMQCCZP22rKfChIJJJT08WRwmvZwKQQSChs3LvJAwruhDWUcS6INJN1uhZOX+bUZ2YWTeAMJRd2eYJbdDB5pINkbD30EWjzDmSLSRa3jfCaXviyuQsqnxKMLJPnc9QPJ5VR8bIFkJn+PxyLd6/7XFFMgyex6GXQspkBCFunXLJr3Vl2IJZDUNu1FH4L03R3VKRvpexp9iyGQEETQvggCCYU9GOkEOGgg0X4bkwTlJvcwgWQ+XQRm7EgoNrmHCiTnKGp0a5BA4ma39fI/wvr0TpjF0CFBJCrJO/Wde/oPJCSR2J00WzxMKV++0O8aEveM1lQm61LxabAnhzuOTfUWSFKeADJTvnDhBzNPs0VbCyRd/hJX3UkjKUp0TuqnQ+JmZluW7L2Un/ID5Ym+4tIhwRFCmteHdNoh8eJCaiQsJLnWpB5Irgdvsbyp6rSlvRKiu0CS8r2CmgTTdo8dEiTGQ+lCHHe3zwY9nBvVUYeEGVvAgXnutSGO4rieMDOzH1sOJBS2pCRXvhJIcIDGdqJFINm6/exmP6mV40EERB7KZSCZ/Xv830jkvsDh0lvUXQoks4EPBWhTGx0S5uxMJPZucqMOSWZbkiOlEzjHXUNSKmombcRorUBSvBAtLqHz8q2yRNeKYV0JvVDXA8m8jGej8i2rV/GhE3WwEbM4otAUSNwaJmpgKYnEvSKQHLem014PiXUksata22tI4v5t0aqoH+4O1pCwKwOG9tyn27FsHY9ERXr+/euv7UAg2XjGZcrOTYSVbWYnI/hbNkjWvGEQY7tk+7HdSP64HpK0/wrtc8MeS9XVH75gHxJswBtuhPLtISd0Agna1bCd8TAVTocEnVg5oVsvCb0aSKIKTBBz1ITexhmg4ilTBJIRV9mgF/trlIobNq/w5Q7IofLfQvkbj679ZW0XgWQ2/zHM2+hBvUSb8kv1Vig/JWprrMv/tPjJdEiQhCOLv/5soUMCEfW8wSkbaCnOHRFIoKm+qJV3kRBBIIEqAgm0hOVHdmqFJAIJVBFIoIlAAlG1QELvDzIIJNBEIIGWYhkgHRKIIpBAE4EEWpYr/2qBJES4XQpwLG1vjAbEgUACMQcu+6W0IYYOCTQRSCCKQAJVbIwGTQQSiCKQQAsbo0EdgQRaDltDAsggkEBTNZDQ+4MMAglUEUigpXljNAIJZFQDyYiLbCCDQAJNnLKBKDZGgyoCCTQRSKCFjdEgj0ACKb6YtwkkEEUggapKIKH3BxkEEohhYzSoo0MCTQQSiCKQQNUikIzMytuTAIkLFrwobUAGgQRa2BgN6uiQQBSBBKoIJNDEPiTQwk6tkEcggZbmjdEAGQQSqCKQQBOBBKKqgYSVf9BBIIEmAgm0sDEa5BFIoIlAAlEEEqhiYzRIOWSnVkobyWveGI1TNtBR2YeEaRsyavuQMG1DBR0SqCp3SIgj0FHpkBBHoKMSSGYDHwywseaN0UZEEshgDQlE0SGBKha1QhNrSCCKQAJV5UBCXxupK/X+yoGEPILULTdqJZBAFh0SaOKUDURVAglZG0IIJNBEIIGWxo3RtoY7HqB1pUBybehjAdpTvcqG05GQwSkbqKp0SPaGPRagPdUOCR1ApI41JJDHKRto4pQNRBFIoIqN0aCJQAJRBBJoad4YDdBBIIEoAglUEUigpHkfEnp/SF5RxOVAwnJtKKn+cT0gcctpu/bH9QAVtQ7Ju86elhBR7ZB8yJtJiKhtHU9dQ8UykEzMbEYYgY5lh+Q1to6HkqJD8tnQhwK0oGFjtBfHTNpQsuyQfGnmFDdkFB2SZ8y+G/hggPYUHZJvbfTmp0+d+nXr/j+mE2cwYh9PHVHby0Ayml05d6eNdyd/BTMGI/5xRGUvA8n779hHp3/Zvf+us+7GYMQ/mgv6YCB59PHJW3fu2k0/3XF18Kcjg7HOaErYxdeXp2zuG19920KYPeCDHzGDscY4ZN4uB5J5af/51cVv/rswne2cGfyFhsFYZ6xe9OTLQHLTC7PXd66Mr+89O135ZADiUNsN/vrJ4vPPXzYz+/j7h8zMRndv2Xu+bWHLhn4yMhjHGBdCUdovublbuHdiZja5dnY8Ge2NR9ORuxmDkdiYnL1itm12fuZ7bm7mFy7eamYWfredne1/zu/8fa7blxGgIw/bzecuvfrg6d3nny5/eXLt+h0T+/6RE1Q2EnVq9Ntvj73x80PP3lOJ4eGLoQ4I6BRXtEPU/9TGqPfu27fxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=730x250 at 0x28948C84940>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = output[0,0].numpy()*255\n",
    "Image.fromarray(predict.astype('uint8'), 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b191c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## output[0,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a554f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_input[0,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5e90b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 250, 730])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e69539bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0129, device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(output[0].reshape(1,-1), data[0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1c661f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2231, 0.1729, 0.2417,  ..., 0.6798, 0.6798, 0.6798]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "083c8ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f23fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
