{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf1101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1ae107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b883e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae69e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5e1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842f554",
   "metadata": {},
   "source": [
    "### read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c46db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\liuya\\Downloads\\3d_printing_research\\clipped_samples_otsu'\n",
    "## path = r'C:\\Users\\liuya\\Downloads\\3d_printing_research\\clipped_samples'\n",
    "image_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01aa86cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 730, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_path = os.path.join(path, image_list[350])\n",
    "np.array(Image.open(full_path).convert('RGB')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "921a343a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7ac8f136fe4cc694a31eac488fa13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_ls = []\n",
    "\n",
    "for i in tqdm(image_list):\n",
    "    full_path = os.path.join(path, i)\n",
    "    img = Image.open(full_path).convert('L')\n",
    "    img_array = np.asarray(img).flatten()\n",
    "    image_ls.append(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3023d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalize\n",
    "data_input = np.array(image_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcbe5af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4046, 182500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ccffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = np.divide((data_input-np.min(data_input)), (np.max(data_input)-np.min(data_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a494be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_input = torch.tensor(data_input[:,None,:,:], dtype=torch.float32)\n",
    "data_input = torch.tensor(data_input, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56b6bc",
   "metadata": {},
   "source": [
    "### build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dffafbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.encoder = nn.Sequential(\n",
    "            ## 250*730=182500\n",
    "            nn.Linear(250*730, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 8))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(8, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 182500),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e203cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c275f52",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27b63c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \n",
    "    model.train() #trian model\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        ##calculate loss\n",
    "        ##loss = 0\n",
    "        ##for i in range(data.shape[0]):\n",
    "            ##loss += F.mse_loss(output[i], data[i], reduction='sum')\n",
    "        ##loss /= data.shape[0]\n",
    "        loss = F.mse_loss(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print result every 10 batch\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} ... Batch: {} ... Loss: {:.8f}'.format(epoch, batch_idx, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25aefaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval() #evaluate model\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            #calculate sum loss\n",
    "            test_loss += F.mse_loss(output, data, reduction='sum').item()\n",
    "    \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('------------------- Test set: Average loss: {:.4f} ... Samples: {}'.format(test_loss, len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf4996",
   "metadata": {},
   "source": [
    "### train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a10eafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_, val_window_ = train_test_split(data_input, test_size=0.2, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9b7cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3236, 182500])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_window_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7de11bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_window_, batch_size=16,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(val_window_, batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2290e897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dafccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "592254fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e46e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "255c041b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 ... Batch: 0 ... Loss: 0.00164014\n",
      "Train Epoch: 1 ... Batch: 10 ... Loss: 0.00193552\n",
      "Train Epoch: 1 ... Batch: 20 ... Loss: 0.00196117\n",
      "Train Epoch: 1 ... Batch: 30 ... Loss: 0.00230773\n",
      "Train Epoch: 1 ... Batch: 40 ... Loss: 0.00210383\n",
      "Train Epoch: 1 ... Batch: 50 ... Loss: 0.00190351\n",
      "Train Epoch: 1 ... Batch: 60 ... Loss: 0.00154617\n",
      "Train Epoch: 1 ... Batch: 70 ... Loss: 0.00194986\n",
      "Train Epoch: 1 ... Batch: 80 ... Loss: 0.00204894\n",
      "Train Epoch: 1 ... Batch: 90 ... Loss: 0.00203136\n",
      "Train Epoch: 1 ... Batch: 100 ... Loss: 0.00182606\n",
      "Train Epoch: 1 ... Batch: 110 ... Loss: 0.00188021\n",
      "Train Epoch: 1 ... Batch: 120 ... Loss: 0.00181447\n",
      "Train Epoch: 1 ... Batch: 130 ... Loss: 0.00206819\n",
      "Train Epoch: 1 ... Batch: 140 ... Loss: 0.00219748\n",
      "Train Epoch: 1 ... Batch: 150 ... Loss: 0.00228457\n",
      "Train Epoch: 1 ... Batch: 160 ... Loss: 0.00182335\n",
      "Train Epoch: 1 ... Batch: 170 ... Loss: 0.00188555\n",
      "Train Epoch: 1 ... Batch: 180 ... Loss: 0.00174191\n",
      "Train Epoch: 1 ... Batch: 190 ... Loss: 0.00199061\n",
      "Train Epoch: 1 ... Batch: 200 ... Loss: 0.00212109\n",
      "------------------- Test set: Average loss: 483.5353 ... Samples: 810\n",
      "Train Epoch: 2 ... Batch: 0 ... Loss: 0.00201523\n",
      "Train Epoch: 2 ... Batch: 10 ... Loss: 0.00191598\n",
      "Train Epoch: 2 ... Batch: 20 ... Loss: 0.00192237\n",
      "Train Epoch: 2 ... Batch: 30 ... Loss: 0.00190328\n",
      "Train Epoch: 2 ... Batch: 40 ... Loss: 0.00166094\n",
      "Train Epoch: 2 ... Batch: 50 ... Loss: 0.00181557\n",
      "Train Epoch: 2 ... Batch: 60 ... Loss: 0.00193059\n",
      "Train Epoch: 2 ... Batch: 70 ... Loss: 0.00165195\n",
      "Train Epoch: 2 ... Batch: 80 ... Loss: 0.00203264\n",
      "Train Epoch: 2 ... Batch: 90 ... Loss: 0.00198541\n",
      "Train Epoch: 2 ... Batch: 100 ... Loss: 0.00195163\n",
      "Train Epoch: 2 ... Batch: 110 ... Loss: 0.00203066\n",
      "Train Epoch: 2 ... Batch: 120 ... Loss: 0.00180642\n",
      "Train Epoch: 2 ... Batch: 130 ... Loss: 0.00188308\n",
      "Train Epoch: 2 ... Batch: 140 ... Loss: 0.00193624\n",
      "Train Epoch: 2 ... Batch: 150 ... Loss: 0.00209723\n",
      "Train Epoch: 2 ... Batch: 160 ... Loss: 0.00191480\n",
      "Train Epoch: 2 ... Batch: 170 ... Loss: 0.00172699\n",
      "Train Epoch: 2 ... Batch: 180 ... Loss: 0.00233958\n",
      "Train Epoch: 2 ... Batch: 190 ... Loss: 0.00243115\n",
      "Train Epoch: 2 ... Batch: 200 ... Loss: 0.00186623\n",
      "------------------- Test set: Average loss: 482.8340 ... Samples: 810\n",
      "Train Epoch: 3 ... Batch: 0 ... Loss: 0.00239564\n",
      "Train Epoch: 3 ... Batch: 10 ... Loss: 0.00217221\n",
      "Train Epoch: 3 ... Batch: 20 ... Loss: 0.00189552\n",
      "Train Epoch: 3 ... Batch: 30 ... Loss: 0.00214763\n",
      "Train Epoch: 3 ... Batch: 40 ... Loss: 0.00191148\n",
      "Train Epoch: 3 ... Batch: 50 ... Loss: 0.00208943\n",
      "Train Epoch: 3 ... Batch: 60 ... Loss: 0.00200238\n",
      "Train Epoch: 3 ... Batch: 70 ... Loss: 0.00195547\n",
      "Train Epoch: 3 ... Batch: 80 ... Loss: 0.00172091\n",
      "Train Epoch: 3 ... Batch: 90 ... Loss: 0.00188598\n",
      "Train Epoch: 3 ... Batch: 100 ... Loss: 0.00172458\n",
      "Train Epoch: 3 ... Batch: 110 ... Loss: 0.00197191\n",
      "Train Epoch: 3 ... Batch: 120 ... Loss: 0.00175758\n",
      "Train Epoch: 3 ... Batch: 130 ... Loss: 0.00230116\n",
      "Train Epoch: 3 ... Batch: 140 ... Loss: 0.00217132\n",
      "Train Epoch: 3 ... Batch: 150 ... Loss: 0.00188292\n",
      "Train Epoch: 3 ... Batch: 160 ... Loss: 0.00188750\n",
      "Train Epoch: 3 ... Batch: 170 ... Loss: 0.00226183\n",
      "Train Epoch: 3 ... Batch: 180 ... Loss: 0.00180289\n",
      "Train Epoch: 3 ... Batch: 190 ... Loss: 0.00197224\n",
      "Train Epoch: 3 ... Batch: 200 ... Loss: 0.00192085\n",
      "------------------- Test set: Average loss: 483.4574 ... Samples: 810\n",
      "Train Epoch: 4 ... Batch: 0 ... Loss: 0.00181426\n",
      "Train Epoch: 4 ... Batch: 10 ... Loss: 0.00188254\n",
      "Train Epoch: 4 ... Batch: 20 ... Loss: 0.00216391\n",
      "Train Epoch: 4 ... Batch: 30 ... Loss: 0.00175554\n",
      "Train Epoch: 4 ... Batch: 40 ... Loss: 0.00207944\n",
      "Train Epoch: 4 ... Batch: 50 ... Loss: 0.00206815\n",
      "Train Epoch: 4 ... Batch: 60 ... Loss: 0.00232751\n",
      "Train Epoch: 4 ... Batch: 70 ... Loss: 0.00227353\n",
      "Train Epoch: 4 ... Batch: 80 ... Loss: 0.00234473\n",
      "Train Epoch: 4 ... Batch: 90 ... Loss: 0.00193307\n",
      "Train Epoch: 4 ... Batch: 100 ... Loss: 0.00168338\n",
      "Train Epoch: 4 ... Batch: 110 ... Loss: 0.00182683\n",
      "Train Epoch: 4 ... Batch: 120 ... Loss: 0.00198872\n",
      "Train Epoch: 4 ... Batch: 130 ... Loss: 0.00205377\n",
      "Train Epoch: 4 ... Batch: 140 ... Loss: 0.00202014\n",
      "Train Epoch: 4 ... Batch: 150 ... Loss: 0.00206248\n",
      "Train Epoch: 4 ... Batch: 160 ... Loss: 0.00195045\n",
      "Train Epoch: 4 ... Batch: 170 ... Loss: 0.00217934\n",
      "Train Epoch: 4 ... Batch: 180 ... Loss: 0.00217483\n",
      "Train Epoch: 4 ... Batch: 190 ... Loss: 0.00209089\n",
      "Train Epoch: 4 ... Batch: 200 ... Loss: 0.00198351\n",
      "------------------- Test set: Average loss: 482.6202 ... Samples: 810\n",
      "Train Epoch: 5 ... Batch: 0 ... Loss: 0.00159007\n",
      "Train Epoch: 5 ... Batch: 10 ... Loss: 0.00194613\n",
      "Train Epoch: 5 ... Batch: 20 ... Loss: 0.00192967\n",
      "Train Epoch: 5 ... Batch: 30 ... Loss: 0.00192691\n",
      "Train Epoch: 5 ... Batch: 40 ... Loss: 0.00194203\n",
      "Train Epoch: 5 ... Batch: 50 ... Loss: 0.00187137\n",
      "Train Epoch: 5 ... Batch: 60 ... Loss: 0.00197271\n",
      "Train Epoch: 5 ... Batch: 70 ... Loss: 0.00207906\n",
      "Train Epoch: 5 ... Batch: 80 ... Loss: 0.00167890\n",
      "Train Epoch: 5 ... Batch: 90 ... Loss: 0.00181826\n",
      "Train Epoch: 5 ... Batch: 100 ... Loss: 0.00216281\n",
      "Train Epoch: 5 ... Batch: 110 ... Loss: 0.00197025\n",
      "Train Epoch: 5 ... Batch: 120 ... Loss: 0.00173897\n",
      "Train Epoch: 5 ... Batch: 130 ... Loss: 0.00201686\n",
      "Train Epoch: 5 ... Batch: 140 ... Loss: 0.00195501\n",
      "Train Epoch: 5 ... Batch: 150 ... Loss: 0.00205045\n",
      "Train Epoch: 5 ... Batch: 160 ... Loss: 0.00191074\n",
      "Train Epoch: 5 ... Batch: 170 ... Loss: 0.00199525\n",
      "Train Epoch: 5 ... Batch: 180 ... Loss: 0.00232558\n",
      "Train Epoch: 5 ... Batch: 190 ... Loss: 0.00190601\n",
      "Train Epoch: 5 ... Batch: 200 ... Loss: 0.00220316\n",
      "------------------- Test set: Average loss: 482.4058 ... Samples: 810\n",
      "Train Epoch: 6 ... Batch: 0 ... Loss: 0.00188272\n",
      "Train Epoch: 6 ... Batch: 10 ... Loss: 0.00179804\n",
      "Train Epoch: 6 ... Batch: 20 ... Loss: 0.00231379\n",
      "Train Epoch: 6 ... Batch: 30 ... Loss: 0.00197337\n",
      "Train Epoch: 6 ... Batch: 40 ... Loss: 0.00202226\n",
      "Train Epoch: 6 ... Batch: 50 ... Loss: 0.00167741\n",
      "Train Epoch: 6 ... Batch: 60 ... Loss: 0.00180637\n",
      "Train Epoch: 6 ... Batch: 70 ... Loss: 0.00221855\n",
      "Train Epoch: 6 ... Batch: 80 ... Loss: 0.00170932\n",
      "Train Epoch: 6 ... Batch: 90 ... Loss: 0.00165160\n",
      "Train Epoch: 6 ... Batch: 100 ... Loss: 0.00181713\n",
      "Train Epoch: 6 ... Batch: 110 ... Loss: 0.00185661\n",
      "Train Epoch: 6 ... Batch: 120 ... Loss: 0.00181782\n",
      "Train Epoch: 6 ... Batch: 130 ... Loss: 0.00192367\n",
      "Train Epoch: 6 ... Batch: 140 ... Loss: 0.00168740\n",
      "Train Epoch: 6 ... Batch: 150 ... Loss: 0.00161506\n",
      "Train Epoch: 6 ... Batch: 160 ... Loss: 0.00200785\n",
      "Train Epoch: 6 ... Batch: 170 ... Loss: 0.00178983\n",
      "Train Epoch: 6 ... Batch: 180 ... Loss: 0.00206718\n",
      "Train Epoch: 6 ... Batch: 190 ... Loss: 0.00172993\n",
      "Train Epoch: 6 ... Batch: 200 ... Loss: 0.00174133\n",
      "------------------- Test set: Average loss: 482.3079 ... Samples: 810\n",
      "Train Epoch: 7 ... Batch: 0 ... Loss: 0.00198032\n",
      "Train Epoch: 7 ... Batch: 10 ... Loss: 0.00226017\n",
      "Train Epoch: 7 ... Batch: 20 ... Loss: 0.00181588\n",
      "Train Epoch: 7 ... Batch: 30 ... Loss: 0.00210357\n",
      "Train Epoch: 7 ... Batch: 40 ... Loss: 0.00181035\n",
      "Train Epoch: 7 ... Batch: 50 ... Loss: 0.00186055\n",
      "Train Epoch: 7 ... Batch: 60 ... Loss: 0.00193562\n",
      "Train Epoch: 7 ... Batch: 70 ... Loss: 0.00200191\n",
      "Train Epoch: 7 ... Batch: 80 ... Loss: 0.00167791\n",
      "Train Epoch: 7 ... Batch: 90 ... Loss: 0.00274763\n",
      "Train Epoch: 7 ... Batch: 100 ... Loss: 0.00241611\n",
      "Train Epoch: 7 ... Batch: 110 ... Loss: 0.00205923\n",
      "Train Epoch: 7 ... Batch: 120 ... Loss: 0.00176521\n",
      "Train Epoch: 7 ... Batch: 130 ... Loss: 0.00200919\n",
      "Train Epoch: 7 ... Batch: 140 ... Loss: 0.00176453\n",
      "Train Epoch: 7 ... Batch: 150 ... Loss: 0.00210774\n",
      "Train Epoch: 7 ... Batch: 160 ... Loss: 0.00195720\n",
      "Train Epoch: 7 ... Batch: 170 ... Loss: 0.00192101\n",
      "Train Epoch: 7 ... Batch: 180 ... Loss: 0.00203714\n",
      "Train Epoch: 7 ... Batch: 190 ... Loss: 0.00220132\n",
      "Train Epoch: 7 ... Batch: 200 ... Loss: 0.00244760\n",
      "------------------- Test set: Average loss: 483.0970 ... Samples: 810\n",
      "Train Epoch: 8 ... Batch: 0 ... Loss: 0.00206089\n",
      "Train Epoch: 8 ... Batch: 10 ... Loss: 0.00208440\n",
      "Train Epoch: 8 ... Batch: 20 ... Loss: 0.00204822\n",
      "Train Epoch: 8 ... Batch: 30 ... Loss: 0.00192195\n",
      "Train Epoch: 8 ... Batch: 40 ... Loss: 0.00175092\n",
      "Train Epoch: 8 ... Batch: 50 ... Loss: 0.00174348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 ... Batch: 60 ... Loss: 0.00218337\n",
      "Train Epoch: 8 ... Batch: 70 ... Loss: 0.00226493\n",
      "Train Epoch: 8 ... Batch: 80 ... Loss: 0.00209180\n",
      "Train Epoch: 8 ... Batch: 90 ... Loss: 0.00212219\n",
      "Train Epoch: 8 ... Batch: 100 ... Loss: 0.00168326\n",
      "Train Epoch: 8 ... Batch: 110 ... Loss: 0.00230667\n",
      "Train Epoch: 8 ... Batch: 120 ... Loss: 0.00198152\n",
      "Train Epoch: 8 ... Batch: 130 ... Loss: 0.00220792\n",
      "Train Epoch: 8 ... Batch: 140 ... Loss: 0.00210697\n",
      "Train Epoch: 8 ... Batch: 150 ... Loss: 0.00205672\n",
      "Train Epoch: 8 ... Batch: 160 ... Loss: 0.00202605\n",
      "Train Epoch: 8 ... Batch: 170 ... Loss: 0.00192972\n",
      "Train Epoch: 8 ... Batch: 180 ... Loss: 0.00186204\n",
      "Train Epoch: 8 ... Batch: 190 ... Loss: 0.00196260\n",
      "Train Epoch: 8 ... Batch: 200 ... Loss: 0.00197416\n",
      "------------------- Test set: Average loss: 482.1555 ... Samples: 810\n",
      "Train Epoch: 9 ... Batch: 0 ... Loss: 0.00188594\n",
      "Train Epoch: 9 ... Batch: 10 ... Loss: 0.00168775\n",
      "Train Epoch: 9 ... Batch: 20 ... Loss: 0.00218043\n",
      "Train Epoch: 9 ... Batch: 30 ... Loss: 0.00186988\n",
      "Train Epoch: 9 ... Batch: 40 ... Loss: 0.00209755\n",
      "Train Epoch: 9 ... Batch: 50 ... Loss: 0.00203980\n",
      "Train Epoch: 9 ... Batch: 60 ... Loss: 0.00196768\n",
      "Train Epoch: 9 ... Batch: 70 ... Loss: 0.00214198\n",
      "Train Epoch: 9 ... Batch: 80 ... Loss: 0.00171916\n",
      "Train Epoch: 9 ... Batch: 90 ... Loss: 0.00154436\n",
      "Train Epoch: 9 ... Batch: 100 ... Loss: 0.00206073\n",
      "Train Epoch: 9 ... Batch: 110 ... Loss: 0.00213719\n",
      "Train Epoch: 9 ... Batch: 120 ... Loss: 0.00199221\n",
      "Train Epoch: 9 ... Batch: 130 ... Loss: 0.00174866\n",
      "Train Epoch: 9 ... Batch: 140 ... Loss: 0.00178316\n",
      "Train Epoch: 9 ... Batch: 150 ... Loss: 0.00200791\n",
      "Train Epoch: 9 ... Batch: 160 ... Loss: 0.00185215\n",
      "Train Epoch: 9 ... Batch: 170 ... Loss: 0.00195254\n",
      "Train Epoch: 9 ... Batch: 180 ... Loss: 0.00172657\n",
      "Train Epoch: 9 ... Batch: 190 ... Loss: 0.00218171\n",
      "Train Epoch: 9 ... Batch: 200 ... Loss: 0.00218548\n",
      "------------------- Test set: Average loss: 481.9573 ... Samples: 810\n",
      "Train Epoch: 10 ... Batch: 0 ... Loss: 0.00193154\n",
      "Train Epoch: 10 ... Batch: 10 ... Loss: 0.00191449\n",
      "Train Epoch: 10 ... Batch: 20 ... Loss: 0.00200586\n",
      "Train Epoch: 10 ... Batch: 30 ... Loss: 0.00201028\n",
      "Train Epoch: 10 ... Batch: 40 ... Loss: 0.00188347\n",
      "Train Epoch: 10 ... Batch: 50 ... Loss: 0.00203870\n",
      "Train Epoch: 10 ... Batch: 60 ... Loss: 0.00176911\n",
      "Train Epoch: 10 ... Batch: 70 ... Loss: 0.00187104\n",
      "Train Epoch: 10 ... Batch: 80 ... Loss: 0.00220330\n",
      "Train Epoch: 10 ... Batch: 90 ... Loss: 0.00194374\n",
      "Train Epoch: 10 ... Batch: 100 ... Loss: 0.00173146\n",
      "Train Epoch: 10 ... Batch: 110 ... Loss: 0.00197087\n",
      "Train Epoch: 10 ... Batch: 120 ... Loss: 0.00186231\n",
      "Train Epoch: 10 ... Batch: 130 ... Loss: 0.00189505\n",
      "Train Epoch: 10 ... Batch: 140 ... Loss: 0.00185194\n",
      "Train Epoch: 10 ... Batch: 150 ... Loss: 0.00180272\n",
      "Train Epoch: 10 ... Batch: 160 ... Loss: 0.00256323\n",
      "Train Epoch: 10 ... Batch: 170 ... Loss: 0.00189322\n",
      "Train Epoch: 10 ... Batch: 180 ... Loss: 0.00174985\n",
      "Train Epoch: 10 ... Batch: 190 ... Loss: 0.00193356\n",
      "Train Epoch: 10 ... Batch: 200 ... Loss: 0.00191333\n",
      "------------------- Test set: Average loss: 481.6872 ... Samples: 810\n",
      "Train Epoch: 11 ... Batch: 0 ... Loss: 0.00190950\n",
      "Train Epoch: 11 ... Batch: 10 ... Loss: 0.00222627\n",
      "Train Epoch: 11 ... Batch: 20 ... Loss: 0.00182276\n",
      "Train Epoch: 11 ... Batch: 30 ... Loss: 0.00188855\n",
      "Train Epoch: 11 ... Batch: 40 ... Loss: 0.00173479\n",
      "Train Epoch: 11 ... Batch: 50 ... Loss: 0.00177664\n",
      "Train Epoch: 11 ... Batch: 60 ... Loss: 0.00190442\n",
      "Train Epoch: 11 ... Batch: 70 ... Loss: 0.00183610\n",
      "Train Epoch: 11 ... Batch: 80 ... Loss: 0.00179371\n",
      "Train Epoch: 11 ... Batch: 90 ... Loss: 0.00176795\n",
      "Train Epoch: 11 ... Batch: 100 ... Loss: 0.00207502\n",
      "Train Epoch: 11 ... Batch: 110 ... Loss: 0.00207186\n",
      "Train Epoch: 11 ... Batch: 120 ... Loss: 0.00191625\n",
      "Train Epoch: 11 ... Batch: 130 ... Loss: 0.00212013\n",
      "Train Epoch: 11 ... Batch: 140 ... Loss: 0.00197285\n",
      "Train Epoch: 11 ... Batch: 150 ... Loss: 0.00197690\n",
      "Train Epoch: 11 ... Batch: 160 ... Loss: 0.00174354\n",
      "Train Epoch: 11 ... Batch: 170 ... Loss: 0.00231698\n",
      "Train Epoch: 11 ... Batch: 180 ... Loss: 0.00202597\n",
      "Train Epoch: 11 ... Batch: 190 ... Loss: 0.00196418\n",
      "Train Epoch: 11 ... Batch: 200 ... Loss: 0.00192091\n",
      "------------------- Test set: Average loss: 481.2925 ... Samples: 810\n",
      "Train Epoch: 12 ... Batch: 0 ... Loss: 0.00180249\n",
      "Train Epoch: 12 ... Batch: 10 ... Loss: 0.00199634\n",
      "Train Epoch: 12 ... Batch: 20 ... Loss: 0.00187725\n",
      "Train Epoch: 12 ... Batch: 30 ... Loss: 0.00194566\n",
      "Train Epoch: 12 ... Batch: 40 ... Loss: 0.00191528\n",
      "Train Epoch: 12 ... Batch: 50 ... Loss: 0.00183219\n",
      "Train Epoch: 12 ... Batch: 60 ... Loss: 0.00171105\n",
      "Train Epoch: 12 ... Batch: 70 ... Loss: 0.00177046\n",
      "Train Epoch: 12 ... Batch: 80 ... Loss: 0.00204733\n",
      "Train Epoch: 12 ... Batch: 90 ... Loss: 0.00162952\n",
      "Train Epoch: 12 ... Batch: 100 ... Loss: 0.00187246\n",
      "Train Epoch: 12 ... Batch: 110 ... Loss: 0.00200316\n",
      "Train Epoch: 12 ... Batch: 120 ... Loss: 0.00231666\n",
      "Train Epoch: 12 ... Batch: 130 ... Loss: 0.00190831\n",
      "Train Epoch: 12 ... Batch: 140 ... Loss: 0.00208553\n",
      "Train Epoch: 12 ... Batch: 150 ... Loss: 0.00199149\n",
      "Train Epoch: 12 ... Batch: 160 ... Loss: 0.00195032\n",
      "Train Epoch: 12 ... Batch: 170 ... Loss: 0.00175455\n",
      "Train Epoch: 12 ... Batch: 180 ... Loss: 0.00186476\n",
      "Train Epoch: 12 ... Batch: 190 ... Loss: 0.00168533\n",
      "Train Epoch: 12 ... Batch: 200 ... Loss: 0.00188763\n",
      "------------------- Test set: Average loss: 481.2679 ... Samples: 810\n",
      "Train Epoch: 13 ... Batch: 0 ... Loss: 0.00199444\n",
      "Train Epoch: 13 ... Batch: 10 ... Loss: 0.00184639\n",
      "Train Epoch: 13 ... Batch: 20 ... Loss: 0.00204724\n",
      "Train Epoch: 13 ... Batch: 30 ... Loss: 0.00218461\n",
      "Train Epoch: 13 ... Batch: 40 ... Loss: 0.00174160\n",
      "Train Epoch: 13 ... Batch: 50 ... Loss: 0.00169842\n",
      "Train Epoch: 13 ... Batch: 60 ... Loss: 0.00193368\n",
      "Train Epoch: 13 ... Batch: 70 ... Loss: 0.00186073\n",
      "Train Epoch: 13 ... Batch: 80 ... Loss: 0.00212607\n",
      "Train Epoch: 13 ... Batch: 90 ... Loss: 0.00207537\n",
      "Train Epoch: 13 ... Batch: 100 ... Loss: 0.00200645\n",
      "Train Epoch: 13 ... Batch: 110 ... Loss: 0.00218350\n",
      "Train Epoch: 13 ... Batch: 120 ... Loss: 0.00226164\n",
      "Train Epoch: 13 ... Batch: 130 ... Loss: 0.00173064\n",
      "Train Epoch: 13 ... Batch: 140 ... Loss: 0.00196554\n",
      "Train Epoch: 13 ... Batch: 150 ... Loss: 0.00182959\n",
      "Train Epoch: 13 ... Batch: 160 ... Loss: 0.00180774\n",
      "Train Epoch: 13 ... Batch: 170 ... Loss: 0.00191679\n",
      "Train Epoch: 13 ... Batch: 180 ... Loss: 0.00184141\n",
      "Train Epoch: 13 ... Batch: 190 ... Loss: 0.00213623\n",
      "Train Epoch: 13 ... Batch: 200 ... Loss: 0.00197029\n",
      "------------------- Test set: Average loss: 481.3559 ... Samples: 810\n",
      "Train Epoch: 14 ... Batch: 0 ... Loss: 0.00213966\n",
      "Train Epoch: 14 ... Batch: 10 ... Loss: 0.00193550\n",
      "Train Epoch: 14 ... Batch: 20 ... Loss: 0.00196285\n",
      "Train Epoch: 14 ... Batch: 30 ... Loss: 0.00191446\n",
      "Train Epoch: 14 ... Batch: 40 ... Loss: 0.00183179\n",
      "Train Epoch: 14 ... Batch: 50 ... Loss: 0.00155393\n",
      "Train Epoch: 14 ... Batch: 60 ... Loss: 0.00180678\n",
      "Train Epoch: 14 ... Batch: 70 ... Loss: 0.00186664\n",
      "Train Epoch: 14 ... Batch: 80 ... Loss: 0.00187977\n",
      "Train Epoch: 14 ... Batch: 90 ... Loss: 0.00196410\n",
      "Train Epoch: 14 ... Batch: 100 ... Loss: 0.00174738\n",
      "Train Epoch: 14 ... Batch: 110 ... Loss: 0.00200112\n",
      "Train Epoch: 14 ... Batch: 120 ... Loss: 0.00177944\n",
      "Train Epoch: 14 ... Batch: 130 ... Loss: 0.00194895\n",
      "Train Epoch: 14 ... Batch: 140 ... Loss: 0.00173425\n",
      "Train Epoch: 14 ... Batch: 150 ... Loss: 0.00181208\n",
      "Train Epoch: 14 ... Batch: 160 ... Loss: 0.00186354\n",
      "Train Epoch: 14 ... Batch: 170 ... Loss: 0.00193763\n",
      "Train Epoch: 14 ... Batch: 180 ... Loss: 0.00179170\n",
      "Train Epoch: 14 ... Batch: 190 ... Loss: 0.00191646\n",
      "Train Epoch: 14 ... Batch: 200 ... Loss: 0.00197937\n",
      "------------------- Test set: Average loss: 480.9609 ... Samples: 810\n",
      "Train Epoch: 15 ... Batch: 0 ... Loss: 0.00208786\n",
      "Train Epoch: 15 ... Batch: 10 ... Loss: 0.00230721\n",
      "Train Epoch: 15 ... Batch: 20 ... Loss: 0.00190771\n",
      "Train Epoch: 15 ... Batch: 30 ... Loss: 0.00233259\n",
      "Train Epoch: 15 ... Batch: 40 ... Loss: 0.00198992\n",
      "Train Epoch: 15 ... Batch: 50 ... Loss: 0.00214330\n",
      "Train Epoch: 15 ... Batch: 60 ... Loss: 0.00180393\n",
      "Train Epoch: 15 ... Batch: 70 ... Loss: 0.00179945\n",
      "Train Epoch: 15 ... Batch: 80 ... Loss: 0.00217666\n",
      "Train Epoch: 15 ... Batch: 90 ... Loss: 0.00215954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 ... Batch: 100 ... Loss: 0.00180631\n",
      "Train Epoch: 15 ... Batch: 110 ... Loss: 0.00196114\n",
      "Train Epoch: 15 ... Batch: 120 ... Loss: 0.00191753\n",
      "Train Epoch: 15 ... Batch: 130 ... Loss: 0.00214466\n",
      "Train Epoch: 15 ... Batch: 140 ... Loss: 0.00195710\n",
      "Train Epoch: 15 ... Batch: 150 ... Loss: 0.00161788\n",
      "Train Epoch: 15 ... Batch: 160 ... Loss: 0.00179711\n",
      "Train Epoch: 15 ... Batch: 170 ... Loss: 0.00187425\n",
      "Train Epoch: 15 ... Batch: 180 ... Loss: 0.00192673\n",
      "Train Epoch: 15 ... Batch: 190 ... Loss: 0.00177481\n",
      "Train Epoch: 15 ... Batch: 200 ... Loss: 0.00198056\n",
      "------------------- Test set: Average loss: 480.6351 ... Samples: 810\n",
      "Train Epoch: 16 ... Batch: 0 ... Loss: 0.00194987\n",
      "Train Epoch: 16 ... Batch: 10 ... Loss: 0.00191486\n",
      "Train Epoch: 16 ... Batch: 20 ... Loss: 0.00170456\n",
      "Train Epoch: 16 ... Batch: 30 ... Loss: 0.00206725\n",
      "Train Epoch: 16 ... Batch: 40 ... Loss: 0.00183754\n",
      "Train Epoch: 16 ... Batch: 50 ... Loss: 0.00183026\n",
      "Train Epoch: 16 ... Batch: 60 ... Loss: 0.00198567\n",
      "Train Epoch: 16 ... Batch: 70 ... Loss: 0.00193983\n",
      "Train Epoch: 16 ... Batch: 80 ... Loss: 0.00252819\n",
      "Train Epoch: 16 ... Batch: 90 ... Loss: 0.00214753\n",
      "Train Epoch: 16 ... Batch: 100 ... Loss: 0.00178968\n",
      "Train Epoch: 16 ... Batch: 110 ... Loss: 0.00208845\n",
      "Train Epoch: 16 ... Batch: 120 ... Loss: 0.00193349\n",
      "Train Epoch: 16 ... Batch: 130 ... Loss: 0.00239591\n",
      "Train Epoch: 16 ... Batch: 140 ... Loss: 0.00187378\n",
      "Train Epoch: 16 ... Batch: 150 ... Loss: 0.00190829\n",
      "Train Epoch: 16 ... Batch: 160 ... Loss: 0.00212825\n",
      "Train Epoch: 16 ... Batch: 170 ... Loss: 0.00199894\n",
      "Train Epoch: 16 ... Batch: 180 ... Loss: 0.00197812\n",
      "Train Epoch: 16 ... Batch: 190 ... Loss: 0.00186805\n",
      "Train Epoch: 16 ... Batch: 200 ... Loss: 0.00201186\n",
      "------------------- Test set: Average loss: 480.5089 ... Samples: 810\n",
      "Train Epoch: 17 ... Batch: 0 ... Loss: 0.00215153\n",
      "Train Epoch: 17 ... Batch: 10 ... Loss: 0.00174539\n",
      "Train Epoch: 17 ... Batch: 20 ... Loss: 0.00206888\n",
      "Train Epoch: 17 ... Batch: 30 ... Loss: 0.00194983\n",
      "Train Epoch: 17 ... Batch: 40 ... Loss: 0.00204851\n",
      "Train Epoch: 17 ... Batch: 50 ... Loss: 0.00176350\n",
      "Train Epoch: 17 ... Batch: 60 ... Loss: 0.00173140\n",
      "Train Epoch: 17 ... Batch: 70 ... Loss: 0.00165418\n",
      "Train Epoch: 17 ... Batch: 80 ... Loss: 0.00191990\n",
      "Train Epoch: 17 ... Batch: 90 ... Loss: 0.00192212\n",
      "Train Epoch: 17 ... Batch: 100 ... Loss: 0.00202126\n",
      "Train Epoch: 17 ... Batch: 110 ... Loss: 0.00195377\n",
      "Train Epoch: 17 ... Batch: 120 ... Loss: 0.00166010\n",
      "Train Epoch: 17 ... Batch: 130 ... Loss: 0.00212007\n",
      "Train Epoch: 17 ... Batch: 140 ... Loss: 0.00184882\n",
      "Train Epoch: 17 ... Batch: 150 ... Loss: 0.00189501\n",
      "Train Epoch: 17 ... Batch: 160 ... Loss: 0.00194567\n",
      "Train Epoch: 17 ... Batch: 170 ... Loss: 0.00205200\n",
      "Train Epoch: 17 ... Batch: 180 ... Loss: 0.00199465\n",
      "Train Epoch: 17 ... Batch: 190 ... Loss: 0.00250860\n",
      "Train Epoch: 17 ... Batch: 200 ... Loss: 0.00189756\n",
      "------------------- Test set: Average loss: 480.2353 ... Samples: 810\n",
      "Train Epoch: 18 ... Batch: 0 ... Loss: 0.00168738\n",
      "Train Epoch: 18 ... Batch: 10 ... Loss: 0.00205482\n",
      "Train Epoch: 18 ... Batch: 20 ... Loss: 0.00194150\n",
      "Train Epoch: 18 ... Batch: 30 ... Loss: 0.00185874\n",
      "Train Epoch: 18 ... Batch: 40 ... Loss: 0.00162190\n",
      "Train Epoch: 18 ... Batch: 50 ... Loss: 0.00201639\n",
      "Train Epoch: 18 ... Batch: 60 ... Loss: 0.00241922\n",
      "Train Epoch: 18 ... Batch: 70 ... Loss: 0.00205465\n",
      "Train Epoch: 18 ... Batch: 80 ... Loss: 0.00193893\n",
      "Train Epoch: 18 ... Batch: 90 ... Loss: 0.00189366\n",
      "Train Epoch: 18 ... Batch: 100 ... Loss: 0.00181460\n",
      "Train Epoch: 18 ... Batch: 110 ... Loss: 0.00218442\n",
      "Train Epoch: 18 ... Batch: 120 ... Loss: 0.00207596\n",
      "Train Epoch: 18 ... Batch: 130 ... Loss: 0.00203243\n",
      "Train Epoch: 18 ... Batch: 140 ... Loss: 0.00182210\n",
      "Train Epoch: 18 ... Batch: 150 ... Loss: 0.00215740\n",
      "Train Epoch: 18 ... Batch: 160 ... Loss: 0.00181173\n",
      "Train Epoch: 18 ... Batch: 170 ... Loss: 0.00162978\n",
      "Train Epoch: 18 ... Batch: 180 ... Loss: 0.00194884\n",
      "Train Epoch: 18 ... Batch: 190 ... Loss: 0.00224495\n",
      "Train Epoch: 18 ... Batch: 200 ... Loss: 0.00216511\n",
      "------------------- Test set: Average loss: 480.2379 ... Samples: 810\n",
      "Train Epoch: 19 ... Batch: 0 ... Loss: 0.00186774\n",
      "Train Epoch: 19 ... Batch: 10 ... Loss: 0.00201733\n",
      "Train Epoch: 19 ... Batch: 20 ... Loss: 0.00184348\n",
      "Train Epoch: 19 ... Batch: 30 ... Loss: 0.00202628\n",
      "Train Epoch: 19 ... Batch: 40 ... Loss: 0.00205625\n",
      "Train Epoch: 19 ... Batch: 50 ... Loss: 0.00191270\n",
      "Train Epoch: 19 ... Batch: 60 ... Loss: 0.00197247\n",
      "Train Epoch: 19 ... Batch: 70 ... Loss: 0.00170196\n",
      "Train Epoch: 19 ... Batch: 80 ... Loss: 0.00214428\n",
      "Train Epoch: 19 ... Batch: 90 ... Loss: 0.00205297\n",
      "Train Epoch: 19 ... Batch: 100 ... Loss: 0.00201389\n",
      "Train Epoch: 19 ... Batch: 110 ... Loss: 0.00209874\n",
      "Train Epoch: 19 ... Batch: 120 ... Loss: 0.00188500\n",
      "Train Epoch: 19 ... Batch: 130 ... Loss: 0.00188161\n",
      "Train Epoch: 19 ... Batch: 140 ... Loss: 0.00182614\n",
      "Train Epoch: 19 ... Batch: 150 ... Loss: 0.00173126\n",
      "Train Epoch: 19 ... Batch: 160 ... Loss: 0.00211139\n",
      "Train Epoch: 19 ... Batch: 170 ... Loss: 0.00197191\n",
      "Train Epoch: 19 ... Batch: 180 ... Loss: 0.00200715\n",
      "Train Epoch: 19 ... Batch: 190 ... Loss: 0.00201732\n",
      "Train Epoch: 19 ... Batch: 200 ... Loss: 0.00175755\n",
      "------------------- Test set: Average loss: 480.1760 ... Samples: 810\n",
      "Train Epoch: 20 ... Batch: 0 ... Loss: 0.00210123\n",
      "Train Epoch: 20 ... Batch: 10 ... Loss: 0.00180767\n",
      "Train Epoch: 20 ... Batch: 20 ... Loss: 0.00213894\n",
      "Train Epoch: 20 ... Batch: 30 ... Loss: 0.00173661\n",
      "Train Epoch: 20 ... Batch: 40 ... Loss: 0.00164735\n",
      "Train Epoch: 20 ... Batch: 50 ... Loss: 0.00199536\n",
      "Train Epoch: 20 ... Batch: 60 ... Loss: 0.00199052\n",
      "Train Epoch: 20 ... Batch: 70 ... Loss: 0.00173099\n",
      "Train Epoch: 20 ... Batch: 80 ... Loss: 0.00198147\n",
      "Train Epoch: 20 ... Batch: 90 ... Loss: 0.00221197\n",
      "Train Epoch: 20 ... Batch: 100 ... Loss: 0.00172792\n",
      "Train Epoch: 20 ... Batch: 110 ... Loss: 0.00198561\n",
      "Train Epoch: 20 ... Batch: 120 ... Loss: 0.00219381\n",
      "Train Epoch: 20 ... Batch: 130 ... Loss: 0.00207123\n",
      "Train Epoch: 20 ... Batch: 140 ... Loss: 0.00207272\n",
      "Train Epoch: 20 ... Batch: 150 ... Loss: 0.00185249\n",
      "Train Epoch: 20 ... Batch: 160 ... Loss: 0.00178402\n",
      "Train Epoch: 20 ... Batch: 170 ... Loss: 0.00172754\n",
      "Train Epoch: 20 ... Batch: 180 ... Loss: 0.00216398\n",
      "Train Epoch: 20 ... Batch: 190 ... Loss: 0.00182339\n",
      "Train Epoch: 20 ... Batch: 200 ... Loss: 0.00190786\n",
      "------------------- Test set: Average loss: 480.1317 ... Samples: 810\n",
      "Train Epoch: 21 ... Batch: 0 ... Loss: 0.00195492\n",
      "Train Epoch: 21 ... Batch: 10 ... Loss: 0.00205002\n",
      "Train Epoch: 21 ... Batch: 20 ... Loss: 0.00193269\n",
      "Train Epoch: 21 ... Batch: 30 ... Loss: 0.00189821\n",
      "Train Epoch: 21 ... Batch: 40 ... Loss: 0.00170332\n",
      "Train Epoch: 21 ... Batch: 50 ... Loss: 0.00187150\n",
      "Train Epoch: 21 ... Batch: 60 ... Loss: 0.00188984\n",
      "Train Epoch: 21 ... Batch: 70 ... Loss: 0.00174178\n",
      "Train Epoch: 21 ... Batch: 80 ... Loss: 0.00216570\n",
      "Train Epoch: 21 ... Batch: 90 ... Loss: 0.00190211\n",
      "Train Epoch: 21 ... Batch: 100 ... Loss: 0.00192815\n",
      "Train Epoch: 21 ... Batch: 110 ... Loss: 0.00199920\n",
      "Train Epoch: 21 ... Batch: 120 ... Loss: 0.00201034\n",
      "Train Epoch: 21 ... Batch: 130 ... Loss: 0.00163727\n",
      "Train Epoch: 21 ... Batch: 140 ... Loss: 0.00186603\n",
      "Train Epoch: 21 ... Batch: 150 ... Loss: 0.00192559\n",
      "Train Epoch: 21 ... Batch: 160 ... Loss: 0.00189267\n",
      "Train Epoch: 21 ... Batch: 170 ... Loss: 0.00186953\n",
      "Train Epoch: 21 ... Batch: 180 ... Loss: 0.00176361\n",
      "Train Epoch: 21 ... Batch: 190 ... Loss: 0.00178552\n",
      "Train Epoch: 21 ... Batch: 200 ... Loss: 0.00214203\n",
      "------------------- Test set: Average loss: 479.5941 ... Samples: 810\n",
      "Train Epoch: 22 ... Batch: 0 ... Loss: 0.00190009\n",
      "Train Epoch: 22 ... Batch: 10 ... Loss: 0.00209612\n",
      "Train Epoch: 22 ... Batch: 20 ... Loss: 0.00205436\n",
      "Train Epoch: 22 ... Batch: 30 ... Loss: 0.00188369\n",
      "Train Epoch: 22 ... Batch: 40 ... Loss: 0.00196384\n",
      "Train Epoch: 22 ... Batch: 50 ... Loss: 0.00209103\n",
      "Train Epoch: 22 ... Batch: 60 ... Loss: 0.00207188\n",
      "Train Epoch: 22 ... Batch: 70 ... Loss: 0.00208587\n",
      "Train Epoch: 22 ... Batch: 80 ... Loss: 0.00211708\n",
      "Train Epoch: 22 ... Batch: 90 ... Loss: 0.00179035\n",
      "Train Epoch: 22 ... Batch: 100 ... Loss: 0.00256832\n",
      "Train Epoch: 22 ... Batch: 110 ... Loss: 0.00201867\n",
      "Train Epoch: 22 ... Batch: 120 ... Loss: 0.00166941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 ... Batch: 130 ... Loss: 0.00232091\n",
      "Train Epoch: 22 ... Batch: 140 ... Loss: 0.00192825\n",
      "Train Epoch: 22 ... Batch: 150 ... Loss: 0.00200101\n",
      "Train Epoch: 22 ... Batch: 160 ... Loss: 0.00201952\n",
      "Train Epoch: 22 ... Batch: 170 ... Loss: 0.00189523\n",
      "Train Epoch: 22 ... Batch: 180 ... Loss: 0.00178960\n",
      "Train Epoch: 22 ... Batch: 190 ... Loss: 0.00194504\n",
      "Train Epoch: 22 ... Batch: 200 ... Loss: 0.00197283\n",
      "------------------- Test set: Average loss: 479.4202 ... Samples: 810\n",
      "Train Epoch: 23 ... Batch: 0 ... Loss: 0.00217143\n",
      "Train Epoch: 23 ... Batch: 10 ... Loss: 0.00192500\n",
      "Train Epoch: 23 ... Batch: 20 ... Loss: 0.00184244\n",
      "Train Epoch: 23 ... Batch: 30 ... Loss: 0.00217367\n",
      "Train Epoch: 23 ... Batch: 40 ... Loss: 0.00175639\n",
      "Train Epoch: 23 ... Batch: 50 ... Loss: 0.00219292\n",
      "Train Epoch: 23 ... Batch: 60 ... Loss: 0.00200819\n",
      "Train Epoch: 23 ... Batch: 70 ... Loss: 0.00189429\n",
      "Train Epoch: 23 ... Batch: 80 ... Loss: 0.00198876\n",
      "Train Epoch: 23 ... Batch: 90 ... Loss: 0.00180397\n",
      "Train Epoch: 23 ... Batch: 100 ... Loss: 0.00178848\n",
      "Train Epoch: 23 ... Batch: 110 ... Loss: 0.00191401\n",
      "Train Epoch: 23 ... Batch: 120 ... Loss: 0.00205100\n",
      "Train Epoch: 23 ... Batch: 130 ... Loss: 0.00163372\n",
      "Train Epoch: 23 ... Batch: 140 ... Loss: 0.00236028\n",
      "Train Epoch: 23 ... Batch: 150 ... Loss: 0.00192446\n",
      "Train Epoch: 23 ... Batch: 160 ... Loss: 0.00218898\n",
      "Train Epoch: 23 ... Batch: 170 ... Loss: 0.00182614\n",
      "Train Epoch: 23 ... Batch: 180 ... Loss: 0.00201403\n",
      "Train Epoch: 23 ... Batch: 190 ... Loss: 0.00193834\n",
      "Train Epoch: 23 ... Batch: 200 ... Loss: 0.00210502\n",
      "------------------- Test set: Average loss: 479.4323 ... Samples: 810\n",
      "Train Epoch: 24 ... Batch: 0 ... Loss: 0.00176487\n",
      "Train Epoch: 24 ... Batch: 10 ... Loss: 0.00179130\n",
      "Train Epoch: 24 ... Batch: 20 ... Loss: 0.00187535\n",
      "Train Epoch: 24 ... Batch: 30 ... Loss: 0.00215987\n",
      "Train Epoch: 24 ... Batch: 40 ... Loss: 0.00192115\n",
      "Train Epoch: 24 ... Batch: 50 ... Loss: 0.00219210\n",
      "Train Epoch: 24 ... Batch: 60 ... Loss: 0.00189326\n",
      "Train Epoch: 24 ... Batch: 70 ... Loss: 0.00183422\n",
      "Train Epoch: 24 ... Batch: 80 ... Loss: 0.00192052\n",
      "Train Epoch: 24 ... Batch: 90 ... Loss: 0.00201903\n",
      "Train Epoch: 24 ... Batch: 100 ... Loss: 0.00192120\n",
      "Train Epoch: 24 ... Batch: 110 ... Loss: 0.00168247\n",
      "Train Epoch: 24 ... Batch: 120 ... Loss: 0.00225273\n",
      "Train Epoch: 24 ... Batch: 130 ... Loss: 0.00212470\n",
      "Train Epoch: 24 ... Batch: 140 ... Loss: 0.00213282\n",
      "Train Epoch: 24 ... Batch: 150 ... Loss: 0.00223395\n",
      "Train Epoch: 24 ... Batch: 160 ... Loss: 0.00203528\n",
      "Train Epoch: 24 ... Batch: 170 ... Loss: 0.00203240\n",
      "Train Epoch: 24 ... Batch: 180 ... Loss: 0.00209081\n",
      "Train Epoch: 24 ... Batch: 190 ... Loss: 0.00213832\n",
      "Train Epoch: 24 ... Batch: 200 ... Loss: 0.00205614\n",
      "------------------- Test set: Average loss: 479.3887 ... Samples: 810\n",
      "Train Epoch: 25 ... Batch: 0 ... Loss: 0.00198183\n",
      "Train Epoch: 25 ... Batch: 10 ... Loss: 0.00216364\n",
      "Train Epoch: 25 ... Batch: 20 ... Loss: 0.00204638\n",
      "Train Epoch: 25 ... Batch: 30 ... Loss: 0.00203583\n",
      "Train Epoch: 25 ... Batch: 40 ... Loss: 0.00167016\n",
      "Train Epoch: 25 ... Batch: 50 ... Loss: 0.00215259\n",
      "Train Epoch: 25 ... Batch: 60 ... Loss: 0.00176199\n",
      "Train Epoch: 25 ... Batch: 70 ... Loss: 0.00211735\n",
      "Train Epoch: 25 ... Batch: 80 ... Loss: 0.00190102\n",
      "Train Epoch: 25 ... Batch: 90 ... Loss: 0.00197950\n",
      "Train Epoch: 25 ... Batch: 100 ... Loss: 0.00180713\n",
      "Train Epoch: 25 ... Batch: 110 ... Loss: 0.00210430\n",
      "Train Epoch: 25 ... Batch: 120 ... Loss: 0.00208429\n",
      "Train Epoch: 25 ... Batch: 130 ... Loss: 0.00174529\n",
      "Train Epoch: 25 ... Batch: 140 ... Loss: 0.00209401\n",
      "Train Epoch: 25 ... Batch: 150 ... Loss: 0.00166609\n",
      "Train Epoch: 25 ... Batch: 160 ... Loss: 0.00192589\n",
      "Train Epoch: 25 ... Batch: 170 ... Loss: 0.00180955\n",
      "Train Epoch: 25 ... Batch: 180 ... Loss: 0.00229027\n",
      "Train Epoch: 25 ... Batch: 190 ... Loss: 0.00201337\n",
      "Train Epoch: 25 ... Batch: 200 ... Loss: 0.00185153\n",
      "------------------- Test set: Average loss: 479.4211 ... Samples: 810\n",
      "Train Epoch: 26 ... Batch: 0 ... Loss: 0.00201221\n",
      "Train Epoch: 26 ... Batch: 10 ... Loss: 0.00219798\n",
      "Train Epoch: 26 ... Batch: 20 ... Loss: 0.00215141\n",
      "Train Epoch: 26 ... Batch: 30 ... Loss: 0.00178067\n",
      "Train Epoch: 26 ... Batch: 40 ... Loss: 0.00188663\n",
      "Train Epoch: 26 ... Batch: 50 ... Loss: 0.00223900\n",
      "Train Epoch: 26 ... Batch: 60 ... Loss: 0.00209989\n",
      "Train Epoch: 26 ... Batch: 70 ... Loss: 0.00219353\n",
      "Train Epoch: 26 ... Batch: 80 ... Loss: 0.00185336\n",
      "Train Epoch: 26 ... Batch: 90 ... Loss: 0.00206700\n",
      "Train Epoch: 26 ... Batch: 100 ... Loss: 0.00209153\n",
      "Train Epoch: 26 ... Batch: 110 ... Loss: 0.00179120\n",
      "Train Epoch: 26 ... Batch: 120 ... Loss: 0.00181294\n",
      "Train Epoch: 26 ... Batch: 130 ... Loss: 0.00179359\n",
      "Train Epoch: 26 ... Batch: 140 ... Loss: 0.00204627\n",
      "Train Epoch: 26 ... Batch: 150 ... Loss: 0.00213247\n",
      "Train Epoch: 26 ... Batch: 160 ... Loss: 0.00176370\n",
      "Train Epoch: 26 ... Batch: 170 ... Loss: 0.00214804\n",
      "Train Epoch: 26 ... Batch: 180 ... Loss: 0.00195938\n",
      "Train Epoch: 26 ... Batch: 190 ... Loss: 0.00190067\n",
      "Train Epoch: 26 ... Batch: 200 ... Loss: 0.00219009\n",
      "------------------- Test set: Average loss: 478.9695 ... Samples: 810\n",
      "Train Epoch: 27 ... Batch: 0 ... Loss: 0.00196994\n",
      "Train Epoch: 27 ... Batch: 10 ... Loss: 0.00190277\n",
      "Train Epoch: 27 ... Batch: 20 ... Loss: 0.00231190\n",
      "Train Epoch: 27 ... Batch: 30 ... Loss: 0.00217123\n",
      "Train Epoch: 27 ... Batch: 40 ... Loss: 0.00175937\n",
      "Train Epoch: 27 ... Batch: 50 ... Loss: 0.00183818\n",
      "Train Epoch: 27 ... Batch: 60 ... Loss: 0.00221470\n",
      "Train Epoch: 27 ... Batch: 70 ... Loss: 0.00178280\n",
      "Train Epoch: 27 ... Batch: 80 ... Loss: 0.00171570\n",
      "Train Epoch: 27 ... Batch: 90 ... Loss: 0.00208270\n",
      "Train Epoch: 27 ... Batch: 100 ... Loss: 0.00177695\n",
      "Train Epoch: 27 ... Batch: 110 ... Loss: 0.00167208\n",
      "Train Epoch: 27 ... Batch: 120 ... Loss: 0.00209312\n",
      "Train Epoch: 27 ... Batch: 130 ... Loss: 0.00209325\n",
      "Train Epoch: 27 ... Batch: 140 ... Loss: 0.00231675\n",
      "Train Epoch: 27 ... Batch: 150 ... Loss: 0.00204212\n",
      "Train Epoch: 27 ... Batch: 160 ... Loss: 0.00210352\n",
      "Train Epoch: 27 ... Batch: 170 ... Loss: 0.00218140\n",
      "Train Epoch: 27 ... Batch: 180 ... Loss: 0.00216225\n",
      "Train Epoch: 27 ... Batch: 190 ... Loss: 0.00216392\n",
      "Train Epoch: 27 ... Batch: 200 ... Loss: 0.00201467\n",
      "------------------- Test set: Average loss: 478.7526 ... Samples: 810\n",
      "Train Epoch: 28 ... Batch: 0 ... Loss: 0.00198127\n",
      "Train Epoch: 28 ... Batch: 10 ... Loss: 0.00188871\n",
      "Train Epoch: 28 ... Batch: 20 ... Loss: 0.00189419\n",
      "Train Epoch: 28 ... Batch: 30 ... Loss: 0.00197917\n",
      "Train Epoch: 28 ... Batch: 40 ... Loss: 0.00188687\n",
      "Train Epoch: 28 ... Batch: 50 ... Loss: 0.00196893\n",
      "Train Epoch: 28 ... Batch: 60 ... Loss: 0.00206236\n",
      "Train Epoch: 28 ... Batch: 70 ... Loss: 0.00161978\n",
      "Train Epoch: 28 ... Batch: 80 ... Loss: 0.00217681\n",
      "Train Epoch: 28 ... Batch: 90 ... Loss: 0.00187544\n",
      "Train Epoch: 28 ... Batch: 100 ... Loss: 0.00177457\n",
      "Train Epoch: 28 ... Batch: 110 ... Loss: 0.00185379\n",
      "Train Epoch: 28 ... Batch: 120 ... Loss: 0.00175709\n",
      "Train Epoch: 28 ... Batch: 130 ... Loss: 0.00174608\n",
      "Train Epoch: 28 ... Batch: 140 ... Loss: 0.00220728\n",
      "Train Epoch: 28 ... Batch: 150 ... Loss: 0.00192963\n",
      "Train Epoch: 28 ... Batch: 160 ... Loss: 0.00183469\n",
      "Train Epoch: 28 ... Batch: 170 ... Loss: 0.00200462\n",
      "Train Epoch: 28 ... Batch: 180 ... Loss: 0.00184554\n",
      "Train Epoch: 28 ... Batch: 190 ... Loss: 0.00198947\n",
      "Train Epoch: 28 ... Batch: 200 ... Loss: 0.00212856\n",
      "------------------- Test set: Average loss: 478.6155 ... Samples: 810\n",
      "Train Epoch: 29 ... Batch: 0 ... Loss: 0.00173166\n",
      "Train Epoch: 29 ... Batch: 10 ... Loss: 0.00203663\n",
      "Train Epoch: 29 ... Batch: 20 ... Loss: 0.00210778\n",
      "Train Epoch: 29 ... Batch: 30 ... Loss: 0.00200899\n",
      "Train Epoch: 29 ... Batch: 40 ... Loss: 0.00200910\n",
      "Train Epoch: 29 ... Batch: 50 ... Loss: 0.00172678\n",
      "Train Epoch: 29 ... Batch: 60 ... Loss: 0.00172738\n",
      "Train Epoch: 29 ... Batch: 70 ... Loss: 0.00196952\n",
      "Train Epoch: 29 ... Batch: 80 ... Loss: 0.00209595\n",
      "Train Epoch: 29 ... Batch: 90 ... Loss: 0.00138676\n",
      "Train Epoch: 29 ... Batch: 100 ... Loss: 0.00197669\n",
      "Train Epoch: 29 ... Batch: 110 ... Loss: 0.00216514\n",
      "Train Epoch: 29 ... Batch: 120 ... Loss: 0.00192858\n",
      "Train Epoch: 29 ... Batch: 130 ... Loss: 0.00197076\n",
      "Train Epoch: 29 ... Batch: 140 ... Loss: 0.00180274\n",
      "Train Epoch: 29 ... Batch: 150 ... Loss: 0.00162418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 ... Batch: 160 ... Loss: 0.00210745\n",
      "Train Epoch: 29 ... Batch: 170 ... Loss: 0.00193539\n",
      "Train Epoch: 29 ... Batch: 180 ... Loss: 0.00201943\n",
      "Train Epoch: 29 ... Batch: 190 ... Loss: 0.00182212\n",
      "Train Epoch: 29 ... Batch: 200 ... Loss: 0.00167982\n",
      "------------------- Test set: Average loss: 478.6121 ... Samples: 810\n",
      "Train Epoch: 30 ... Batch: 0 ... Loss: 0.00193108\n",
      "Train Epoch: 30 ... Batch: 10 ... Loss: 0.00189113\n",
      "Train Epoch: 30 ... Batch: 20 ... Loss: 0.00209585\n",
      "Train Epoch: 30 ... Batch: 30 ... Loss: 0.00183768\n",
      "Train Epoch: 30 ... Batch: 40 ... Loss: 0.00184036\n",
      "Train Epoch: 30 ... Batch: 50 ... Loss: 0.00215095\n",
      "Train Epoch: 30 ... Batch: 60 ... Loss: 0.00168528\n",
      "Train Epoch: 30 ... Batch: 70 ... Loss: 0.00174733\n",
      "Train Epoch: 30 ... Batch: 80 ... Loss: 0.00177619\n",
      "Train Epoch: 30 ... Batch: 90 ... Loss: 0.00203009\n",
      "Train Epoch: 30 ... Batch: 100 ... Loss: 0.00183849\n",
      "Train Epoch: 30 ... Batch: 110 ... Loss: 0.00187747\n",
      "Train Epoch: 30 ... Batch: 120 ... Loss: 0.00207539\n",
      "Train Epoch: 30 ... Batch: 130 ... Loss: 0.00210737\n",
      "Train Epoch: 30 ... Batch: 140 ... Loss: 0.00181596\n",
      "Train Epoch: 30 ... Batch: 150 ... Loss: 0.00204641\n",
      "Train Epoch: 30 ... Batch: 160 ... Loss: 0.00194775\n",
      "Train Epoch: 30 ... Batch: 170 ... Loss: 0.00205657\n",
      "Train Epoch: 30 ... Batch: 180 ... Loss: 0.00200674\n",
      "Train Epoch: 30 ... Batch: 190 ... Loss: 0.00173103\n",
      "Train Epoch: 30 ... Batch: 200 ... Loss: 0.00198321\n",
      "------------------- Test set: Average loss: 479.4522 ... Samples: 810\n",
      "Train Epoch: 31 ... Batch: 0 ... Loss: 0.00167543\n",
      "Train Epoch: 31 ... Batch: 10 ... Loss: 0.00161213\n",
      "Train Epoch: 31 ... Batch: 20 ... Loss: 0.00202927\n",
      "Train Epoch: 31 ... Batch: 30 ... Loss: 0.00174302\n",
      "Train Epoch: 31 ... Batch: 40 ... Loss: 0.00197954\n",
      "Train Epoch: 31 ... Batch: 50 ... Loss: 0.00219845\n",
      "Train Epoch: 31 ... Batch: 60 ... Loss: 0.00183455\n",
      "Train Epoch: 31 ... Batch: 70 ... Loss: 0.00204079\n",
      "Train Epoch: 31 ... Batch: 80 ... Loss: 0.00201486\n",
      "Train Epoch: 31 ... Batch: 90 ... Loss: 0.00181067\n",
      "Train Epoch: 31 ... Batch: 100 ... Loss: 0.00191509\n",
      "Train Epoch: 31 ... Batch: 110 ... Loss: 0.00170968\n",
      "Train Epoch: 31 ... Batch: 120 ... Loss: 0.00158644\n",
      "Train Epoch: 31 ... Batch: 130 ... Loss: 0.00242446\n",
      "Train Epoch: 31 ... Batch: 140 ... Loss: 0.00224109\n",
      "Train Epoch: 31 ... Batch: 150 ... Loss: 0.00210484\n",
      "Train Epoch: 31 ... Batch: 160 ... Loss: 0.00191279\n",
      "Train Epoch: 31 ... Batch: 170 ... Loss: 0.00204319\n",
      "Train Epoch: 31 ... Batch: 180 ... Loss: 0.00204725\n",
      "Train Epoch: 31 ... Batch: 190 ... Loss: 0.00197602\n",
      "Train Epoch: 31 ... Batch: 200 ... Loss: 0.00226472\n",
      "------------------- Test set: Average loss: 477.9865 ... Samples: 810\n",
      "Train Epoch: 32 ... Batch: 0 ... Loss: 0.00167550\n",
      "Train Epoch: 32 ... Batch: 10 ... Loss: 0.00186871\n",
      "Train Epoch: 32 ... Batch: 20 ... Loss: 0.00201297\n",
      "Train Epoch: 32 ... Batch: 30 ... Loss: 0.00170680\n",
      "Train Epoch: 32 ... Batch: 40 ... Loss: 0.00219403\n",
      "Train Epoch: 32 ... Batch: 50 ... Loss: 0.00179501\n",
      "Train Epoch: 32 ... Batch: 60 ... Loss: 0.00193497\n",
      "Train Epoch: 32 ... Batch: 70 ... Loss: 0.00206507\n",
      "Train Epoch: 32 ... Batch: 80 ... Loss: 0.00196171\n",
      "Train Epoch: 32 ... Batch: 90 ... Loss: 0.00200021\n",
      "Train Epoch: 32 ... Batch: 100 ... Loss: 0.00195056\n",
      "Train Epoch: 32 ... Batch: 110 ... Loss: 0.00191379\n",
      "Train Epoch: 32 ... Batch: 120 ... Loss: 0.00211464\n",
      "Train Epoch: 32 ... Batch: 130 ... Loss: 0.00197374\n",
      "Train Epoch: 32 ... Batch: 140 ... Loss: 0.00196757\n",
      "Train Epoch: 32 ... Batch: 150 ... Loss: 0.00167823\n",
      "Train Epoch: 32 ... Batch: 160 ... Loss: 0.00191956\n",
      "Train Epoch: 32 ... Batch: 170 ... Loss: 0.00187221\n",
      "Train Epoch: 32 ... Batch: 180 ... Loss: 0.00177313\n",
      "Train Epoch: 32 ... Batch: 190 ... Loss: 0.00212276\n",
      "Train Epoch: 32 ... Batch: 200 ... Loss: 0.00194907\n",
      "------------------- Test set: Average loss: 477.6927 ... Samples: 810\n",
      "Train Epoch: 33 ... Batch: 0 ... Loss: 0.00165700\n",
      "Train Epoch: 33 ... Batch: 10 ... Loss: 0.00171604\n",
      "Train Epoch: 33 ... Batch: 20 ... Loss: 0.00184453\n",
      "Train Epoch: 33 ... Batch: 30 ... Loss: 0.00163810\n",
      "Train Epoch: 33 ... Batch: 40 ... Loss: 0.00215596\n",
      "Train Epoch: 33 ... Batch: 50 ... Loss: 0.00201207\n",
      "Train Epoch: 33 ... Batch: 60 ... Loss: 0.00201210\n",
      "Train Epoch: 33 ... Batch: 70 ... Loss: 0.00195464\n",
      "Train Epoch: 33 ... Batch: 80 ... Loss: 0.00172474\n",
      "Train Epoch: 33 ... Batch: 90 ... Loss: 0.00197614\n",
      "Train Epoch: 33 ... Batch: 100 ... Loss: 0.00167080\n",
      "Train Epoch: 33 ... Batch: 110 ... Loss: 0.00188185\n",
      "Train Epoch: 33 ... Batch: 120 ... Loss: 0.00201398\n",
      "Train Epoch: 33 ... Batch: 130 ... Loss: 0.00199971\n",
      "Train Epoch: 33 ... Batch: 140 ... Loss: 0.00193903\n",
      "Train Epoch: 33 ... Batch: 150 ... Loss: 0.00172401\n",
      "Train Epoch: 33 ... Batch: 160 ... Loss: 0.00192153\n",
      "Train Epoch: 33 ... Batch: 170 ... Loss: 0.00192515\n",
      "Train Epoch: 33 ... Batch: 180 ... Loss: 0.00171100\n",
      "Train Epoch: 33 ... Batch: 190 ... Loss: 0.00197412\n",
      "Train Epoch: 33 ... Batch: 200 ... Loss: 0.00211614\n",
      "------------------- Test set: Average loss: 477.6078 ... Samples: 810\n",
      "Train Epoch: 34 ... Batch: 0 ... Loss: 0.00201793\n",
      "Train Epoch: 34 ... Batch: 10 ... Loss: 0.00198783\n",
      "Train Epoch: 34 ... Batch: 20 ... Loss: 0.00147938\n",
      "Train Epoch: 34 ... Batch: 30 ... Loss: 0.00188303\n",
      "Train Epoch: 34 ... Batch: 40 ... Loss: 0.00185127\n",
      "Train Epoch: 34 ... Batch: 50 ... Loss: 0.00180275\n",
      "Train Epoch: 34 ... Batch: 60 ... Loss: 0.00206369\n",
      "Train Epoch: 34 ... Batch: 70 ... Loss: 0.00165808\n",
      "Train Epoch: 34 ... Batch: 80 ... Loss: 0.00185103\n",
      "Train Epoch: 34 ... Batch: 90 ... Loss: 0.00208341\n",
      "Train Epoch: 34 ... Batch: 100 ... Loss: 0.00202511\n",
      "Train Epoch: 34 ... Batch: 110 ... Loss: 0.00161571\n",
      "Train Epoch: 34 ... Batch: 120 ... Loss: 0.00191277\n",
      "Train Epoch: 34 ... Batch: 130 ... Loss: 0.00191912\n",
      "Train Epoch: 34 ... Batch: 140 ... Loss: 0.00178189\n",
      "Train Epoch: 34 ... Batch: 150 ... Loss: 0.00181278\n",
      "Train Epoch: 34 ... Batch: 160 ... Loss: 0.00196617\n",
      "Train Epoch: 34 ... Batch: 170 ... Loss: 0.00156248\n",
      "Train Epoch: 34 ... Batch: 180 ... Loss: 0.00153121\n",
      "Train Epoch: 34 ... Batch: 190 ... Loss: 0.00231000\n",
      "Train Epoch: 34 ... Batch: 200 ... Loss: 0.00198744\n",
      "------------------- Test set: Average loss: 477.6879 ... Samples: 810\n",
      "Train Epoch: 35 ... Batch: 0 ... Loss: 0.00192844\n",
      "Train Epoch: 35 ... Batch: 10 ... Loss: 0.00218452\n",
      "Train Epoch: 35 ... Batch: 20 ... Loss: 0.00187424\n",
      "Train Epoch: 35 ... Batch: 30 ... Loss: 0.00181059\n",
      "Train Epoch: 35 ... Batch: 40 ... Loss: 0.00223738\n",
      "Train Epoch: 35 ... Batch: 50 ... Loss: 0.00222939\n",
      "Train Epoch: 35 ... Batch: 60 ... Loss: 0.00205193\n",
      "Train Epoch: 35 ... Batch: 70 ... Loss: 0.00181177\n",
      "Train Epoch: 35 ... Batch: 80 ... Loss: 0.00202297\n",
      "Train Epoch: 35 ... Batch: 90 ... Loss: 0.00188656\n",
      "Train Epoch: 35 ... Batch: 100 ... Loss: 0.00205595\n",
      "Train Epoch: 35 ... Batch: 110 ... Loss: 0.00201790\n",
      "Train Epoch: 35 ... Batch: 120 ... Loss: 0.00211076\n",
      "Train Epoch: 35 ... Batch: 130 ... Loss: 0.00207850\n",
      "Train Epoch: 35 ... Batch: 140 ... Loss: 0.00183249\n",
      "Train Epoch: 35 ... Batch: 150 ... Loss: 0.00199900\n",
      "Train Epoch: 35 ... Batch: 160 ... Loss: 0.00204497\n",
      "Train Epoch: 35 ... Batch: 170 ... Loss: 0.00221111\n",
      "Train Epoch: 35 ... Batch: 180 ... Loss: 0.00196994\n",
      "Train Epoch: 35 ... Batch: 190 ... Loss: 0.00178575\n",
      "Train Epoch: 35 ... Batch: 200 ... Loss: 0.00195232\n",
      "------------------- Test set: Average loss: 477.3997 ... Samples: 810\n",
      "Train Epoch: 36 ... Batch: 0 ... Loss: 0.00180466\n",
      "Train Epoch: 36 ... Batch: 10 ... Loss: 0.00186206\n",
      "Train Epoch: 36 ... Batch: 20 ... Loss: 0.00185236\n",
      "Train Epoch: 36 ... Batch: 30 ... Loss: 0.00199811\n",
      "Train Epoch: 36 ... Batch: 40 ... Loss: 0.00181039\n",
      "Train Epoch: 36 ... Batch: 50 ... Loss: 0.00186354\n",
      "Train Epoch: 36 ... Batch: 60 ... Loss: 0.00179114\n",
      "Train Epoch: 36 ... Batch: 70 ... Loss: 0.00178792\n",
      "Train Epoch: 36 ... Batch: 80 ... Loss: 0.00171326\n",
      "Train Epoch: 36 ... Batch: 90 ... Loss: 0.00186108\n",
      "Train Epoch: 36 ... Batch: 100 ... Loss: 0.00184448\n",
      "Train Epoch: 36 ... Batch: 110 ... Loss: 0.00196953\n",
      "Train Epoch: 36 ... Batch: 120 ... Loss: 0.00198281\n",
      "Train Epoch: 36 ... Batch: 130 ... Loss: 0.00150170\n",
      "Train Epoch: 36 ... Batch: 140 ... Loss: 0.00192455\n",
      "Train Epoch: 36 ... Batch: 150 ... Loss: 0.00191832\n",
      "Train Epoch: 36 ... Batch: 160 ... Loss: 0.00173424\n",
      "Train Epoch: 36 ... Batch: 170 ... Loss: 0.00211127\n",
      "Train Epoch: 36 ... Batch: 180 ... Loss: 0.00195464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 36 ... Batch: 190 ... Loss: 0.00206161\n",
      "Train Epoch: 36 ... Batch: 200 ... Loss: 0.00207772\n",
      "------------------- Test set: Average loss: 477.4248 ... Samples: 810\n",
      "Train Epoch: 37 ... Batch: 0 ... Loss: 0.00171119\n",
      "Train Epoch: 37 ... Batch: 10 ... Loss: 0.00173829\n",
      "Train Epoch: 37 ... Batch: 20 ... Loss: 0.00202285\n",
      "Train Epoch: 37 ... Batch: 30 ... Loss: 0.00180937\n",
      "Train Epoch: 37 ... Batch: 40 ... Loss: 0.00211642\n",
      "Train Epoch: 37 ... Batch: 50 ... Loss: 0.00173363\n",
      "Train Epoch: 37 ... Batch: 60 ... Loss: 0.00174633\n",
      "Train Epoch: 37 ... Batch: 70 ... Loss: 0.00202161\n",
      "Train Epoch: 37 ... Batch: 80 ... Loss: 0.00210192\n",
      "Train Epoch: 37 ... Batch: 90 ... Loss: 0.00183216\n",
      "Train Epoch: 37 ... Batch: 100 ... Loss: 0.00199999\n",
      "Train Epoch: 37 ... Batch: 110 ... Loss: 0.00179537\n",
      "Train Epoch: 37 ... Batch: 120 ... Loss: 0.00224361\n",
      "Train Epoch: 37 ... Batch: 130 ... Loss: 0.00208519\n",
      "Train Epoch: 37 ... Batch: 140 ... Loss: 0.00187884\n",
      "Train Epoch: 37 ... Batch: 150 ... Loss: 0.00188286\n",
      "Train Epoch: 37 ... Batch: 160 ... Loss: 0.00173891\n",
      "Train Epoch: 37 ... Batch: 170 ... Loss: 0.00209408\n",
      "Train Epoch: 37 ... Batch: 180 ... Loss: 0.00203476\n",
      "Train Epoch: 37 ... Batch: 190 ... Loss: 0.00202319\n",
      "Train Epoch: 37 ... Batch: 200 ... Loss: 0.00156777\n",
      "------------------- Test set: Average loss: 477.3865 ... Samples: 810\n",
      "Train Epoch: 38 ... Batch: 0 ... Loss: 0.00193734\n",
      "Train Epoch: 38 ... Batch: 10 ... Loss: 0.00191982\n",
      "Train Epoch: 38 ... Batch: 20 ... Loss: 0.00190338\n",
      "Train Epoch: 38 ... Batch: 30 ... Loss: 0.00212001\n",
      "Train Epoch: 38 ... Batch: 40 ... Loss: 0.00194908\n",
      "Train Epoch: 38 ... Batch: 50 ... Loss: 0.00182459\n",
      "Train Epoch: 38 ... Batch: 60 ... Loss: 0.00192397\n",
      "Train Epoch: 38 ... Batch: 70 ... Loss: 0.00212484\n",
      "Train Epoch: 38 ... Batch: 80 ... Loss: 0.00195631\n",
      "Train Epoch: 38 ... Batch: 90 ... Loss: 0.00181027\n",
      "Train Epoch: 38 ... Batch: 100 ... Loss: 0.00199989\n",
      "Train Epoch: 38 ... Batch: 110 ... Loss: 0.00201140\n",
      "Train Epoch: 38 ... Batch: 120 ... Loss: 0.00194562\n",
      "Train Epoch: 38 ... Batch: 130 ... Loss: 0.00221681\n",
      "Train Epoch: 38 ... Batch: 140 ... Loss: 0.00206977\n",
      "Train Epoch: 38 ... Batch: 150 ... Loss: 0.00199912\n",
      "Train Epoch: 38 ... Batch: 160 ... Loss: 0.00179776\n",
      "Train Epoch: 38 ... Batch: 170 ... Loss: 0.00168587\n",
      "Train Epoch: 38 ... Batch: 180 ... Loss: 0.00215263\n",
      "Train Epoch: 38 ... Batch: 190 ... Loss: 0.00209770\n",
      "Train Epoch: 38 ... Batch: 200 ... Loss: 0.00223455\n",
      "------------------- Test set: Average loss: 476.7702 ... Samples: 810\n",
      "Train Epoch: 39 ... Batch: 0 ... Loss: 0.00181440\n",
      "Train Epoch: 39 ... Batch: 10 ... Loss: 0.00181628\n",
      "Train Epoch: 39 ... Batch: 20 ... Loss: 0.00191939\n",
      "Train Epoch: 39 ... Batch: 30 ... Loss: 0.00188451\n",
      "Train Epoch: 39 ... Batch: 40 ... Loss: 0.00205908\n",
      "Train Epoch: 39 ... Batch: 50 ... Loss: 0.00219885\n",
      "Train Epoch: 39 ... Batch: 60 ... Loss: 0.00206960\n",
      "Train Epoch: 39 ... Batch: 70 ... Loss: 0.00175645\n",
      "Train Epoch: 39 ... Batch: 80 ... Loss: 0.00188611\n",
      "Train Epoch: 39 ... Batch: 90 ... Loss: 0.00190618\n",
      "Train Epoch: 39 ... Batch: 100 ... Loss: 0.00197661\n",
      "Train Epoch: 39 ... Batch: 110 ... Loss: 0.00194962\n",
      "Train Epoch: 39 ... Batch: 120 ... Loss: 0.00184860\n",
      "Train Epoch: 39 ... Batch: 130 ... Loss: 0.00211764\n",
      "Train Epoch: 39 ... Batch: 140 ... Loss: 0.00184125\n",
      "Train Epoch: 39 ... Batch: 150 ... Loss: 0.00179568\n",
      "Train Epoch: 39 ... Batch: 160 ... Loss: 0.00189159\n",
      "Train Epoch: 39 ... Batch: 170 ... Loss: 0.00199738\n",
      "Train Epoch: 39 ... Batch: 180 ... Loss: 0.00192206\n",
      "Train Epoch: 39 ... Batch: 190 ... Loss: 0.00180408\n",
      "Train Epoch: 39 ... Batch: 200 ... Loss: 0.00189474\n",
      "------------------- Test set: Average loss: 477.3044 ... Samples: 810\n",
      "Train Epoch: 40 ... Batch: 0 ... Loss: 0.00183041\n",
      "Train Epoch: 40 ... Batch: 10 ... Loss: 0.00168871\n",
      "Train Epoch: 40 ... Batch: 20 ... Loss: 0.00193209\n",
      "Train Epoch: 40 ... Batch: 30 ... Loss: 0.00184913\n",
      "Train Epoch: 40 ... Batch: 40 ... Loss: 0.00242728\n",
      "Train Epoch: 40 ... Batch: 50 ... Loss: 0.00190786\n",
      "Train Epoch: 40 ... Batch: 60 ... Loss: 0.00183707\n",
      "Train Epoch: 40 ... Batch: 70 ... Loss: 0.00202253\n",
      "Train Epoch: 40 ... Batch: 80 ... Loss: 0.00183623\n",
      "Train Epoch: 40 ... Batch: 90 ... Loss: 0.00175294\n",
      "Train Epoch: 40 ... Batch: 100 ... Loss: 0.00204150\n",
      "Train Epoch: 40 ... Batch: 110 ... Loss: 0.00185325\n",
      "Train Epoch: 40 ... Batch: 120 ... Loss: 0.00147371\n",
      "Train Epoch: 40 ... Batch: 130 ... Loss: 0.00188909\n",
      "Train Epoch: 40 ... Batch: 140 ... Loss: 0.00200949\n",
      "Train Epoch: 40 ... Batch: 150 ... Loss: 0.00174301\n",
      "Train Epoch: 40 ... Batch: 160 ... Loss: 0.00175899\n",
      "Train Epoch: 40 ... Batch: 170 ... Loss: 0.00222968\n",
      "Train Epoch: 40 ... Batch: 180 ... Loss: 0.00202474\n",
      "Train Epoch: 40 ... Batch: 190 ... Loss: 0.00210887\n",
      "Train Epoch: 40 ... Batch: 200 ... Loss: 0.00216477\n",
      "------------------- Test set: Average loss: 476.8895 ... Samples: 810\n",
      "Train Epoch: 41 ... Batch: 0 ... Loss: 0.00228968\n",
      "Train Epoch: 41 ... Batch: 10 ... Loss: 0.00187436\n",
      "Train Epoch: 41 ... Batch: 20 ... Loss: 0.00193441\n",
      "Train Epoch: 41 ... Batch: 30 ... Loss: 0.00173477\n",
      "Train Epoch: 41 ... Batch: 40 ... Loss: 0.00207202\n",
      "Train Epoch: 41 ... Batch: 50 ... Loss: 0.00205727\n",
      "Train Epoch: 41 ... Batch: 60 ... Loss: 0.00194476\n",
      "Train Epoch: 41 ... Batch: 70 ... Loss: 0.00189865\n",
      "Train Epoch: 41 ... Batch: 80 ... Loss: 0.00184085\n",
      "Train Epoch: 41 ... Batch: 90 ... Loss: 0.00173210\n",
      "Train Epoch: 41 ... Batch: 100 ... Loss: 0.00223671\n",
      "Train Epoch: 41 ... Batch: 110 ... Loss: 0.00205420\n",
      "Train Epoch: 41 ... Batch: 120 ... Loss: 0.00207469\n",
      "Train Epoch: 41 ... Batch: 130 ... Loss: 0.00194241\n",
      "Train Epoch: 41 ... Batch: 140 ... Loss: 0.00239525\n",
      "Train Epoch: 41 ... Batch: 150 ... Loss: 0.00183038\n",
      "Train Epoch: 41 ... Batch: 160 ... Loss: 0.00204885\n",
      "Train Epoch: 41 ... Batch: 170 ... Loss: 0.00218297\n",
      "Train Epoch: 41 ... Batch: 180 ... Loss: 0.00200184\n",
      "Train Epoch: 41 ... Batch: 190 ... Loss: 0.00172006\n",
      "Train Epoch: 41 ... Batch: 200 ... Loss: 0.00218047\n",
      "------------------- Test set: Average loss: 477.4104 ... Samples: 810\n",
      "Train Epoch: 42 ... Batch: 0 ... Loss: 0.00207516\n",
      "Train Epoch: 42 ... Batch: 10 ... Loss: 0.00202185\n",
      "Train Epoch: 42 ... Batch: 20 ... Loss: 0.00174704\n",
      "Train Epoch: 42 ... Batch: 30 ... Loss: 0.00174610\n",
      "Train Epoch: 42 ... Batch: 40 ... Loss: 0.00184102\n",
      "Train Epoch: 42 ... Batch: 50 ... Loss: 0.00204782\n",
      "Train Epoch: 42 ... Batch: 60 ... Loss: 0.00160541\n",
      "Train Epoch: 42 ... Batch: 70 ... Loss: 0.00177435\n",
      "Train Epoch: 42 ... Batch: 80 ... Loss: 0.00177602\n",
      "Train Epoch: 42 ... Batch: 90 ... Loss: 0.00171585\n",
      "Train Epoch: 42 ... Batch: 100 ... Loss: 0.00235777\n",
      "Train Epoch: 42 ... Batch: 110 ... Loss: 0.00208960\n",
      "Train Epoch: 42 ... Batch: 120 ... Loss: 0.00162366\n",
      "Train Epoch: 42 ... Batch: 130 ... Loss: 0.00178149\n",
      "Train Epoch: 42 ... Batch: 140 ... Loss: 0.00186776\n",
      "Train Epoch: 42 ... Batch: 150 ... Loss: 0.00211108\n",
      "Train Epoch: 42 ... Batch: 160 ... Loss: 0.00180202\n",
      "Train Epoch: 42 ... Batch: 170 ... Loss: 0.00179975\n",
      "Train Epoch: 42 ... Batch: 180 ... Loss: 0.00160603\n",
      "Train Epoch: 42 ... Batch: 190 ... Loss: 0.00186087\n",
      "Train Epoch: 42 ... Batch: 200 ... Loss: 0.00220248\n",
      "------------------- Test set: Average loss: 476.4489 ... Samples: 810\n",
      "Train Epoch: 43 ... Batch: 0 ... Loss: 0.00163752\n",
      "Train Epoch: 43 ... Batch: 10 ... Loss: 0.00188586\n",
      "Train Epoch: 43 ... Batch: 20 ... Loss: 0.00167894\n",
      "Train Epoch: 43 ... Batch: 30 ... Loss: 0.00203020\n",
      "Train Epoch: 43 ... Batch: 40 ... Loss: 0.00199449\n",
      "Train Epoch: 43 ... Batch: 50 ... Loss: 0.00199188\n",
      "Train Epoch: 43 ... Batch: 60 ... Loss: 0.00173849\n",
      "Train Epoch: 43 ... Batch: 70 ... Loss: 0.00191374\n",
      "Train Epoch: 43 ... Batch: 80 ... Loss: 0.00209472\n",
      "Train Epoch: 43 ... Batch: 90 ... Loss: 0.00167589\n",
      "Train Epoch: 43 ... Batch: 100 ... Loss: 0.00160959\n",
      "Train Epoch: 43 ... Batch: 110 ... Loss: 0.00187714\n",
      "Train Epoch: 43 ... Batch: 120 ... Loss: 0.00166703\n",
      "Train Epoch: 43 ... Batch: 130 ... Loss: 0.00236215\n",
      "Train Epoch: 43 ... Batch: 140 ... Loss: 0.00189735\n",
      "Train Epoch: 43 ... Batch: 150 ... Loss: 0.00166921\n",
      "Train Epoch: 43 ... Batch: 160 ... Loss: 0.00229886\n",
      "Train Epoch: 43 ... Batch: 170 ... Loss: 0.00165429\n",
      "Train Epoch: 43 ... Batch: 180 ... Loss: 0.00180096\n",
      "Train Epoch: 43 ... Batch: 190 ... Loss: 0.00146343\n",
      "Train Epoch: 43 ... Batch: 200 ... Loss: 0.00208263\n",
      "------------------- Test set: Average loss: 476.0662 ... Samples: 810\n",
      "Train Epoch: 44 ... Batch: 0 ... Loss: 0.00211835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44 ... Batch: 10 ... Loss: 0.00209422\n",
      "Train Epoch: 44 ... Batch: 20 ... Loss: 0.00207007\n",
      "Train Epoch: 44 ... Batch: 30 ... Loss: 0.00218716\n",
      "Train Epoch: 44 ... Batch: 40 ... Loss: 0.00183821\n",
      "Train Epoch: 44 ... Batch: 50 ... Loss: 0.00181351\n",
      "Train Epoch: 44 ... Batch: 60 ... Loss: 0.00185190\n",
      "Train Epoch: 44 ... Batch: 70 ... Loss: 0.00195962\n",
      "Train Epoch: 44 ... Batch: 80 ... Loss: 0.00207473\n",
      "Train Epoch: 44 ... Batch: 90 ... Loss: 0.00158415\n",
      "Train Epoch: 44 ... Batch: 100 ... Loss: 0.00178122\n",
      "Train Epoch: 44 ... Batch: 110 ... Loss: 0.00222650\n",
      "Train Epoch: 44 ... Batch: 120 ... Loss: 0.00174827\n",
      "Train Epoch: 44 ... Batch: 130 ... Loss: 0.00177054\n",
      "Train Epoch: 44 ... Batch: 140 ... Loss: 0.00192168\n",
      "Train Epoch: 44 ... Batch: 150 ... Loss: 0.00203295\n",
      "Train Epoch: 44 ... Batch: 160 ... Loss: 0.00203314\n",
      "Train Epoch: 44 ... Batch: 170 ... Loss: 0.00195925\n",
      "Train Epoch: 44 ... Batch: 180 ... Loss: 0.00172121\n",
      "Train Epoch: 44 ... Batch: 190 ... Loss: 0.00211127\n",
      "Train Epoch: 44 ... Batch: 200 ... Loss: 0.00169805\n",
      "------------------- Test set: Average loss: 476.1541 ... Samples: 810\n",
      "Train Epoch: 45 ... Batch: 0 ... Loss: 0.00201974\n",
      "Train Epoch: 45 ... Batch: 10 ... Loss: 0.00200228\n",
      "Train Epoch: 45 ... Batch: 20 ... Loss: 0.00176679\n",
      "Train Epoch: 45 ... Batch: 30 ... Loss: 0.00214910\n",
      "Train Epoch: 45 ... Batch: 40 ... Loss: 0.00154587\n",
      "Train Epoch: 45 ... Batch: 50 ... Loss: 0.00154453\n",
      "Train Epoch: 45 ... Batch: 60 ... Loss: 0.00204501\n",
      "Train Epoch: 45 ... Batch: 70 ... Loss: 0.00222307\n",
      "Train Epoch: 45 ... Batch: 80 ... Loss: 0.00211402\n",
      "Train Epoch: 45 ... Batch: 90 ... Loss: 0.00198585\n",
      "Train Epoch: 45 ... Batch: 100 ... Loss: 0.00169463\n",
      "Train Epoch: 45 ... Batch: 110 ... Loss: 0.00211050\n",
      "Train Epoch: 45 ... Batch: 120 ... Loss: 0.00162746\n",
      "Train Epoch: 45 ... Batch: 130 ... Loss: 0.00200975\n",
      "Train Epoch: 45 ... Batch: 140 ... Loss: 0.00213292\n",
      "Train Epoch: 45 ... Batch: 150 ... Loss: 0.00207544\n",
      "Train Epoch: 45 ... Batch: 160 ... Loss: 0.00170224\n",
      "Train Epoch: 45 ... Batch: 170 ... Loss: 0.00183111\n",
      "Train Epoch: 45 ... Batch: 180 ... Loss: 0.00192341\n",
      "Train Epoch: 45 ... Batch: 190 ... Loss: 0.00186647\n",
      "Train Epoch: 45 ... Batch: 200 ... Loss: 0.00178741\n",
      "------------------- Test set: Average loss: 475.9358 ... Samples: 810\n",
      "Train Epoch: 46 ... Batch: 0 ... Loss: 0.00172024\n",
      "Train Epoch: 46 ... Batch: 10 ... Loss: 0.00187832\n",
      "Train Epoch: 46 ... Batch: 20 ... Loss: 0.00179616\n",
      "Train Epoch: 46 ... Batch: 30 ... Loss: 0.00174977\n",
      "Train Epoch: 46 ... Batch: 40 ... Loss: 0.00213247\n",
      "Train Epoch: 46 ... Batch: 50 ... Loss: 0.00213736\n",
      "Train Epoch: 46 ... Batch: 60 ... Loss: 0.00168009\n",
      "Train Epoch: 46 ... Batch: 70 ... Loss: 0.00214240\n",
      "Train Epoch: 46 ... Batch: 80 ... Loss: 0.00193289\n",
      "Train Epoch: 46 ... Batch: 90 ... Loss: 0.00185258\n",
      "Train Epoch: 46 ... Batch: 100 ... Loss: 0.00199496\n",
      "Train Epoch: 46 ... Batch: 110 ... Loss: 0.00181487\n",
      "Train Epoch: 46 ... Batch: 120 ... Loss: 0.00157441\n",
      "Train Epoch: 46 ... Batch: 130 ... Loss: 0.00208984\n",
      "Train Epoch: 46 ... Batch: 140 ... Loss: 0.00178260\n",
      "Train Epoch: 46 ... Batch: 150 ... Loss: 0.00206191\n",
      "Train Epoch: 46 ... Batch: 160 ... Loss: 0.00212459\n",
      "Train Epoch: 46 ... Batch: 170 ... Loss: 0.00187865\n",
      "Train Epoch: 46 ... Batch: 180 ... Loss: 0.00164469\n",
      "Train Epoch: 46 ... Batch: 190 ... Loss: 0.00183857\n",
      "Train Epoch: 46 ... Batch: 200 ... Loss: 0.00178653\n",
      "------------------- Test set: Average loss: 475.6016 ... Samples: 810\n",
      "Train Epoch: 47 ... Batch: 0 ... Loss: 0.00174415\n",
      "Train Epoch: 47 ... Batch: 10 ... Loss: 0.00171773\n",
      "Train Epoch: 47 ... Batch: 20 ... Loss: 0.00196757\n",
      "Train Epoch: 47 ... Batch: 30 ... Loss: 0.00164714\n",
      "Train Epoch: 47 ... Batch: 40 ... Loss: 0.00175264\n",
      "Train Epoch: 47 ... Batch: 50 ... Loss: 0.00167800\n",
      "Train Epoch: 47 ... Batch: 60 ... Loss: 0.00188315\n",
      "Train Epoch: 47 ... Batch: 70 ... Loss: 0.00194211\n",
      "Train Epoch: 47 ... Batch: 80 ... Loss: 0.00186476\n",
      "Train Epoch: 47 ... Batch: 90 ... Loss: 0.00188092\n",
      "Train Epoch: 47 ... Batch: 100 ... Loss: 0.00174815\n",
      "Train Epoch: 47 ... Batch: 110 ... Loss: 0.00168960\n",
      "Train Epoch: 47 ... Batch: 120 ... Loss: 0.00203640\n",
      "Train Epoch: 47 ... Batch: 130 ... Loss: 0.00181184\n",
      "Train Epoch: 47 ... Batch: 140 ... Loss: 0.00194475\n",
      "Train Epoch: 47 ... Batch: 150 ... Loss: 0.00189552\n",
      "Train Epoch: 47 ... Batch: 160 ... Loss: 0.00172778\n",
      "Train Epoch: 47 ... Batch: 170 ... Loss: 0.00196950\n",
      "Train Epoch: 47 ... Batch: 180 ... Loss: 0.00196044\n",
      "Train Epoch: 47 ... Batch: 190 ... Loss: 0.00160919\n",
      "Train Epoch: 47 ... Batch: 200 ... Loss: 0.00170855\n",
      "------------------- Test set: Average loss: 475.7538 ... Samples: 810\n",
      "Train Epoch: 48 ... Batch: 0 ... Loss: 0.00167312\n",
      "Train Epoch: 48 ... Batch: 10 ... Loss: 0.00202163\n",
      "Train Epoch: 48 ... Batch: 20 ... Loss: 0.00204980\n",
      "Train Epoch: 48 ... Batch: 30 ... Loss: 0.00177812\n",
      "Train Epoch: 48 ... Batch: 40 ... Loss: 0.00196108\n",
      "Train Epoch: 48 ... Batch: 50 ... Loss: 0.00195001\n",
      "Train Epoch: 48 ... Batch: 60 ... Loss: 0.00211527\n",
      "Train Epoch: 48 ... Batch: 70 ... Loss: 0.00176307\n",
      "Train Epoch: 48 ... Batch: 80 ... Loss: 0.00200833\n",
      "Train Epoch: 48 ... Batch: 90 ... Loss: 0.00167005\n",
      "Train Epoch: 48 ... Batch: 100 ... Loss: 0.00165891\n",
      "Train Epoch: 48 ... Batch: 110 ... Loss: 0.00186346\n",
      "Train Epoch: 48 ... Batch: 120 ... Loss: 0.00178774\n",
      "Train Epoch: 48 ... Batch: 130 ... Loss: 0.00219311\n",
      "Train Epoch: 48 ... Batch: 140 ... Loss: 0.00200062\n",
      "Train Epoch: 48 ... Batch: 150 ... Loss: 0.00194476\n",
      "Train Epoch: 48 ... Batch: 160 ... Loss: 0.00173508\n",
      "Train Epoch: 48 ... Batch: 170 ... Loss: 0.00191848\n",
      "Train Epoch: 48 ... Batch: 180 ... Loss: 0.00239149\n",
      "Train Epoch: 48 ... Batch: 190 ... Loss: 0.00181896\n",
      "Train Epoch: 48 ... Batch: 200 ... Loss: 0.00193062\n",
      "------------------- Test set: Average loss: 475.3456 ... Samples: 810\n",
      "Train Epoch: 49 ... Batch: 0 ... Loss: 0.00165617\n",
      "Train Epoch: 49 ... Batch: 10 ... Loss: 0.00227313\n",
      "Train Epoch: 49 ... Batch: 20 ... Loss: 0.00190958\n",
      "Train Epoch: 49 ... Batch: 30 ... Loss: 0.00210147\n",
      "Train Epoch: 49 ... Batch: 40 ... Loss: 0.00199310\n",
      "Train Epoch: 49 ... Batch: 50 ... Loss: 0.00184361\n",
      "Train Epoch: 49 ... Batch: 60 ... Loss: 0.00253428\n",
      "Train Epoch: 49 ... Batch: 70 ... Loss: 0.00225151\n",
      "Train Epoch: 49 ... Batch: 80 ... Loss: 0.00184712\n",
      "Train Epoch: 49 ... Batch: 90 ... Loss: 0.00220966\n",
      "Train Epoch: 49 ... Batch: 100 ... Loss: 0.00198900\n",
      "Train Epoch: 49 ... Batch: 110 ... Loss: 0.00188524\n",
      "Train Epoch: 49 ... Batch: 120 ... Loss: 0.00234573\n",
      "Train Epoch: 49 ... Batch: 130 ... Loss: 0.00195340\n",
      "Train Epoch: 49 ... Batch: 140 ... Loss: 0.00242866\n",
      "Train Epoch: 49 ... Batch: 150 ... Loss: 0.00203613\n",
      "Train Epoch: 49 ... Batch: 160 ... Loss: 0.00181566\n",
      "Train Epoch: 49 ... Batch: 170 ... Loss: 0.00142409\n",
      "Train Epoch: 49 ... Batch: 180 ... Loss: 0.00182850\n",
      "Train Epoch: 49 ... Batch: 190 ... Loss: 0.00192108\n",
      "Train Epoch: 49 ... Batch: 200 ... Loss: 0.00206919\n",
      "------------------- Test set: Average loss: 475.1338 ... Samples: 810\n",
      "Train Epoch: 50 ... Batch: 0 ... Loss: 0.00190455\n",
      "Train Epoch: 50 ... Batch: 10 ... Loss: 0.00198464\n",
      "Train Epoch: 50 ... Batch: 20 ... Loss: 0.00208320\n",
      "Train Epoch: 50 ... Batch: 30 ... Loss: 0.00165322\n",
      "Train Epoch: 50 ... Batch: 40 ... Loss: 0.00227512\n",
      "Train Epoch: 50 ... Batch: 50 ... Loss: 0.00219839\n",
      "Train Epoch: 50 ... Batch: 60 ... Loss: 0.00210845\n",
      "Train Epoch: 50 ... Batch: 70 ... Loss: 0.00167777\n",
      "Train Epoch: 50 ... Batch: 80 ... Loss: 0.00185174\n",
      "Train Epoch: 50 ... Batch: 90 ... Loss: 0.00171806\n",
      "Train Epoch: 50 ... Batch: 100 ... Loss: 0.00181240\n",
      "Train Epoch: 50 ... Batch: 110 ... Loss: 0.00193029\n",
      "Train Epoch: 50 ... Batch: 120 ... Loss: 0.00194627\n",
      "Train Epoch: 50 ... Batch: 130 ... Loss: 0.00224527\n",
      "Train Epoch: 50 ... Batch: 140 ... Loss: 0.00217264\n",
      "Train Epoch: 50 ... Batch: 150 ... Loss: 0.00214757\n",
      "Train Epoch: 50 ... Batch: 160 ... Loss: 0.00157651\n",
      "Train Epoch: 50 ... Batch: 170 ... Loss: 0.00203800\n",
      "Train Epoch: 50 ... Batch: 180 ... Loss: 0.00214881\n",
      "Train Epoch: 50 ... Batch: 190 ... Loss: 0.00162052\n",
      "Train Epoch: 50 ... Batch: 200 ... Loss: 0.00215695\n",
      "------------------- Test set: Average loss: 475.7877 ... Samples: 810\n",
      "Train Epoch: 51 ... Batch: 0 ... Loss: 0.00194748\n",
      "Train Epoch: 51 ... Batch: 10 ... Loss: 0.00195312\n",
      "Train Epoch: 51 ... Batch: 20 ... Loss: 0.00196615\n",
      "Train Epoch: 51 ... Batch: 30 ... Loss: 0.00162238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 51 ... Batch: 40 ... Loss: 0.00166787\n",
      "Train Epoch: 51 ... Batch: 50 ... Loss: 0.00167011\n",
      "Train Epoch: 51 ... Batch: 60 ... Loss: 0.00228229\n",
      "Train Epoch: 51 ... Batch: 70 ... Loss: 0.00178224\n",
      "Train Epoch: 51 ... Batch: 80 ... Loss: 0.00203705\n",
      "Train Epoch: 51 ... Batch: 90 ... Loss: 0.00202189\n",
      "Train Epoch: 51 ... Batch: 100 ... Loss: 0.00199715\n",
      "Train Epoch: 51 ... Batch: 110 ... Loss: 0.00177339\n",
      "Train Epoch: 51 ... Batch: 120 ... Loss: 0.00214264\n",
      "Train Epoch: 51 ... Batch: 130 ... Loss: 0.00171720\n",
      "Train Epoch: 51 ... Batch: 140 ... Loss: 0.00169052\n",
      "Train Epoch: 51 ... Batch: 150 ... Loss: 0.00166476\n",
      "Train Epoch: 51 ... Batch: 160 ... Loss: 0.00180264\n",
      "Train Epoch: 51 ... Batch: 170 ... Loss: 0.00193884\n",
      "Train Epoch: 51 ... Batch: 180 ... Loss: 0.00173614\n",
      "Train Epoch: 51 ... Batch: 190 ... Loss: 0.00176837\n",
      "Train Epoch: 51 ... Batch: 200 ... Loss: 0.00203685\n",
      "------------------- Test set: Average loss: 475.0215 ... Samples: 810\n",
      "Train Epoch: 52 ... Batch: 0 ... Loss: 0.00180329\n",
      "Train Epoch: 52 ... Batch: 10 ... Loss: 0.00170453\n",
      "Train Epoch: 52 ... Batch: 20 ... Loss: 0.00175195\n",
      "Train Epoch: 52 ... Batch: 30 ... Loss: 0.00190189\n",
      "Train Epoch: 52 ... Batch: 40 ... Loss: 0.00185797\n",
      "Train Epoch: 52 ... Batch: 50 ... Loss: 0.00158868\n",
      "Train Epoch: 52 ... Batch: 60 ... Loss: 0.00187019\n",
      "Train Epoch: 52 ... Batch: 70 ... Loss: 0.00187108\n",
      "Train Epoch: 52 ... Batch: 80 ... Loss: 0.00201620\n",
      "Train Epoch: 52 ... Batch: 90 ... Loss: 0.00132140\n",
      "Train Epoch: 52 ... Batch: 100 ... Loss: 0.00157250\n",
      "Train Epoch: 52 ... Batch: 110 ... Loss: 0.00176963\n",
      "Train Epoch: 52 ... Batch: 120 ... Loss: 0.00203560\n",
      "Train Epoch: 52 ... Batch: 130 ... Loss: 0.00158538\n",
      "Train Epoch: 52 ... Batch: 140 ... Loss: 0.00165508\n",
      "Train Epoch: 52 ... Batch: 150 ... Loss: 0.00177968\n",
      "Train Epoch: 52 ... Batch: 160 ... Loss: 0.00193190\n",
      "Train Epoch: 52 ... Batch: 170 ... Loss: 0.00180695\n",
      "Train Epoch: 52 ... Batch: 180 ... Loss: 0.00219996\n",
      "Train Epoch: 52 ... Batch: 190 ... Loss: 0.00214908\n",
      "Train Epoch: 52 ... Batch: 200 ... Loss: 0.00181938\n",
      "------------------- Test set: Average loss: 474.7766 ... Samples: 810\n",
      "Train Epoch: 53 ... Batch: 0 ... Loss: 0.00212003\n",
      "Train Epoch: 53 ... Batch: 10 ... Loss: 0.00194163\n",
      "Train Epoch: 53 ... Batch: 20 ... Loss: 0.00209498\n",
      "Train Epoch: 53 ... Batch: 30 ... Loss: 0.00180607\n",
      "Train Epoch: 53 ... Batch: 40 ... Loss: 0.00203893\n",
      "Train Epoch: 53 ... Batch: 50 ... Loss: 0.00205968\n",
      "Train Epoch: 53 ... Batch: 60 ... Loss: 0.00185407\n",
      "Train Epoch: 53 ... Batch: 70 ... Loss: 0.00174737\n",
      "Train Epoch: 53 ... Batch: 80 ... Loss: 0.00189187\n",
      "Train Epoch: 53 ... Batch: 90 ... Loss: 0.00174747\n",
      "Train Epoch: 53 ... Batch: 100 ... Loss: 0.00210047\n",
      "Train Epoch: 53 ... Batch: 110 ... Loss: 0.00184046\n",
      "Train Epoch: 53 ... Batch: 120 ... Loss: 0.00211101\n",
      "Train Epoch: 53 ... Batch: 130 ... Loss: 0.00204341\n",
      "Train Epoch: 53 ... Batch: 140 ... Loss: 0.00175993\n",
      "Train Epoch: 53 ... Batch: 150 ... Loss: 0.00174364\n",
      "Train Epoch: 53 ... Batch: 160 ... Loss: 0.00203892\n",
      "Train Epoch: 53 ... Batch: 170 ... Loss: 0.00208910\n",
      "Train Epoch: 53 ... Batch: 180 ... Loss: 0.00212361\n",
      "Train Epoch: 53 ... Batch: 190 ... Loss: 0.00204625\n",
      "Train Epoch: 53 ... Batch: 200 ... Loss: 0.00169593\n",
      "------------------- Test set: Average loss: 475.0671 ... Samples: 810\n",
      "Train Epoch: 54 ... Batch: 0 ... Loss: 0.00179641\n",
      "Train Epoch: 54 ... Batch: 10 ... Loss: 0.00186144\n",
      "Train Epoch: 54 ... Batch: 20 ... Loss: 0.00207136\n",
      "Train Epoch: 54 ... Batch: 30 ... Loss: 0.00195973\n",
      "Train Epoch: 54 ... Batch: 40 ... Loss: 0.00177525\n",
      "Train Epoch: 54 ... Batch: 50 ... Loss: 0.00258741\n",
      "Train Epoch: 54 ... Batch: 60 ... Loss: 0.00207877\n",
      "Train Epoch: 54 ... Batch: 70 ... Loss: 0.00167251\n",
      "Train Epoch: 54 ... Batch: 80 ... Loss: 0.00189792\n",
      "Train Epoch: 54 ... Batch: 90 ... Loss: 0.00168075\n",
      "Train Epoch: 54 ... Batch: 100 ... Loss: 0.00177685\n",
      "Train Epoch: 54 ... Batch: 110 ... Loss: 0.00169068\n",
      "Train Epoch: 54 ... Batch: 120 ... Loss: 0.00180533\n",
      "Train Epoch: 54 ... Batch: 130 ... Loss: 0.00239233\n",
      "Train Epoch: 54 ... Batch: 140 ... Loss: 0.00224097\n",
      "Train Epoch: 54 ... Batch: 150 ... Loss: 0.00173838\n",
      "Train Epoch: 54 ... Batch: 160 ... Loss: 0.00208585\n",
      "Train Epoch: 54 ... Batch: 170 ... Loss: 0.00204769\n",
      "Train Epoch: 54 ... Batch: 180 ... Loss: 0.00184565\n",
      "Train Epoch: 54 ... Batch: 190 ... Loss: 0.00171913\n",
      "Train Epoch: 54 ... Batch: 200 ... Loss: 0.00186714\n",
      "------------------- Test set: Average loss: 474.6873 ... Samples: 810\n",
      "Train Epoch: 55 ... Batch: 0 ... Loss: 0.00171070\n",
      "Train Epoch: 55 ... Batch: 10 ... Loss: 0.00166764\n",
      "Train Epoch: 55 ... Batch: 20 ... Loss: 0.00178242\n",
      "Train Epoch: 55 ... Batch: 30 ... Loss: 0.00188885\n",
      "Train Epoch: 55 ... Batch: 40 ... Loss: 0.00182484\n",
      "Train Epoch: 55 ... Batch: 50 ... Loss: 0.00190430\n",
      "Train Epoch: 55 ... Batch: 60 ... Loss: 0.00213332\n",
      "Train Epoch: 55 ... Batch: 70 ... Loss: 0.00213524\n",
      "Train Epoch: 55 ... Batch: 80 ... Loss: 0.00179939\n",
      "Train Epoch: 55 ... Batch: 90 ... Loss: 0.00184507\n",
      "Train Epoch: 55 ... Batch: 100 ... Loss: 0.00197172\n",
      "Train Epoch: 55 ... Batch: 110 ... Loss: 0.00180194\n",
      "Train Epoch: 55 ... Batch: 120 ... Loss: 0.00177500\n",
      "Train Epoch: 55 ... Batch: 130 ... Loss: 0.00204000\n",
      "Train Epoch: 55 ... Batch: 140 ... Loss: 0.00200293\n",
      "Train Epoch: 55 ... Batch: 150 ... Loss: 0.00240251\n",
      "Train Epoch: 55 ... Batch: 160 ... Loss: 0.00198024\n",
      "Train Epoch: 55 ... Batch: 170 ... Loss: 0.00160264\n",
      "Train Epoch: 55 ... Batch: 180 ... Loss: 0.00160482\n",
      "Train Epoch: 55 ... Batch: 190 ... Loss: 0.00205154\n",
      "Train Epoch: 55 ... Batch: 200 ... Loss: 0.00200378\n",
      "------------------- Test set: Average loss: 474.6270 ... Samples: 810\n",
      "Train Epoch: 56 ... Batch: 0 ... Loss: 0.00221796\n",
      "Train Epoch: 56 ... Batch: 10 ... Loss: 0.00182110\n",
      "Train Epoch: 56 ... Batch: 20 ... Loss: 0.00181491\n",
      "Train Epoch: 56 ... Batch: 30 ... Loss: 0.00150355\n",
      "Train Epoch: 56 ... Batch: 40 ... Loss: 0.00196745\n",
      "Train Epoch: 56 ... Batch: 50 ... Loss: 0.00191900\n",
      "Train Epoch: 56 ... Batch: 60 ... Loss: 0.00187088\n",
      "Train Epoch: 56 ... Batch: 70 ... Loss: 0.00198571\n",
      "Train Epoch: 56 ... Batch: 80 ... Loss: 0.00198220\n",
      "Train Epoch: 56 ... Batch: 90 ... Loss: 0.00172676\n",
      "Train Epoch: 56 ... Batch: 100 ... Loss: 0.00217400\n",
      "Train Epoch: 56 ... Batch: 110 ... Loss: 0.00183925\n",
      "Train Epoch: 56 ... Batch: 120 ... Loss: 0.00183180\n",
      "Train Epoch: 56 ... Batch: 130 ... Loss: 0.00145779\n",
      "Train Epoch: 56 ... Batch: 140 ... Loss: 0.00196081\n",
      "Train Epoch: 56 ... Batch: 150 ... Loss: 0.00215583\n",
      "Train Epoch: 56 ... Batch: 160 ... Loss: 0.00197512\n",
      "Train Epoch: 56 ... Batch: 170 ... Loss: 0.00178603\n",
      "Train Epoch: 56 ... Batch: 180 ... Loss: 0.00183097\n",
      "Train Epoch: 56 ... Batch: 190 ... Loss: 0.00193636\n",
      "Train Epoch: 56 ... Batch: 200 ... Loss: 0.00172927\n",
      "------------------- Test set: Average loss: 474.4533 ... Samples: 810\n",
      "Train Epoch: 57 ... Batch: 0 ... Loss: 0.00209885\n",
      "Train Epoch: 57 ... Batch: 10 ... Loss: 0.00181063\n",
      "Train Epoch: 57 ... Batch: 20 ... Loss: 0.00202003\n",
      "Train Epoch: 57 ... Batch: 30 ... Loss: 0.00180934\n",
      "Train Epoch: 57 ... Batch: 40 ... Loss: 0.00182566\n",
      "Train Epoch: 57 ... Batch: 50 ... Loss: 0.00206701\n",
      "Train Epoch: 57 ... Batch: 60 ... Loss: 0.00196520\n",
      "Train Epoch: 57 ... Batch: 70 ... Loss: 0.00155364\n",
      "Train Epoch: 57 ... Batch: 80 ... Loss: 0.00173249\n",
      "Train Epoch: 57 ... Batch: 90 ... Loss: 0.00168285\n",
      "Train Epoch: 57 ... Batch: 100 ... Loss: 0.00193394\n",
      "Train Epoch: 57 ... Batch: 110 ... Loss: 0.00175929\n",
      "Train Epoch: 57 ... Batch: 120 ... Loss: 0.00174077\n",
      "Train Epoch: 57 ... Batch: 130 ... Loss: 0.00164761\n",
      "Train Epoch: 57 ... Batch: 140 ... Loss: 0.00178834\n",
      "Train Epoch: 57 ... Batch: 150 ... Loss: 0.00195897\n",
      "Train Epoch: 57 ... Batch: 160 ... Loss: 0.00181499\n",
      "Train Epoch: 57 ... Batch: 170 ... Loss: 0.00211614\n",
      "Train Epoch: 57 ... Batch: 180 ... Loss: 0.00192842\n",
      "Train Epoch: 57 ... Batch: 190 ... Loss: 0.00193364\n",
      "Train Epoch: 57 ... Batch: 200 ... Loss: 0.00205483\n",
      "------------------- Test set: Average loss: 474.2346 ... Samples: 810\n",
      "Train Epoch: 58 ... Batch: 0 ... Loss: 0.00222681\n",
      "Train Epoch: 58 ... Batch: 10 ... Loss: 0.00182573\n",
      "Train Epoch: 58 ... Batch: 20 ... Loss: 0.00190875\n",
      "Train Epoch: 58 ... Batch: 30 ... Loss: 0.00140242\n",
      "Train Epoch: 58 ... Batch: 40 ... Loss: 0.00212911\n",
      "Train Epoch: 58 ... Batch: 50 ... Loss: 0.00180949\n",
      "Train Epoch: 58 ... Batch: 60 ... Loss: 0.00154210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 58 ... Batch: 70 ... Loss: 0.00222243\n",
      "Train Epoch: 58 ... Batch: 80 ... Loss: 0.00187147\n",
      "Train Epoch: 58 ... Batch: 90 ... Loss: 0.00174233\n",
      "Train Epoch: 58 ... Batch: 100 ... Loss: 0.00196418\n",
      "Train Epoch: 58 ... Batch: 110 ... Loss: 0.00202461\n",
      "Train Epoch: 58 ... Batch: 120 ... Loss: 0.00181185\n",
      "Train Epoch: 58 ... Batch: 130 ... Loss: 0.00219593\n",
      "Train Epoch: 58 ... Batch: 140 ... Loss: 0.00201725\n",
      "Train Epoch: 58 ... Batch: 150 ... Loss: 0.00172241\n",
      "Train Epoch: 58 ... Batch: 160 ... Loss: 0.00208210\n",
      "Train Epoch: 58 ... Batch: 170 ... Loss: 0.00196300\n",
      "Train Epoch: 58 ... Batch: 180 ... Loss: 0.00173552\n",
      "Train Epoch: 58 ... Batch: 190 ... Loss: 0.00159435\n",
      "Train Epoch: 58 ... Batch: 200 ... Loss: 0.00224513\n",
      "------------------- Test set: Average loss: 473.8628 ... Samples: 810\n",
      "Train Epoch: 59 ... Batch: 0 ... Loss: 0.00169663\n",
      "Train Epoch: 59 ... Batch: 10 ... Loss: 0.00206939\n",
      "Train Epoch: 59 ... Batch: 20 ... Loss: 0.00146531\n",
      "Train Epoch: 59 ... Batch: 30 ... Loss: 0.00175988\n",
      "Train Epoch: 59 ... Batch: 40 ... Loss: 0.00202017\n",
      "Train Epoch: 59 ... Batch: 50 ... Loss: 0.00181516\n",
      "Train Epoch: 59 ... Batch: 60 ... Loss: 0.00192656\n",
      "Train Epoch: 59 ... Batch: 70 ... Loss: 0.00204539\n",
      "Train Epoch: 59 ... Batch: 80 ... Loss: 0.00191182\n",
      "Train Epoch: 59 ... Batch: 90 ... Loss: 0.00192050\n",
      "Train Epoch: 59 ... Batch: 100 ... Loss: 0.00191771\n",
      "Train Epoch: 59 ... Batch: 110 ... Loss: 0.00207480\n",
      "Train Epoch: 59 ... Batch: 120 ... Loss: 0.00202819\n",
      "Train Epoch: 59 ... Batch: 130 ... Loss: 0.00176181\n",
      "Train Epoch: 59 ... Batch: 140 ... Loss: 0.00229184\n",
      "Train Epoch: 59 ... Batch: 150 ... Loss: 0.00175002\n",
      "Train Epoch: 59 ... Batch: 160 ... Loss: 0.00182468\n",
      "Train Epoch: 59 ... Batch: 170 ... Loss: 0.00178439\n",
      "Train Epoch: 59 ... Batch: 180 ... Loss: 0.00210230\n",
      "Train Epoch: 59 ... Batch: 190 ... Loss: 0.00185580\n",
      "Train Epoch: 59 ... Batch: 200 ... Loss: 0.00198819\n",
      "------------------- Test set: Average loss: 474.2151 ... Samples: 810\n",
      "Train Epoch: 60 ... Batch: 0 ... Loss: 0.00198208\n",
      "Train Epoch: 60 ... Batch: 10 ... Loss: 0.00219394\n",
      "Train Epoch: 60 ... Batch: 20 ... Loss: 0.00158667\n",
      "Train Epoch: 60 ... Batch: 30 ... Loss: 0.00204346\n",
      "Train Epoch: 60 ... Batch: 40 ... Loss: 0.00179472\n",
      "Train Epoch: 60 ... Batch: 50 ... Loss: 0.00164028\n",
      "Train Epoch: 60 ... Batch: 60 ... Loss: 0.00185284\n",
      "Train Epoch: 60 ... Batch: 70 ... Loss: 0.00188339\n",
      "Train Epoch: 60 ... Batch: 80 ... Loss: 0.00196285\n",
      "Train Epoch: 60 ... Batch: 90 ... Loss: 0.00197548\n",
      "Train Epoch: 60 ... Batch: 100 ... Loss: 0.00167648\n",
      "Train Epoch: 60 ... Batch: 110 ... Loss: 0.00190630\n",
      "Train Epoch: 60 ... Batch: 120 ... Loss: 0.00205733\n",
      "Train Epoch: 60 ... Batch: 130 ... Loss: 0.00159876\n",
      "Train Epoch: 60 ... Batch: 140 ... Loss: 0.00240490\n",
      "Train Epoch: 60 ... Batch: 150 ... Loss: 0.00160877\n",
      "Train Epoch: 60 ... Batch: 160 ... Loss: 0.00162924\n",
      "Train Epoch: 60 ... Batch: 170 ... Loss: 0.00197962\n",
      "Train Epoch: 60 ... Batch: 180 ... Loss: 0.00188471\n",
      "Train Epoch: 60 ... Batch: 190 ... Loss: 0.00195878\n",
      "Train Epoch: 60 ... Batch: 200 ... Loss: 0.00173841\n",
      "------------------- Test set: Average loss: 473.3926 ... Samples: 810\n",
      "Train Epoch: 61 ... Batch: 0 ... Loss: 0.00189719\n",
      "Train Epoch: 61 ... Batch: 10 ... Loss: 0.00194725\n",
      "Train Epoch: 61 ... Batch: 20 ... Loss: 0.00193454\n",
      "Train Epoch: 61 ... Batch: 30 ... Loss: 0.00194074\n",
      "Train Epoch: 61 ... Batch: 40 ... Loss: 0.00180016\n",
      "Train Epoch: 61 ... Batch: 50 ... Loss: 0.00195975\n",
      "Train Epoch: 61 ... Batch: 60 ... Loss: 0.00150804\n",
      "Train Epoch: 61 ... Batch: 70 ... Loss: 0.00202666\n",
      "Train Epoch: 61 ... Batch: 80 ... Loss: 0.00189922\n",
      "Train Epoch: 61 ... Batch: 90 ... Loss: 0.00195993\n",
      "Train Epoch: 61 ... Batch: 100 ... Loss: 0.00188794\n",
      "Train Epoch: 61 ... Batch: 110 ... Loss: 0.00202694\n",
      "Train Epoch: 61 ... Batch: 120 ... Loss: 0.00187611\n",
      "Train Epoch: 61 ... Batch: 130 ... Loss: 0.00191660\n",
      "Train Epoch: 61 ... Batch: 140 ... Loss: 0.00208311\n",
      "Train Epoch: 61 ... Batch: 150 ... Loss: 0.00154296\n",
      "Train Epoch: 61 ... Batch: 160 ... Loss: 0.00210077\n",
      "Train Epoch: 61 ... Batch: 170 ... Loss: 0.00198149\n",
      "Train Epoch: 61 ... Batch: 180 ... Loss: 0.00180391\n",
      "Train Epoch: 61 ... Batch: 190 ... Loss: 0.00169807\n",
      "Train Epoch: 61 ... Batch: 200 ... Loss: 0.00175282\n",
      "------------------- Test set: Average loss: 474.2307 ... Samples: 810\n",
      "Train Epoch: 62 ... Batch: 0 ... Loss: 0.00160030\n",
      "Train Epoch: 62 ... Batch: 10 ... Loss: 0.00187357\n",
      "Train Epoch: 62 ... Batch: 20 ... Loss: 0.00191139\n",
      "Train Epoch: 62 ... Batch: 30 ... Loss: 0.00190669\n",
      "Train Epoch: 62 ... Batch: 40 ... Loss: 0.00181807\n",
      "Train Epoch: 62 ... Batch: 50 ... Loss: 0.00189782\n",
      "Train Epoch: 62 ... Batch: 60 ... Loss: 0.00181680\n",
      "Train Epoch: 62 ... Batch: 70 ... Loss: 0.00178649\n",
      "Train Epoch: 62 ... Batch: 80 ... Loss: 0.00269306\n",
      "Train Epoch: 62 ... Batch: 90 ... Loss: 0.00200435\n",
      "Train Epoch: 62 ... Batch: 100 ... Loss: 0.00180187\n",
      "Train Epoch: 62 ... Batch: 110 ... Loss: 0.00169780\n",
      "Train Epoch: 62 ... Batch: 120 ... Loss: 0.00135609\n",
      "Train Epoch: 62 ... Batch: 130 ... Loss: 0.00209425\n",
      "Train Epoch: 62 ... Batch: 140 ... Loss: 0.00193021\n",
      "Train Epoch: 62 ... Batch: 150 ... Loss: 0.00198035\n",
      "Train Epoch: 62 ... Batch: 160 ... Loss: 0.00212227\n",
      "Train Epoch: 62 ... Batch: 170 ... Loss: 0.00187458\n",
      "Train Epoch: 62 ... Batch: 180 ... Loss: 0.00190776\n",
      "Train Epoch: 62 ... Batch: 190 ... Loss: 0.00190160\n",
      "Train Epoch: 62 ... Batch: 200 ... Loss: 0.00207217\n",
      "------------------- Test set: Average loss: 473.1448 ... Samples: 810\n",
      "Train Epoch: 63 ... Batch: 0 ... Loss: 0.00188229\n",
      "Train Epoch: 63 ... Batch: 10 ... Loss: 0.00204183\n",
      "Train Epoch: 63 ... Batch: 20 ... Loss: 0.00184645\n",
      "Train Epoch: 63 ... Batch: 30 ... Loss: 0.00177532\n",
      "Train Epoch: 63 ... Batch: 40 ... Loss: 0.00177610\n",
      "Train Epoch: 63 ... Batch: 50 ... Loss: 0.00199671\n",
      "Train Epoch: 63 ... Batch: 60 ... Loss: 0.00173323\n",
      "Train Epoch: 63 ... Batch: 70 ... Loss: 0.00173435\n",
      "Train Epoch: 63 ... Batch: 80 ... Loss: 0.00175578\n",
      "Train Epoch: 63 ... Batch: 90 ... Loss: 0.00167615\n",
      "Train Epoch: 63 ... Batch: 100 ... Loss: 0.00198808\n",
      "Train Epoch: 63 ... Batch: 110 ... Loss: 0.00177773\n",
      "Train Epoch: 63 ... Batch: 120 ... Loss: 0.00193295\n",
      "Train Epoch: 63 ... Batch: 130 ... Loss: 0.00185785\n",
      "Train Epoch: 63 ... Batch: 140 ... Loss: 0.00162302\n",
      "Train Epoch: 63 ... Batch: 150 ... Loss: 0.00190809\n",
      "Train Epoch: 63 ... Batch: 160 ... Loss: 0.00180063\n",
      "Train Epoch: 63 ... Batch: 170 ... Loss: 0.00200749\n",
      "Train Epoch: 63 ... Batch: 180 ... Loss: 0.00209013\n",
      "Train Epoch: 63 ... Batch: 190 ... Loss: 0.00232295\n",
      "Train Epoch: 63 ... Batch: 200 ... Loss: 0.00215175\n",
      "------------------- Test set: Average loss: 473.3018 ... Samples: 810\n",
      "Train Epoch: 64 ... Batch: 0 ... Loss: 0.00190317\n",
      "Train Epoch: 64 ... Batch: 10 ... Loss: 0.00203019\n",
      "Train Epoch: 64 ... Batch: 20 ... Loss: 0.00182349\n",
      "Train Epoch: 64 ... Batch: 30 ... Loss: 0.00174302\n",
      "Train Epoch: 64 ... Batch: 40 ... Loss: 0.00232899\n",
      "Train Epoch: 64 ... Batch: 50 ... Loss: 0.00173437\n",
      "Train Epoch: 64 ... Batch: 60 ... Loss: 0.00199266\n",
      "Train Epoch: 64 ... Batch: 70 ... Loss: 0.00193657\n",
      "Train Epoch: 64 ... Batch: 80 ... Loss: 0.00200747\n",
      "Train Epoch: 64 ... Batch: 90 ... Loss: 0.00178292\n",
      "Train Epoch: 64 ... Batch: 100 ... Loss: 0.00185207\n",
      "Train Epoch: 64 ... Batch: 110 ... Loss: 0.00196567\n",
      "Train Epoch: 64 ... Batch: 120 ... Loss: 0.00183705\n",
      "Train Epoch: 64 ... Batch: 130 ... Loss: 0.00211415\n",
      "Train Epoch: 64 ... Batch: 140 ... Loss: 0.00191401\n",
      "Train Epoch: 64 ... Batch: 150 ... Loss: 0.00201107\n",
      "Train Epoch: 64 ... Batch: 160 ... Loss: 0.00209623\n",
      "Train Epoch: 64 ... Batch: 170 ... Loss: 0.00179418\n",
      "Train Epoch: 64 ... Batch: 180 ... Loss: 0.00210749\n",
      "Train Epoch: 64 ... Batch: 190 ... Loss: 0.00202012\n",
      "Train Epoch: 64 ... Batch: 200 ... Loss: 0.00179275\n",
      "------------------- Test set: Average loss: 472.9446 ... Samples: 810\n",
      "Train Epoch: 65 ... Batch: 0 ... Loss: 0.00185443\n",
      "Train Epoch: 65 ... Batch: 10 ... Loss: 0.00173096\n",
      "Train Epoch: 65 ... Batch: 20 ... Loss: 0.00199967\n",
      "Train Epoch: 65 ... Batch: 30 ... Loss: 0.00185139\n",
      "Train Epoch: 65 ... Batch: 40 ... Loss: 0.00183486\n",
      "Train Epoch: 65 ... Batch: 50 ... Loss: 0.00180512\n",
      "Train Epoch: 65 ... Batch: 60 ... Loss: 0.00172986\n",
      "Train Epoch: 65 ... Batch: 70 ... Loss: 0.00170548\n",
      "Train Epoch: 65 ... Batch: 80 ... Loss: 0.00200655\n",
      "Train Epoch: 65 ... Batch: 90 ... Loss: 0.00186431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 65 ... Batch: 100 ... Loss: 0.00200573\n",
      "Train Epoch: 65 ... Batch: 110 ... Loss: 0.00182432\n",
      "Train Epoch: 65 ... Batch: 120 ... Loss: 0.00181535\n",
      "Train Epoch: 65 ... Batch: 130 ... Loss: 0.00214717\n",
      "Train Epoch: 65 ... Batch: 140 ... Loss: 0.00193847\n",
      "Train Epoch: 65 ... Batch: 150 ... Loss: 0.00173919\n",
      "Train Epoch: 65 ... Batch: 160 ... Loss: 0.00203365\n",
      "Train Epoch: 65 ... Batch: 170 ... Loss: 0.00185279\n",
      "Train Epoch: 65 ... Batch: 180 ... Loss: 0.00164169\n",
      "Train Epoch: 65 ... Batch: 190 ... Loss: 0.00188817\n",
      "Train Epoch: 65 ... Batch: 200 ... Loss: 0.00199739\n",
      "------------------- Test set: Average loss: 473.5303 ... Samples: 810\n",
      "Train Epoch: 66 ... Batch: 0 ... Loss: 0.00186790\n",
      "Train Epoch: 66 ... Batch: 10 ... Loss: 0.00188903\n",
      "Train Epoch: 66 ... Batch: 20 ... Loss: 0.00216106\n",
      "Train Epoch: 66 ... Batch: 30 ... Loss: 0.00181027\n",
      "Train Epoch: 66 ... Batch: 40 ... Loss: 0.00201722\n",
      "Train Epoch: 66 ... Batch: 50 ... Loss: 0.00197817\n",
      "Train Epoch: 66 ... Batch: 60 ... Loss: 0.00186514\n",
      "Train Epoch: 66 ... Batch: 70 ... Loss: 0.00199190\n",
      "Train Epoch: 66 ... Batch: 80 ... Loss: 0.00198168\n",
      "Train Epoch: 66 ... Batch: 90 ... Loss: 0.00161424\n",
      "Train Epoch: 66 ... Batch: 100 ... Loss: 0.00173871\n",
      "Train Epoch: 66 ... Batch: 110 ... Loss: 0.00197290\n",
      "Train Epoch: 66 ... Batch: 120 ... Loss: 0.00184722\n",
      "Train Epoch: 66 ... Batch: 130 ... Loss: 0.00193200\n",
      "Train Epoch: 66 ... Batch: 140 ... Loss: 0.00171038\n",
      "Train Epoch: 66 ... Batch: 150 ... Loss: 0.00182526\n",
      "Train Epoch: 66 ... Batch: 160 ... Loss: 0.00186637\n",
      "Train Epoch: 66 ... Batch: 170 ... Loss: 0.00172475\n",
      "Train Epoch: 66 ... Batch: 180 ... Loss: 0.00208093\n",
      "Train Epoch: 66 ... Batch: 190 ... Loss: 0.00171488\n",
      "Train Epoch: 66 ... Batch: 200 ... Loss: 0.00168034\n",
      "------------------- Test set: Average loss: 472.4043 ... Samples: 810\n",
      "Train Epoch: 67 ... Batch: 0 ... Loss: 0.00142364\n",
      "Train Epoch: 67 ... Batch: 10 ... Loss: 0.00188553\n",
      "Train Epoch: 67 ... Batch: 20 ... Loss: 0.00159985\n",
      "Train Epoch: 67 ... Batch: 30 ... Loss: 0.00193992\n",
      "Train Epoch: 67 ... Batch: 40 ... Loss: 0.00169892\n",
      "Train Epoch: 67 ... Batch: 50 ... Loss: 0.00156020\n",
      "Train Epoch: 67 ... Batch: 60 ... Loss: 0.00198430\n",
      "Train Epoch: 67 ... Batch: 70 ... Loss: 0.00166086\n",
      "Train Epoch: 67 ... Batch: 80 ... Loss: 0.00205234\n",
      "Train Epoch: 67 ... Batch: 90 ... Loss: 0.00185323\n",
      "Train Epoch: 67 ... Batch: 100 ... Loss: 0.00185151\n",
      "Train Epoch: 67 ... Batch: 110 ... Loss: 0.00182971\n",
      "Train Epoch: 67 ... Batch: 120 ... Loss: 0.00166793\n",
      "Train Epoch: 67 ... Batch: 130 ... Loss: 0.00183776\n",
      "Train Epoch: 67 ... Batch: 140 ... Loss: 0.00236307\n",
      "Train Epoch: 67 ... Batch: 150 ... Loss: 0.00174904\n",
      "Train Epoch: 67 ... Batch: 160 ... Loss: 0.00181823\n",
      "Train Epoch: 67 ... Batch: 170 ... Loss: 0.00166938\n",
      "Train Epoch: 67 ... Batch: 180 ... Loss: 0.00181029\n",
      "Train Epoch: 67 ... Batch: 190 ... Loss: 0.00174576\n",
      "Train Epoch: 67 ... Batch: 200 ... Loss: 0.00169975\n",
      "------------------- Test set: Average loss: 472.5595 ... Samples: 810\n",
      "Train Epoch: 68 ... Batch: 0 ... Loss: 0.00180954\n",
      "Train Epoch: 68 ... Batch: 10 ... Loss: 0.00224618\n",
      "Train Epoch: 68 ... Batch: 20 ... Loss: 0.00205372\n",
      "Train Epoch: 68 ... Batch: 30 ... Loss: 0.00163602\n",
      "Train Epoch: 68 ... Batch: 40 ... Loss: 0.00193364\n",
      "Train Epoch: 68 ... Batch: 50 ... Loss: 0.00154653\n",
      "Train Epoch: 68 ... Batch: 60 ... Loss: 0.00218128\n",
      "Train Epoch: 68 ... Batch: 70 ... Loss: 0.00211759\n",
      "Train Epoch: 68 ... Batch: 80 ... Loss: 0.00204288\n",
      "Train Epoch: 68 ... Batch: 90 ... Loss: 0.00208365\n",
      "Train Epoch: 68 ... Batch: 100 ... Loss: 0.00199515\n",
      "Train Epoch: 68 ... Batch: 110 ... Loss: 0.00186109\n",
      "Train Epoch: 68 ... Batch: 120 ... Loss: 0.00231451\n",
      "Train Epoch: 68 ... Batch: 130 ... Loss: 0.00193394\n",
      "Train Epoch: 68 ... Batch: 140 ... Loss: 0.00196784\n",
      "Train Epoch: 68 ... Batch: 150 ... Loss: 0.00179824\n",
      "Train Epoch: 68 ... Batch: 160 ... Loss: 0.00194042\n",
      "Train Epoch: 68 ... Batch: 170 ... Loss: 0.00181282\n",
      "Train Epoch: 68 ... Batch: 180 ... Loss: 0.00178048\n",
      "Train Epoch: 68 ... Batch: 190 ... Loss: 0.00210222\n",
      "Train Epoch: 68 ... Batch: 200 ... Loss: 0.00190154\n",
      "------------------- Test set: Average loss: 472.5059 ... Samples: 810\n",
      "Train Epoch: 69 ... Batch: 0 ... Loss: 0.00169669\n",
      "Train Epoch: 69 ... Batch: 10 ... Loss: 0.00166045\n",
      "Train Epoch: 69 ... Batch: 20 ... Loss: 0.00197175\n",
      "Train Epoch: 69 ... Batch: 30 ... Loss: 0.00218283\n",
      "Train Epoch: 69 ... Batch: 40 ... Loss: 0.00183729\n",
      "Train Epoch: 69 ... Batch: 50 ... Loss: 0.00143915\n",
      "Train Epoch: 69 ... Batch: 60 ... Loss: 0.00184598\n",
      "Train Epoch: 69 ... Batch: 70 ... Loss: 0.00193222\n",
      "Train Epoch: 69 ... Batch: 80 ... Loss: 0.00161802\n",
      "Train Epoch: 69 ... Batch: 90 ... Loss: 0.00183598\n",
      "Train Epoch: 69 ... Batch: 100 ... Loss: 0.00182225\n",
      "Train Epoch: 69 ... Batch: 110 ... Loss: 0.00193786\n",
      "Train Epoch: 69 ... Batch: 120 ... Loss: 0.00215311\n",
      "Train Epoch: 69 ... Batch: 130 ... Loss: 0.00205383\n",
      "Train Epoch: 69 ... Batch: 140 ... Loss: 0.00178245\n",
      "Train Epoch: 69 ... Batch: 150 ... Loss: 0.00242477\n",
      "Train Epoch: 69 ... Batch: 160 ... Loss: 0.00176392\n",
      "Train Epoch: 69 ... Batch: 170 ... Loss: 0.00184659\n",
      "Train Epoch: 69 ... Batch: 180 ... Loss: 0.00187902\n",
      "Train Epoch: 69 ... Batch: 190 ... Loss: 0.00195600\n",
      "Train Epoch: 69 ... Batch: 200 ... Loss: 0.00158291\n",
      "------------------- Test set: Average loss: 472.2657 ... Samples: 810\n",
      "Train Epoch: 70 ... Batch: 0 ... Loss: 0.00240988\n",
      "Train Epoch: 70 ... Batch: 10 ... Loss: 0.00183915\n",
      "Train Epoch: 70 ... Batch: 20 ... Loss: 0.00206963\n",
      "Train Epoch: 70 ... Batch: 30 ... Loss: 0.00180162\n",
      "Train Epoch: 70 ... Batch: 40 ... Loss: 0.00182538\n",
      "Train Epoch: 70 ... Batch: 50 ... Loss: 0.00187726\n",
      "Train Epoch: 70 ... Batch: 60 ... Loss: 0.00193416\n",
      "Train Epoch: 70 ... Batch: 70 ... Loss: 0.00171430\n",
      "Train Epoch: 70 ... Batch: 80 ... Loss: 0.00202018\n",
      "Train Epoch: 70 ... Batch: 90 ... Loss: 0.00172264\n",
      "Train Epoch: 70 ... Batch: 100 ... Loss: 0.00226401\n",
      "Train Epoch: 70 ... Batch: 110 ... Loss: 0.00230733\n",
      "Train Epoch: 70 ... Batch: 120 ... Loss: 0.00176583\n",
      "Train Epoch: 70 ... Batch: 130 ... Loss: 0.00226957\n",
      "Train Epoch: 70 ... Batch: 140 ... Loss: 0.00210132\n",
      "Train Epoch: 70 ... Batch: 150 ... Loss: 0.00199804\n",
      "Train Epoch: 70 ... Batch: 160 ... Loss: 0.00181575\n",
      "Train Epoch: 70 ... Batch: 170 ... Loss: 0.00210013\n",
      "Train Epoch: 70 ... Batch: 180 ... Loss: 0.00188036\n",
      "Train Epoch: 70 ... Batch: 190 ... Loss: 0.00174907\n",
      "Train Epoch: 70 ... Batch: 200 ... Loss: 0.00195794\n",
      "------------------- Test set: Average loss: 471.7545 ... Samples: 810\n",
      "Train Epoch: 71 ... Batch: 0 ... Loss: 0.00206963\n",
      "Train Epoch: 71 ... Batch: 10 ... Loss: 0.00213658\n",
      "Train Epoch: 71 ... Batch: 20 ... Loss: 0.00222412\n",
      "Train Epoch: 71 ... Batch: 30 ... Loss: 0.00176255\n",
      "Train Epoch: 71 ... Batch: 40 ... Loss: 0.00184353\n",
      "Train Epoch: 71 ... Batch: 50 ... Loss: 0.00193958\n",
      "Train Epoch: 71 ... Batch: 60 ... Loss: 0.00175698\n",
      "Train Epoch: 71 ... Batch: 70 ... Loss: 0.00216369\n",
      "Train Epoch: 71 ... Batch: 80 ... Loss: 0.00189636\n",
      "Train Epoch: 71 ... Batch: 90 ... Loss: 0.00172784\n",
      "Train Epoch: 71 ... Batch: 100 ... Loss: 0.00225880\n",
      "Train Epoch: 71 ... Batch: 110 ... Loss: 0.00200908\n",
      "Train Epoch: 71 ... Batch: 120 ... Loss: 0.00170292\n",
      "Train Epoch: 71 ... Batch: 130 ... Loss: 0.00206276\n",
      "Train Epoch: 71 ... Batch: 140 ... Loss: 0.00229109\n",
      "Train Epoch: 71 ... Batch: 150 ... Loss: 0.00206070\n",
      "Train Epoch: 71 ... Batch: 160 ... Loss: 0.00217521\n",
      "Train Epoch: 71 ... Batch: 170 ... Loss: 0.00194830\n",
      "Train Epoch: 71 ... Batch: 180 ... Loss: 0.00194394\n",
      "Train Epoch: 71 ... Batch: 190 ... Loss: 0.00196748\n",
      "Train Epoch: 71 ... Batch: 200 ... Loss: 0.00193157\n",
      "------------------- Test set: Average loss: 472.1127 ... Samples: 810\n",
      "Train Epoch: 72 ... Batch: 0 ... Loss: 0.00176328\n",
      "Train Epoch: 72 ... Batch: 10 ... Loss: 0.00205403\n",
      "Train Epoch: 72 ... Batch: 20 ... Loss: 0.00184333\n",
      "Train Epoch: 72 ... Batch: 30 ... Loss: 0.00175129\n",
      "Train Epoch: 72 ... Batch: 40 ... Loss: 0.00208054\n",
      "Train Epoch: 72 ... Batch: 50 ... Loss: 0.00201100\n",
      "Train Epoch: 72 ... Batch: 60 ... Loss: 0.00160361\n",
      "Train Epoch: 72 ... Batch: 70 ... Loss: 0.00167919\n",
      "Train Epoch: 72 ... Batch: 80 ... Loss: 0.00185380\n",
      "Train Epoch: 72 ... Batch: 90 ... Loss: 0.00169150\n",
      "Train Epoch: 72 ... Batch: 100 ... Loss: 0.00222982\n",
      "Train Epoch: 72 ... Batch: 110 ... Loss: 0.00168274\n",
      "Train Epoch: 72 ... Batch: 120 ... Loss: 0.00162172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 72 ... Batch: 130 ... Loss: 0.00184524\n",
      "Train Epoch: 72 ... Batch: 140 ... Loss: 0.00180826\n",
      "Train Epoch: 72 ... Batch: 150 ... Loss: 0.00225564\n",
      "Train Epoch: 72 ... Batch: 160 ... Loss: 0.00197838\n",
      "Train Epoch: 72 ... Batch: 170 ... Loss: 0.00171803\n",
      "Train Epoch: 72 ... Batch: 180 ... Loss: 0.00189250\n",
      "Train Epoch: 72 ... Batch: 190 ... Loss: 0.00182488\n",
      "Train Epoch: 72 ... Batch: 200 ... Loss: 0.00202469\n",
      "------------------- Test set: Average loss: 472.5590 ... Samples: 810\n",
      "Train Epoch: 73 ... Batch: 0 ... Loss: 0.00173675\n",
      "Train Epoch: 73 ... Batch: 10 ... Loss: 0.00186756\n",
      "Train Epoch: 73 ... Batch: 20 ... Loss: 0.00179886\n",
      "Train Epoch: 73 ... Batch: 30 ... Loss: 0.00177491\n",
      "Train Epoch: 73 ... Batch: 40 ... Loss: 0.00162860\n",
      "Train Epoch: 73 ... Batch: 50 ... Loss: 0.00199914\n",
      "Train Epoch: 73 ... Batch: 60 ... Loss: 0.00178119\n",
      "Train Epoch: 73 ... Batch: 70 ... Loss: 0.00191374\n",
      "Train Epoch: 73 ... Batch: 80 ... Loss: 0.00138305\n",
      "Train Epoch: 73 ... Batch: 90 ... Loss: 0.00201064\n",
      "Train Epoch: 73 ... Batch: 100 ... Loss: 0.00174146\n",
      "Train Epoch: 73 ... Batch: 110 ... Loss: 0.00179456\n",
      "Train Epoch: 73 ... Batch: 120 ... Loss: 0.00170378\n",
      "Train Epoch: 73 ... Batch: 130 ... Loss: 0.00202571\n",
      "Train Epoch: 73 ... Batch: 140 ... Loss: 0.00185697\n",
      "Train Epoch: 73 ... Batch: 150 ... Loss: 0.00185852\n",
      "Train Epoch: 73 ... Batch: 160 ... Loss: 0.00201906\n",
      "Train Epoch: 73 ... Batch: 170 ... Loss: 0.00178510\n",
      "Train Epoch: 73 ... Batch: 180 ... Loss: 0.00164050\n",
      "Train Epoch: 73 ... Batch: 190 ... Loss: 0.00199182\n",
      "Train Epoch: 73 ... Batch: 200 ... Loss: 0.00211705\n",
      "------------------- Test set: Average loss: 471.5131 ... Samples: 810\n",
      "Train Epoch: 74 ... Batch: 0 ... Loss: 0.00195413\n",
      "Train Epoch: 74 ... Batch: 10 ... Loss: 0.00162663\n",
      "Train Epoch: 74 ... Batch: 20 ... Loss: 0.00177631\n",
      "Train Epoch: 74 ... Batch: 30 ... Loss: 0.00211032\n",
      "Train Epoch: 74 ... Batch: 40 ... Loss: 0.00225321\n",
      "Train Epoch: 74 ... Batch: 50 ... Loss: 0.00176097\n",
      "Train Epoch: 74 ... Batch: 60 ... Loss: 0.00187180\n",
      "Train Epoch: 74 ... Batch: 70 ... Loss: 0.00218309\n",
      "Train Epoch: 74 ... Batch: 80 ... Loss: 0.00204856\n",
      "Train Epoch: 74 ... Batch: 90 ... Loss: 0.00173454\n",
      "Train Epoch: 74 ... Batch: 100 ... Loss: 0.00221797\n",
      "Train Epoch: 74 ... Batch: 110 ... Loss: 0.00197075\n",
      "Train Epoch: 74 ... Batch: 120 ... Loss: 0.00192334\n",
      "Train Epoch: 74 ... Batch: 130 ... Loss: 0.00160716\n",
      "Train Epoch: 74 ... Batch: 140 ... Loss: 0.00177150\n",
      "Train Epoch: 74 ... Batch: 150 ... Loss: 0.00170675\n",
      "Train Epoch: 74 ... Batch: 160 ... Loss: 0.00193241\n",
      "Train Epoch: 74 ... Batch: 170 ... Loss: 0.00181518\n",
      "Train Epoch: 74 ... Batch: 180 ... Loss: 0.00205010\n",
      "Train Epoch: 74 ... Batch: 190 ... Loss: 0.00203786\n",
      "Train Epoch: 74 ... Batch: 200 ... Loss: 0.00187098\n",
      "------------------- Test set: Average loss: 471.6562 ... Samples: 810\n",
      "Train Epoch: 75 ... Batch: 0 ... Loss: 0.00187038\n",
      "Train Epoch: 75 ... Batch: 10 ... Loss: 0.00185967\n",
      "Train Epoch: 75 ... Batch: 20 ... Loss: 0.00185115\n",
      "Train Epoch: 75 ... Batch: 30 ... Loss: 0.00160391\n",
      "Train Epoch: 75 ... Batch: 40 ... Loss: 0.00167788\n",
      "Train Epoch: 75 ... Batch: 50 ... Loss: 0.00204625\n",
      "Train Epoch: 75 ... Batch: 60 ... Loss: 0.00192804\n",
      "Train Epoch: 75 ... Batch: 70 ... Loss: 0.00207997\n",
      "Train Epoch: 75 ... Batch: 80 ... Loss: 0.00165166\n",
      "Train Epoch: 75 ... Batch: 90 ... Loss: 0.00179874\n",
      "Train Epoch: 75 ... Batch: 100 ... Loss: 0.00186814\n",
      "Train Epoch: 75 ... Batch: 110 ... Loss: 0.00212026\n",
      "Train Epoch: 75 ... Batch: 120 ... Loss: 0.00182816\n",
      "Train Epoch: 75 ... Batch: 130 ... Loss: 0.00169434\n",
      "Train Epoch: 75 ... Batch: 140 ... Loss: 0.00181156\n",
      "Train Epoch: 75 ... Batch: 150 ... Loss: 0.00171971\n",
      "Train Epoch: 75 ... Batch: 160 ... Loss: 0.00199415\n",
      "Train Epoch: 75 ... Batch: 170 ... Loss: 0.00202867\n",
      "Train Epoch: 75 ... Batch: 180 ... Loss: 0.00201777\n",
      "Train Epoch: 75 ... Batch: 190 ... Loss: 0.00172630\n",
      "Train Epoch: 75 ... Batch: 200 ... Loss: 0.00184068\n",
      "------------------- Test set: Average loss: 471.7511 ... Samples: 810\n",
      "Train Epoch: 76 ... Batch: 0 ... Loss: 0.00202511\n",
      "Train Epoch: 76 ... Batch: 10 ... Loss: 0.00193238\n",
      "Train Epoch: 76 ... Batch: 20 ... Loss: 0.00225942\n",
      "Train Epoch: 76 ... Batch: 30 ... Loss: 0.00184730\n",
      "Train Epoch: 76 ... Batch: 40 ... Loss: 0.00237524\n",
      "Train Epoch: 76 ... Batch: 50 ... Loss: 0.00185183\n",
      "Train Epoch: 76 ... Batch: 60 ... Loss: 0.00168978\n",
      "Train Epoch: 76 ... Batch: 70 ... Loss: 0.00153339\n",
      "Train Epoch: 76 ... Batch: 80 ... Loss: 0.00200544\n",
      "Train Epoch: 76 ... Batch: 90 ... Loss: 0.00184022\n",
      "Train Epoch: 76 ... Batch: 100 ... Loss: 0.00192522\n",
      "Train Epoch: 76 ... Batch: 110 ... Loss: 0.00181499\n",
      "Train Epoch: 76 ... Batch: 120 ... Loss: 0.00185448\n",
      "Train Epoch: 76 ... Batch: 130 ... Loss: 0.00180985\n",
      "Train Epoch: 76 ... Batch: 140 ... Loss: 0.00200925\n",
      "Train Epoch: 76 ... Batch: 150 ... Loss: 0.00187054\n",
      "Train Epoch: 76 ... Batch: 160 ... Loss: 0.00148590\n",
      "Train Epoch: 76 ... Batch: 170 ... Loss: 0.00169499\n",
      "Train Epoch: 76 ... Batch: 180 ... Loss: 0.00179287\n",
      "Train Epoch: 76 ... Batch: 190 ... Loss: 0.00235292\n",
      "Train Epoch: 76 ... Batch: 200 ... Loss: 0.00185030\n",
      "------------------- Test set: Average loss: 471.6539 ... Samples: 810\n",
      "Train Epoch: 77 ... Batch: 0 ... Loss: 0.00200025\n",
      "Train Epoch: 77 ... Batch: 10 ... Loss: 0.00189376\n",
      "Train Epoch: 77 ... Batch: 20 ... Loss: 0.00198578\n",
      "Train Epoch: 77 ... Batch: 30 ... Loss: 0.00185474\n",
      "Train Epoch: 77 ... Batch: 40 ... Loss: 0.00188704\n",
      "Train Epoch: 77 ... Batch: 50 ... Loss: 0.00210330\n",
      "Train Epoch: 77 ... Batch: 60 ... Loss: 0.00179752\n",
      "Train Epoch: 77 ... Batch: 70 ... Loss: 0.00198227\n",
      "Train Epoch: 77 ... Batch: 80 ... Loss: 0.00177693\n",
      "Train Epoch: 77 ... Batch: 90 ... Loss: 0.00176707\n",
      "Train Epoch: 77 ... Batch: 100 ... Loss: 0.00186403\n",
      "Train Epoch: 77 ... Batch: 110 ... Loss: 0.00174598\n",
      "Train Epoch: 77 ... Batch: 120 ... Loss: 0.00215029\n",
      "Train Epoch: 77 ... Batch: 130 ... Loss: 0.00191370\n",
      "Train Epoch: 77 ... Batch: 140 ... Loss: 0.00185314\n",
      "Train Epoch: 77 ... Batch: 150 ... Loss: 0.00188214\n",
      "Train Epoch: 77 ... Batch: 160 ... Loss: 0.00204993\n",
      "Train Epoch: 77 ... Batch: 170 ... Loss: 0.00175225\n",
      "Train Epoch: 77 ... Batch: 180 ... Loss: 0.00168199\n",
      "Train Epoch: 77 ... Batch: 190 ... Loss: 0.00208103\n",
      "Train Epoch: 77 ... Batch: 200 ... Loss: 0.00168072\n",
      "------------------- Test set: Average loss: 470.8738 ... Samples: 810\n",
      "Train Epoch: 78 ... Batch: 0 ... Loss: 0.00196537\n",
      "Train Epoch: 78 ... Batch: 10 ... Loss: 0.00149949\n",
      "Train Epoch: 78 ... Batch: 20 ... Loss: 0.00180583\n",
      "Train Epoch: 78 ... Batch: 30 ... Loss: 0.00150643\n",
      "Train Epoch: 78 ... Batch: 40 ... Loss: 0.00168003\n",
      "Train Epoch: 78 ... Batch: 50 ... Loss: 0.00190812\n",
      "Train Epoch: 78 ... Batch: 60 ... Loss: 0.00166004\n",
      "Train Epoch: 78 ... Batch: 70 ... Loss: 0.00164726\n",
      "Train Epoch: 78 ... Batch: 80 ... Loss: 0.00184951\n",
      "Train Epoch: 78 ... Batch: 90 ... Loss: 0.00184030\n",
      "Train Epoch: 78 ... Batch: 100 ... Loss: 0.00184051\n",
      "Train Epoch: 78 ... Batch: 110 ... Loss: 0.00200018\n",
      "Train Epoch: 78 ... Batch: 120 ... Loss: 0.00233796\n",
      "Train Epoch: 78 ... Batch: 130 ... Loss: 0.00176473\n",
      "Train Epoch: 78 ... Batch: 140 ... Loss: 0.00194971\n",
      "Train Epoch: 78 ... Batch: 150 ... Loss: 0.00198402\n",
      "Train Epoch: 78 ... Batch: 160 ... Loss: 0.00204965\n",
      "Train Epoch: 78 ... Batch: 170 ... Loss: 0.00193317\n",
      "Train Epoch: 78 ... Batch: 180 ... Loss: 0.00209573\n",
      "Train Epoch: 78 ... Batch: 190 ... Loss: 0.00201483\n",
      "Train Epoch: 78 ... Batch: 200 ... Loss: 0.00223122\n",
      "------------------- Test set: Average loss: 470.9024 ... Samples: 810\n",
      "Train Epoch: 79 ... Batch: 0 ... Loss: 0.00182000\n",
      "Train Epoch: 79 ... Batch: 10 ... Loss: 0.00180105\n",
      "Train Epoch: 79 ... Batch: 20 ... Loss: 0.00201799\n",
      "Train Epoch: 79 ... Batch: 30 ... Loss: 0.00207762\n",
      "Train Epoch: 79 ... Batch: 40 ... Loss: 0.00204238\n",
      "Train Epoch: 79 ... Batch: 50 ... Loss: 0.00156287\n",
      "Train Epoch: 79 ... Batch: 60 ... Loss: 0.00199510\n",
      "Train Epoch: 79 ... Batch: 70 ... Loss: 0.00198371\n",
      "Train Epoch: 79 ... Batch: 80 ... Loss: 0.00199874\n",
      "Train Epoch: 79 ... Batch: 90 ... Loss: 0.00179359\n",
      "Train Epoch: 79 ... Batch: 100 ... Loss: 0.00190852\n",
      "Train Epoch: 79 ... Batch: 110 ... Loss: 0.00185737\n",
      "Train Epoch: 79 ... Batch: 120 ... Loss: 0.00187340\n",
      "Train Epoch: 79 ... Batch: 130 ... Loss: 0.00164615\n",
      "Train Epoch: 79 ... Batch: 140 ... Loss: 0.00191961\n",
      "Train Epoch: 79 ... Batch: 150 ... Loss: 0.00163831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 79 ... Batch: 160 ... Loss: 0.00164565\n",
      "Train Epoch: 79 ... Batch: 170 ... Loss: 0.00183595\n",
      "Train Epoch: 79 ... Batch: 180 ... Loss: 0.00192607\n",
      "Train Epoch: 79 ... Batch: 190 ... Loss: 0.00192022\n",
      "Train Epoch: 79 ... Batch: 200 ... Loss: 0.00195055\n",
      "------------------- Test set: Average loss: 470.6823 ... Samples: 810\n",
      "Train Epoch: 80 ... Batch: 0 ... Loss: 0.00191489\n",
      "Train Epoch: 80 ... Batch: 10 ... Loss: 0.00166306\n",
      "Train Epoch: 80 ... Batch: 20 ... Loss: 0.00191501\n",
      "Train Epoch: 80 ... Batch: 30 ... Loss: 0.00226489\n",
      "Train Epoch: 80 ... Batch: 40 ... Loss: 0.00215081\n",
      "Train Epoch: 80 ... Batch: 50 ... Loss: 0.00189342\n",
      "Train Epoch: 80 ... Batch: 60 ... Loss: 0.00181912\n",
      "Train Epoch: 80 ... Batch: 70 ... Loss: 0.00184509\n",
      "Train Epoch: 80 ... Batch: 80 ... Loss: 0.00207130\n",
      "Train Epoch: 80 ... Batch: 90 ... Loss: 0.00187516\n",
      "Train Epoch: 80 ... Batch: 100 ... Loss: 0.00199758\n",
      "Train Epoch: 80 ... Batch: 110 ... Loss: 0.00185666\n",
      "Train Epoch: 80 ... Batch: 120 ... Loss: 0.00169476\n",
      "Train Epoch: 80 ... Batch: 130 ... Loss: 0.00190702\n",
      "Train Epoch: 80 ... Batch: 140 ... Loss: 0.00202914\n",
      "Train Epoch: 80 ... Batch: 150 ... Loss: 0.00155577\n",
      "Train Epoch: 80 ... Batch: 160 ... Loss: 0.00181365\n",
      "Train Epoch: 80 ... Batch: 170 ... Loss: 0.00201676\n",
      "Train Epoch: 80 ... Batch: 180 ... Loss: 0.00181676\n",
      "Train Epoch: 80 ... Batch: 190 ... Loss: 0.00199895\n",
      "Train Epoch: 80 ... Batch: 200 ... Loss: 0.00188255\n",
      "------------------- Test set: Average loss: 470.7006 ... Samples: 810\n",
      "Train Epoch: 81 ... Batch: 0 ... Loss: 0.00194239\n",
      "Train Epoch: 81 ... Batch: 10 ... Loss: 0.00233894\n",
      "Train Epoch: 81 ... Batch: 20 ... Loss: 0.00178801\n",
      "Train Epoch: 81 ... Batch: 30 ... Loss: 0.00202076\n",
      "Train Epoch: 81 ... Batch: 40 ... Loss: 0.00203248\n",
      "Train Epoch: 81 ... Batch: 50 ... Loss: 0.00190114\n",
      "Train Epoch: 81 ... Batch: 60 ... Loss: 0.00174974\n",
      "Train Epoch: 81 ... Batch: 70 ... Loss: 0.00185793\n",
      "Train Epoch: 81 ... Batch: 80 ... Loss: 0.00168139\n",
      "Train Epoch: 81 ... Batch: 90 ... Loss: 0.00168236\n",
      "Train Epoch: 81 ... Batch: 100 ... Loss: 0.00143250\n",
      "Train Epoch: 81 ... Batch: 110 ... Loss: 0.00192215\n",
      "Train Epoch: 81 ... Batch: 120 ... Loss: 0.00175320\n",
      "Train Epoch: 81 ... Batch: 130 ... Loss: 0.00185997\n",
      "Train Epoch: 81 ... Batch: 140 ... Loss: 0.00236266\n",
      "Train Epoch: 81 ... Batch: 150 ... Loss: 0.00170982\n",
      "Train Epoch: 81 ... Batch: 160 ... Loss: 0.00196662\n",
      "Train Epoch: 81 ... Batch: 170 ... Loss: 0.00173014\n",
      "Train Epoch: 81 ... Batch: 180 ... Loss: 0.00201400\n",
      "Train Epoch: 81 ... Batch: 190 ... Loss: 0.00170559\n",
      "Train Epoch: 81 ... Batch: 200 ... Loss: 0.00198652\n",
      "------------------- Test set: Average loss: 470.7039 ... Samples: 810\n",
      "Train Epoch: 82 ... Batch: 0 ... Loss: 0.00175020\n",
      "Train Epoch: 82 ... Batch: 10 ... Loss: 0.00208776\n",
      "Train Epoch: 82 ... Batch: 20 ... Loss: 0.00191033\n",
      "Train Epoch: 82 ... Batch: 30 ... Loss: 0.00181954\n",
      "Train Epoch: 82 ... Batch: 40 ... Loss: 0.00182559\n",
      "Train Epoch: 82 ... Batch: 50 ... Loss: 0.00173695\n",
      "Train Epoch: 82 ... Batch: 60 ... Loss: 0.00158921\n",
      "Train Epoch: 82 ... Batch: 70 ... Loss: 0.00175465\n",
      "Train Epoch: 82 ... Batch: 80 ... Loss: 0.00183045\n",
      "Train Epoch: 82 ... Batch: 90 ... Loss: 0.00203082\n",
      "Train Epoch: 82 ... Batch: 100 ... Loss: 0.00189186\n",
      "Train Epoch: 82 ... Batch: 110 ... Loss: 0.00204570\n",
      "Train Epoch: 82 ... Batch: 120 ... Loss: 0.00171663\n",
      "Train Epoch: 82 ... Batch: 130 ... Loss: 0.00178192\n",
      "Train Epoch: 82 ... Batch: 140 ... Loss: 0.00226155\n",
      "Train Epoch: 82 ... Batch: 150 ... Loss: 0.00178001\n",
      "Train Epoch: 82 ... Batch: 160 ... Loss: 0.00146389\n",
      "Train Epoch: 82 ... Batch: 170 ... Loss: 0.00171119\n",
      "Train Epoch: 82 ... Batch: 180 ... Loss: 0.00160452\n",
      "Train Epoch: 82 ... Batch: 190 ... Loss: 0.00190062\n",
      "Train Epoch: 82 ... Batch: 200 ... Loss: 0.00166754\n",
      "------------------- Test set: Average loss: 470.6257 ... Samples: 810\n",
      "Train Epoch: 83 ... Batch: 0 ... Loss: 0.00201250\n",
      "Train Epoch: 83 ... Batch: 10 ... Loss: 0.00186534\n",
      "Train Epoch: 83 ... Batch: 20 ... Loss: 0.00215374\n",
      "Train Epoch: 83 ... Batch: 30 ... Loss: 0.00197010\n",
      "Train Epoch: 83 ... Batch: 40 ... Loss: 0.00187493\n",
      "Train Epoch: 83 ... Batch: 50 ... Loss: 0.00160644\n",
      "Train Epoch: 83 ... Batch: 60 ... Loss: 0.00193222\n",
      "Train Epoch: 83 ... Batch: 70 ... Loss: 0.00169610\n",
      "Train Epoch: 83 ... Batch: 80 ... Loss: 0.00174528\n",
      "Train Epoch: 83 ... Batch: 90 ... Loss: 0.00222552\n",
      "Train Epoch: 83 ... Batch: 100 ... Loss: 0.00191780\n",
      "Train Epoch: 83 ... Batch: 110 ... Loss: 0.00164105\n",
      "Train Epoch: 83 ... Batch: 120 ... Loss: 0.00188168\n",
      "Train Epoch: 83 ... Batch: 130 ... Loss: 0.00171480\n",
      "Train Epoch: 83 ... Batch: 140 ... Loss: 0.00156124\n",
      "Train Epoch: 83 ... Batch: 150 ... Loss: 0.00190079\n",
      "Train Epoch: 83 ... Batch: 160 ... Loss: 0.00210646\n",
      "Train Epoch: 83 ... Batch: 170 ... Loss: 0.00199314\n",
      "Train Epoch: 83 ... Batch: 180 ... Loss: 0.00198213\n",
      "Train Epoch: 83 ... Batch: 190 ... Loss: 0.00148017\n",
      "Train Epoch: 83 ... Batch: 200 ... Loss: 0.00199113\n",
      "------------------- Test set: Average loss: 470.2538 ... Samples: 810\n",
      "Train Epoch: 84 ... Batch: 0 ... Loss: 0.00167576\n",
      "Train Epoch: 84 ... Batch: 10 ... Loss: 0.00184390\n",
      "Train Epoch: 84 ... Batch: 20 ... Loss: 0.00184028\n",
      "Train Epoch: 84 ... Batch: 30 ... Loss: 0.00189162\n",
      "Train Epoch: 84 ... Batch: 40 ... Loss: 0.00196305\n",
      "Train Epoch: 84 ... Batch: 50 ... Loss: 0.00214184\n",
      "Train Epoch: 84 ... Batch: 60 ... Loss: 0.00186536\n",
      "Train Epoch: 84 ... Batch: 70 ... Loss: 0.00152368\n",
      "Train Epoch: 84 ... Batch: 80 ... Loss: 0.00150440\n",
      "Train Epoch: 84 ... Batch: 90 ... Loss: 0.00177305\n",
      "Train Epoch: 84 ... Batch: 100 ... Loss: 0.00230130\n",
      "Train Epoch: 84 ... Batch: 110 ... Loss: 0.00191770\n",
      "Train Epoch: 84 ... Batch: 120 ... Loss: 0.00183527\n",
      "Train Epoch: 84 ... Batch: 130 ... Loss: 0.00204632\n",
      "Train Epoch: 84 ... Batch: 140 ... Loss: 0.00208267\n",
      "Train Epoch: 84 ... Batch: 150 ... Loss: 0.00215583\n",
      "Train Epoch: 84 ... Batch: 160 ... Loss: 0.00191853\n",
      "Train Epoch: 84 ... Batch: 170 ... Loss: 0.00178520\n",
      "Train Epoch: 84 ... Batch: 180 ... Loss: 0.00213991\n",
      "Train Epoch: 84 ... Batch: 190 ... Loss: 0.00184984\n",
      "Train Epoch: 84 ... Batch: 200 ... Loss: 0.00186879\n",
      "------------------- Test set: Average loss: 470.4431 ... Samples: 810\n",
      "Train Epoch: 85 ... Batch: 0 ... Loss: 0.00194213\n",
      "Train Epoch: 85 ... Batch: 10 ... Loss: 0.00191799\n",
      "Train Epoch: 85 ... Batch: 20 ... Loss: 0.00206546\n",
      "Train Epoch: 85 ... Batch: 30 ... Loss: 0.00188368\n",
      "Train Epoch: 85 ... Batch: 40 ... Loss: 0.00174573\n",
      "Train Epoch: 85 ... Batch: 50 ... Loss: 0.00187440\n",
      "Train Epoch: 85 ... Batch: 60 ... Loss: 0.00151908\n",
      "Train Epoch: 85 ... Batch: 70 ... Loss: 0.00217192\n",
      "Train Epoch: 85 ... Batch: 80 ... Loss: 0.00175736\n",
      "Train Epoch: 85 ... Batch: 90 ... Loss: 0.00161497\n",
      "Train Epoch: 85 ... Batch: 100 ... Loss: 0.00179377\n",
      "Train Epoch: 85 ... Batch: 110 ... Loss: 0.00174431\n",
      "Train Epoch: 85 ... Batch: 120 ... Loss: 0.00174312\n",
      "Train Epoch: 85 ... Batch: 130 ... Loss: 0.00165876\n",
      "Train Epoch: 85 ... Batch: 140 ... Loss: 0.00170364\n",
      "Train Epoch: 85 ... Batch: 150 ... Loss: 0.00212223\n",
      "Train Epoch: 85 ... Batch: 160 ... Loss: 0.00212516\n",
      "Train Epoch: 85 ... Batch: 170 ... Loss: 0.00219249\n",
      "Train Epoch: 85 ... Batch: 180 ... Loss: 0.00186422\n",
      "Train Epoch: 85 ... Batch: 190 ... Loss: 0.00187107\n",
      "Train Epoch: 85 ... Batch: 200 ... Loss: 0.00193578\n",
      "------------------- Test set: Average loss: 469.9690 ... Samples: 810\n",
      "Train Epoch: 86 ... Batch: 0 ... Loss: 0.00165283\n",
      "Train Epoch: 86 ... Batch: 10 ... Loss: 0.00165398\n",
      "Train Epoch: 86 ... Batch: 20 ... Loss: 0.00179863\n",
      "Train Epoch: 86 ... Batch: 30 ... Loss: 0.00163984\n",
      "Train Epoch: 86 ... Batch: 40 ... Loss: 0.00167438\n",
      "Train Epoch: 86 ... Batch: 50 ... Loss: 0.00175214\n",
      "Train Epoch: 86 ... Batch: 60 ... Loss: 0.00194775\n",
      "Train Epoch: 86 ... Batch: 70 ... Loss: 0.00159081\n",
      "Train Epoch: 86 ... Batch: 80 ... Loss: 0.00164605\n",
      "Train Epoch: 86 ... Batch: 90 ... Loss: 0.00189304\n",
      "Train Epoch: 86 ... Batch: 100 ... Loss: 0.00213792\n",
      "Train Epoch: 86 ... Batch: 110 ... Loss: 0.00170283\n",
      "Train Epoch: 86 ... Batch: 120 ... Loss: 0.00245988\n",
      "Train Epoch: 86 ... Batch: 130 ... Loss: 0.00198912\n",
      "Train Epoch: 86 ... Batch: 140 ... Loss: 0.00152764\n",
      "Train Epoch: 86 ... Batch: 150 ... Loss: 0.00245677\n",
      "Train Epoch: 86 ... Batch: 160 ... Loss: 0.00173134\n",
      "Train Epoch: 86 ... Batch: 170 ... Loss: 0.00145178\n",
      "Train Epoch: 86 ... Batch: 180 ... Loss: 0.00186640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 86 ... Batch: 190 ... Loss: 0.00188333\n",
      "Train Epoch: 86 ... Batch: 200 ... Loss: 0.00231074\n",
      "------------------- Test set: Average loss: 469.9995 ... Samples: 810\n",
      "Train Epoch: 87 ... Batch: 0 ... Loss: 0.00234154\n",
      "Train Epoch: 87 ... Batch: 10 ... Loss: 0.00186998\n",
      "Train Epoch: 87 ... Batch: 20 ... Loss: 0.00181744\n",
      "Train Epoch: 87 ... Batch: 30 ... Loss: 0.00203814\n",
      "Train Epoch: 87 ... Batch: 40 ... Loss: 0.00183600\n",
      "Train Epoch: 87 ... Batch: 50 ... Loss: 0.00178718\n",
      "Train Epoch: 87 ... Batch: 60 ... Loss: 0.00192150\n",
      "Train Epoch: 87 ... Batch: 70 ... Loss: 0.00185514\n",
      "Train Epoch: 87 ... Batch: 80 ... Loss: 0.00172739\n",
      "Train Epoch: 87 ... Batch: 90 ... Loss: 0.00193571\n",
      "Train Epoch: 87 ... Batch: 100 ... Loss: 0.00180119\n",
      "Train Epoch: 87 ... Batch: 110 ... Loss: 0.00173921\n",
      "Train Epoch: 87 ... Batch: 120 ... Loss: 0.00178733\n",
      "Train Epoch: 87 ... Batch: 130 ... Loss: 0.00217697\n",
      "Train Epoch: 87 ... Batch: 140 ... Loss: 0.00172367\n",
      "Train Epoch: 87 ... Batch: 150 ... Loss: 0.00196254\n",
      "Train Epoch: 87 ... Batch: 160 ... Loss: 0.00194820\n",
      "Train Epoch: 87 ... Batch: 170 ... Loss: 0.00195184\n",
      "Train Epoch: 87 ... Batch: 180 ... Loss: 0.00190123\n",
      "Train Epoch: 87 ... Batch: 190 ... Loss: 0.00162718\n",
      "Train Epoch: 87 ... Batch: 200 ... Loss: 0.00163294\n",
      "------------------- Test set: Average loss: 469.5157 ... Samples: 810\n",
      "Train Epoch: 88 ... Batch: 0 ... Loss: 0.00183916\n",
      "Train Epoch: 88 ... Batch: 10 ... Loss: 0.00239427\n",
      "Train Epoch: 88 ... Batch: 20 ... Loss: 0.00187217\n",
      "Train Epoch: 88 ... Batch: 30 ... Loss: 0.00176524\n",
      "Train Epoch: 88 ... Batch: 40 ... Loss: 0.00181388\n",
      "Train Epoch: 88 ... Batch: 50 ... Loss: 0.00185648\n",
      "Train Epoch: 88 ... Batch: 60 ... Loss: 0.00189183\n",
      "Train Epoch: 88 ... Batch: 70 ... Loss: 0.00195322\n",
      "Train Epoch: 88 ... Batch: 80 ... Loss: 0.00165204\n",
      "Train Epoch: 88 ... Batch: 90 ... Loss: 0.00199533\n",
      "Train Epoch: 88 ... Batch: 100 ... Loss: 0.00188033\n",
      "Train Epoch: 88 ... Batch: 110 ... Loss: 0.00166651\n",
      "Train Epoch: 88 ... Batch: 120 ... Loss: 0.00204047\n",
      "Train Epoch: 88 ... Batch: 130 ... Loss: 0.00209377\n",
      "Train Epoch: 88 ... Batch: 140 ... Loss: 0.00174684\n",
      "Train Epoch: 88 ... Batch: 150 ... Loss: 0.00186268\n",
      "Train Epoch: 88 ... Batch: 160 ... Loss: 0.00164878\n",
      "Train Epoch: 88 ... Batch: 170 ... Loss: 0.00175006\n",
      "Train Epoch: 88 ... Batch: 180 ... Loss: 0.00216670\n",
      "Train Epoch: 88 ... Batch: 190 ... Loss: 0.00192819\n",
      "Train Epoch: 88 ... Batch: 200 ... Loss: 0.00177047\n",
      "------------------- Test set: Average loss: 469.4841 ... Samples: 810\n",
      "Train Epoch: 89 ... Batch: 0 ... Loss: 0.00162704\n",
      "Train Epoch: 89 ... Batch: 10 ... Loss: 0.00236503\n",
      "Train Epoch: 89 ... Batch: 20 ... Loss: 0.00182687\n",
      "Train Epoch: 89 ... Batch: 30 ... Loss: 0.00204321\n",
      "Train Epoch: 89 ... Batch: 40 ... Loss: 0.00212468\n",
      "Train Epoch: 89 ... Batch: 50 ... Loss: 0.00189543\n",
      "Train Epoch: 89 ... Batch: 60 ... Loss: 0.00190913\n",
      "Train Epoch: 89 ... Batch: 70 ... Loss: 0.00178385\n",
      "Train Epoch: 89 ... Batch: 80 ... Loss: 0.00178630\n",
      "Train Epoch: 89 ... Batch: 90 ... Loss: 0.00184393\n",
      "Train Epoch: 89 ... Batch: 100 ... Loss: 0.00211864\n",
      "Train Epoch: 89 ... Batch: 110 ... Loss: 0.00176887\n",
      "Train Epoch: 89 ... Batch: 120 ... Loss: 0.00164974\n",
      "Train Epoch: 89 ... Batch: 130 ... Loss: 0.00175705\n",
      "Train Epoch: 89 ... Batch: 140 ... Loss: 0.00171367\n",
      "Train Epoch: 89 ... Batch: 150 ... Loss: 0.00228430\n",
      "Train Epoch: 89 ... Batch: 160 ... Loss: 0.00206571\n",
      "Train Epoch: 89 ... Batch: 170 ... Loss: 0.00169224\n",
      "Train Epoch: 89 ... Batch: 180 ... Loss: 0.00200714\n",
      "Train Epoch: 89 ... Batch: 190 ... Loss: 0.00188975\n",
      "Train Epoch: 89 ... Batch: 200 ... Loss: 0.00192513\n",
      "------------------- Test set: Average loss: 469.7393 ... Samples: 810\n",
      "Train Epoch: 90 ... Batch: 0 ... Loss: 0.00178275\n",
      "Train Epoch: 90 ... Batch: 10 ... Loss: 0.00193520\n",
      "Train Epoch: 90 ... Batch: 20 ... Loss: 0.00184689\n",
      "Train Epoch: 90 ... Batch: 30 ... Loss: 0.00208986\n",
      "Train Epoch: 90 ... Batch: 40 ... Loss: 0.00175098\n",
      "Train Epoch: 90 ... Batch: 50 ... Loss: 0.00204876\n",
      "Train Epoch: 90 ... Batch: 60 ... Loss: 0.00180753\n",
      "Train Epoch: 90 ... Batch: 70 ... Loss: 0.00171270\n",
      "Train Epoch: 90 ... Batch: 80 ... Loss: 0.00172577\n",
      "Train Epoch: 90 ... Batch: 90 ... Loss: 0.00186338\n",
      "Train Epoch: 90 ... Batch: 100 ... Loss: 0.00178926\n",
      "Train Epoch: 90 ... Batch: 110 ... Loss: 0.00182703\n",
      "Train Epoch: 90 ... Batch: 120 ... Loss: 0.00185534\n",
      "Train Epoch: 90 ... Batch: 130 ... Loss: 0.00211131\n",
      "Train Epoch: 90 ... Batch: 140 ... Loss: 0.00183811\n",
      "Train Epoch: 90 ... Batch: 150 ... Loss: 0.00167889\n",
      "Train Epoch: 90 ... Batch: 160 ... Loss: 0.00200148\n",
      "Train Epoch: 90 ... Batch: 170 ... Loss: 0.00184082\n",
      "Train Epoch: 90 ... Batch: 180 ... Loss: 0.00185267\n",
      "Train Epoch: 90 ... Batch: 190 ... Loss: 0.00173966\n",
      "Train Epoch: 90 ... Batch: 200 ... Loss: 0.00212750\n",
      "------------------- Test set: Average loss: 469.6191 ... Samples: 810\n",
      "Train Epoch: 91 ... Batch: 0 ... Loss: 0.00167372\n",
      "Train Epoch: 91 ... Batch: 10 ... Loss: 0.00197638\n",
      "Train Epoch: 91 ... Batch: 20 ... Loss: 0.00165738\n",
      "Train Epoch: 91 ... Batch: 30 ... Loss: 0.00187350\n",
      "Train Epoch: 91 ... Batch: 40 ... Loss: 0.00194737\n",
      "Train Epoch: 91 ... Batch: 50 ... Loss: 0.00192690\n",
      "Train Epoch: 91 ... Batch: 60 ... Loss: 0.00215331\n",
      "Train Epoch: 91 ... Batch: 70 ... Loss: 0.00166527\n",
      "Train Epoch: 91 ... Batch: 80 ... Loss: 0.00165127\n",
      "Train Epoch: 91 ... Batch: 90 ... Loss: 0.00195021\n",
      "Train Epoch: 91 ... Batch: 100 ... Loss: 0.00181537\n",
      "Train Epoch: 91 ... Batch: 110 ... Loss: 0.00195624\n",
      "Train Epoch: 91 ... Batch: 120 ... Loss: 0.00188333\n",
      "Train Epoch: 91 ... Batch: 130 ... Loss: 0.00155059\n",
      "Train Epoch: 91 ... Batch: 140 ... Loss: 0.00208761\n",
      "Train Epoch: 91 ... Batch: 150 ... Loss: 0.00216345\n",
      "Train Epoch: 91 ... Batch: 160 ... Loss: 0.00185461\n",
      "Train Epoch: 91 ... Batch: 170 ... Loss: 0.00155222\n",
      "Train Epoch: 91 ... Batch: 180 ... Loss: 0.00156575\n",
      "Train Epoch: 91 ... Batch: 190 ... Loss: 0.00171852\n",
      "Train Epoch: 91 ... Batch: 200 ... Loss: 0.00157268\n",
      "------------------- Test set: Average loss: 469.1365 ... Samples: 810\n",
      "Train Epoch: 92 ... Batch: 0 ... Loss: 0.00189653\n",
      "Train Epoch: 92 ... Batch: 10 ... Loss: 0.00164997\n",
      "Train Epoch: 92 ... Batch: 20 ... Loss: 0.00184510\n",
      "Train Epoch: 92 ... Batch: 30 ... Loss: 0.00183753\n",
      "Train Epoch: 92 ... Batch: 40 ... Loss: 0.00189788\n",
      "Train Epoch: 92 ... Batch: 50 ... Loss: 0.00175050\n",
      "Train Epoch: 92 ... Batch: 60 ... Loss: 0.00188684\n",
      "Train Epoch: 92 ... Batch: 70 ... Loss: 0.00195308\n",
      "Train Epoch: 92 ... Batch: 80 ... Loss: 0.00180498\n",
      "Train Epoch: 92 ... Batch: 90 ... Loss: 0.00187091\n",
      "Train Epoch: 92 ... Batch: 100 ... Loss: 0.00191026\n",
      "Train Epoch: 92 ... Batch: 110 ... Loss: 0.00178229\n",
      "Train Epoch: 92 ... Batch: 120 ... Loss: 0.00173348\n",
      "Train Epoch: 92 ... Batch: 130 ... Loss: 0.00186296\n",
      "Train Epoch: 92 ... Batch: 140 ... Loss: 0.00172454\n",
      "Train Epoch: 92 ... Batch: 150 ... Loss: 0.00180490\n",
      "Train Epoch: 92 ... Batch: 160 ... Loss: 0.00200971\n",
      "Train Epoch: 92 ... Batch: 170 ... Loss: 0.00199317\n",
      "Train Epoch: 92 ... Batch: 180 ... Loss: 0.00175188\n",
      "Train Epoch: 92 ... Batch: 190 ... Loss: 0.00155190\n",
      "Train Epoch: 92 ... Batch: 200 ... Loss: 0.00209783\n",
      "------------------- Test set: Average loss: 469.0713 ... Samples: 810\n",
      "Train Epoch: 93 ... Batch: 0 ... Loss: 0.00199286\n",
      "Train Epoch: 93 ... Batch: 10 ... Loss: 0.00165086\n",
      "Train Epoch: 93 ... Batch: 20 ... Loss: 0.00171602\n",
      "Train Epoch: 93 ... Batch: 30 ... Loss: 0.00189447\n",
      "Train Epoch: 93 ... Batch: 40 ... Loss: 0.00179808\n",
      "Train Epoch: 93 ... Batch: 50 ... Loss: 0.00223903\n",
      "Train Epoch: 93 ... Batch: 60 ... Loss: 0.00182239\n",
      "Train Epoch: 93 ... Batch: 70 ... Loss: 0.00175265\n",
      "Train Epoch: 93 ... Batch: 80 ... Loss: 0.00165770\n",
      "Train Epoch: 93 ... Batch: 90 ... Loss: 0.00150341\n",
      "Train Epoch: 93 ... Batch: 100 ... Loss: 0.00188403\n",
      "Train Epoch: 93 ... Batch: 110 ... Loss: 0.00151900\n",
      "Train Epoch: 93 ... Batch: 120 ... Loss: 0.00185219\n",
      "Train Epoch: 93 ... Batch: 130 ... Loss: 0.00157179\n",
      "Train Epoch: 93 ... Batch: 140 ... Loss: 0.00193726\n",
      "Train Epoch: 93 ... Batch: 150 ... Loss: 0.00183560\n",
      "Train Epoch: 93 ... Batch: 160 ... Loss: 0.00173807\n",
      "Train Epoch: 93 ... Batch: 170 ... Loss: 0.00206413\n",
      "Train Epoch: 93 ... Batch: 180 ... Loss: 0.00213648\n",
      "Train Epoch: 93 ... Batch: 190 ... Loss: 0.00159457\n",
      "Train Epoch: 93 ... Batch: 200 ... Loss: 0.00171383\n",
      "------------------- Test set: Average loss: 468.4241 ... Samples: 810\n",
      "Train Epoch: 94 ... Batch: 0 ... Loss: 0.00171729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 94 ... Batch: 10 ... Loss: 0.00189350\n",
      "Train Epoch: 94 ... Batch: 20 ... Loss: 0.00177964\n",
      "Train Epoch: 94 ... Batch: 30 ... Loss: 0.00225694\n",
      "Train Epoch: 94 ... Batch: 40 ... Loss: 0.00182088\n",
      "Train Epoch: 94 ... Batch: 50 ... Loss: 0.00158854\n",
      "Train Epoch: 94 ... Batch: 60 ... Loss: 0.00190734\n",
      "Train Epoch: 94 ... Batch: 70 ... Loss: 0.00224461\n",
      "Train Epoch: 94 ... Batch: 80 ... Loss: 0.00177675\n",
      "Train Epoch: 94 ... Batch: 90 ... Loss: 0.00190811\n",
      "Train Epoch: 94 ... Batch: 100 ... Loss: 0.00194768\n",
      "Train Epoch: 94 ... Batch: 110 ... Loss: 0.00187729\n",
      "Train Epoch: 94 ... Batch: 120 ... Loss: 0.00181214\n",
      "Train Epoch: 94 ... Batch: 130 ... Loss: 0.00165992\n",
      "Train Epoch: 94 ... Batch: 140 ... Loss: 0.00203057\n",
      "Train Epoch: 94 ... Batch: 150 ... Loss: 0.00195832\n",
      "Train Epoch: 94 ... Batch: 160 ... Loss: 0.00188965\n",
      "Train Epoch: 94 ... Batch: 170 ... Loss: 0.00197442\n",
      "Train Epoch: 94 ... Batch: 180 ... Loss: 0.00173649\n",
      "Train Epoch: 94 ... Batch: 190 ... Loss: 0.00205496\n",
      "Train Epoch: 94 ... Batch: 200 ... Loss: 0.00184410\n",
      "------------------- Test set: Average loss: 468.8713 ... Samples: 810\n",
      "Train Epoch: 95 ... Batch: 0 ... Loss: 0.00178714\n",
      "Train Epoch: 95 ... Batch: 10 ... Loss: 0.00211618\n",
      "Train Epoch: 95 ... Batch: 20 ... Loss: 0.00196816\n",
      "Train Epoch: 95 ... Batch: 30 ... Loss: 0.00192088\n",
      "Train Epoch: 95 ... Batch: 40 ... Loss: 0.00180630\n",
      "Train Epoch: 95 ... Batch: 50 ... Loss: 0.00193082\n",
      "Train Epoch: 95 ... Batch: 60 ... Loss: 0.00191151\n",
      "Train Epoch: 95 ... Batch: 70 ... Loss: 0.00165407\n",
      "Train Epoch: 95 ... Batch: 80 ... Loss: 0.00137714\n",
      "Train Epoch: 95 ... Batch: 90 ... Loss: 0.00141121\n",
      "Train Epoch: 95 ... Batch: 100 ... Loss: 0.00212389\n",
      "Train Epoch: 95 ... Batch: 110 ... Loss: 0.00178516\n",
      "Train Epoch: 95 ... Batch: 120 ... Loss: 0.00166393\n",
      "Train Epoch: 95 ... Batch: 130 ... Loss: 0.00176261\n",
      "Train Epoch: 95 ... Batch: 140 ... Loss: 0.00221336\n",
      "Train Epoch: 95 ... Batch: 150 ... Loss: 0.00184422\n",
      "Train Epoch: 95 ... Batch: 160 ... Loss: 0.00184699\n",
      "Train Epoch: 95 ... Batch: 170 ... Loss: 0.00187833\n",
      "Train Epoch: 95 ... Batch: 180 ... Loss: 0.00182932\n",
      "Train Epoch: 95 ... Batch: 190 ... Loss: 0.00184383\n",
      "Train Epoch: 95 ... Batch: 200 ... Loss: 0.00206516\n",
      "------------------- Test set: Average loss: 469.0536 ... Samples: 810\n",
      "Train Epoch: 96 ... Batch: 0 ... Loss: 0.00165264\n",
      "Train Epoch: 96 ... Batch: 10 ... Loss: 0.00178248\n",
      "Train Epoch: 96 ... Batch: 20 ... Loss: 0.00197676\n",
      "Train Epoch: 96 ... Batch: 30 ... Loss: 0.00204140\n",
      "Train Epoch: 96 ... Batch: 40 ... Loss: 0.00170698\n",
      "Train Epoch: 96 ... Batch: 50 ... Loss: 0.00165214\n",
      "Train Epoch: 96 ... Batch: 60 ... Loss: 0.00207152\n",
      "Train Epoch: 96 ... Batch: 70 ... Loss: 0.00181883\n",
      "Train Epoch: 96 ... Batch: 80 ... Loss: 0.00210654\n",
      "Train Epoch: 96 ... Batch: 90 ... Loss: 0.00194835\n",
      "Train Epoch: 96 ... Batch: 100 ... Loss: 0.00233905\n",
      "Train Epoch: 96 ... Batch: 110 ... Loss: 0.00191947\n",
      "Train Epoch: 96 ... Batch: 120 ... Loss: 0.00173160\n",
      "Train Epoch: 96 ... Batch: 130 ... Loss: 0.00172558\n",
      "Train Epoch: 96 ... Batch: 140 ... Loss: 0.00184305\n",
      "Train Epoch: 96 ... Batch: 150 ... Loss: 0.00190385\n",
      "Train Epoch: 96 ... Batch: 160 ... Loss: 0.00184396\n",
      "Train Epoch: 96 ... Batch: 170 ... Loss: 0.00199187\n",
      "Train Epoch: 96 ... Batch: 180 ... Loss: 0.00170783\n",
      "Train Epoch: 96 ... Batch: 190 ... Loss: 0.00193203\n",
      "Train Epoch: 96 ... Batch: 200 ... Loss: 0.00220445\n",
      "------------------- Test set: Average loss: 469.1054 ... Samples: 810\n",
      "Train Epoch: 97 ... Batch: 0 ... Loss: 0.00184695\n",
      "Train Epoch: 97 ... Batch: 10 ... Loss: 0.00186336\n",
      "Train Epoch: 97 ... Batch: 20 ... Loss: 0.00239307\n",
      "Train Epoch: 97 ... Batch: 30 ... Loss: 0.00196860\n",
      "Train Epoch: 97 ... Batch: 40 ... Loss: 0.00195914\n",
      "Train Epoch: 97 ... Batch: 50 ... Loss: 0.00186886\n",
      "Train Epoch: 97 ... Batch: 60 ... Loss: 0.00181718\n",
      "Train Epoch: 97 ... Batch: 70 ... Loss: 0.00169295\n",
      "Train Epoch: 97 ... Batch: 80 ... Loss: 0.00177133\n",
      "Train Epoch: 97 ... Batch: 90 ... Loss: 0.00180089\n",
      "Train Epoch: 97 ... Batch: 100 ... Loss: 0.00185375\n",
      "Train Epoch: 97 ... Batch: 110 ... Loss: 0.00194650\n",
      "Train Epoch: 97 ... Batch: 120 ... Loss: 0.00162079\n",
      "Train Epoch: 97 ... Batch: 130 ... Loss: 0.00213034\n",
      "Train Epoch: 97 ... Batch: 140 ... Loss: 0.00142639\n",
      "Train Epoch: 97 ... Batch: 150 ... Loss: 0.00183091\n",
      "Train Epoch: 97 ... Batch: 160 ... Loss: 0.00155986\n",
      "Train Epoch: 97 ... Batch: 170 ... Loss: 0.00195510\n",
      "Train Epoch: 97 ... Batch: 180 ... Loss: 0.00180009\n",
      "Train Epoch: 97 ... Batch: 190 ... Loss: 0.00197654\n",
      "Train Epoch: 97 ... Batch: 200 ... Loss: 0.00202321\n",
      "------------------- Test set: Average loss: 468.3117 ... Samples: 810\n",
      "Train Epoch: 98 ... Batch: 0 ... Loss: 0.00208307\n",
      "Train Epoch: 98 ... Batch: 10 ... Loss: 0.00184327\n",
      "Train Epoch: 98 ... Batch: 20 ... Loss: 0.00186375\n",
      "Train Epoch: 98 ... Batch: 30 ... Loss: 0.00199670\n",
      "Train Epoch: 98 ... Batch: 40 ... Loss: 0.00202854\n",
      "Train Epoch: 98 ... Batch: 50 ... Loss: 0.00183409\n",
      "Train Epoch: 98 ... Batch: 60 ... Loss: 0.00177313\n",
      "Train Epoch: 98 ... Batch: 70 ... Loss: 0.00181534\n",
      "Train Epoch: 98 ... Batch: 80 ... Loss: 0.00182121\n",
      "Train Epoch: 98 ... Batch: 90 ... Loss: 0.00169164\n",
      "Train Epoch: 98 ... Batch: 100 ... Loss: 0.00198811\n",
      "Train Epoch: 98 ... Batch: 110 ... Loss: 0.00196873\n",
      "Train Epoch: 98 ... Batch: 120 ... Loss: 0.00216174\n",
      "Train Epoch: 98 ... Batch: 130 ... Loss: 0.00191691\n",
      "Train Epoch: 98 ... Batch: 140 ... Loss: 0.00225609\n",
      "Train Epoch: 98 ... Batch: 150 ... Loss: 0.00207459\n",
      "Train Epoch: 98 ... Batch: 160 ... Loss: 0.00163181\n",
      "Train Epoch: 98 ... Batch: 170 ... Loss: 0.00177888\n",
      "Train Epoch: 98 ... Batch: 180 ... Loss: 0.00203629\n",
      "Train Epoch: 98 ... Batch: 190 ... Loss: 0.00188254\n",
      "Train Epoch: 98 ... Batch: 200 ... Loss: 0.00125716\n",
      "------------------- Test set: Average loss: 468.1936 ... Samples: 810\n",
      "Train Epoch: 99 ... Batch: 0 ... Loss: 0.00162688\n",
      "Train Epoch: 99 ... Batch: 10 ... Loss: 0.00189777\n",
      "Train Epoch: 99 ... Batch: 20 ... Loss: 0.00170616\n",
      "Train Epoch: 99 ... Batch: 30 ... Loss: 0.00152710\n",
      "Train Epoch: 99 ... Batch: 40 ... Loss: 0.00147710\n",
      "Train Epoch: 99 ... Batch: 50 ... Loss: 0.00180996\n",
      "Train Epoch: 99 ... Batch: 60 ... Loss: 0.00151852\n",
      "Train Epoch: 99 ... Batch: 70 ... Loss: 0.00198158\n",
      "Train Epoch: 99 ... Batch: 80 ... Loss: 0.00201130\n",
      "Train Epoch: 99 ... Batch: 90 ... Loss: 0.00162140\n",
      "Train Epoch: 99 ... Batch: 100 ... Loss: 0.00199359\n",
      "Train Epoch: 99 ... Batch: 110 ... Loss: 0.00185360\n",
      "Train Epoch: 99 ... Batch: 120 ... Loss: 0.00154940\n",
      "Train Epoch: 99 ... Batch: 130 ... Loss: 0.00195397\n",
      "Train Epoch: 99 ... Batch: 140 ... Loss: 0.00193630\n",
      "Train Epoch: 99 ... Batch: 150 ... Loss: 0.00152487\n",
      "Train Epoch: 99 ... Batch: 160 ... Loss: 0.00159843\n",
      "Train Epoch: 99 ... Batch: 170 ... Loss: 0.00173063\n",
      "Train Epoch: 99 ... Batch: 180 ... Loss: 0.00184409\n",
      "Train Epoch: 99 ... Batch: 190 ... Loss: 0.00163741\n",
      "Train Epoch: 99 ... Batch: 200 ... Loss: 0.00192283\n",
      "------------------- Test set: Average loss: 467.9217 ... Samples: 810\n",
      "Train Epoch: 100 ... Batch: 0 ... Loss: 0.00150549\n",
      "Train Epoch: 100 ... Batch: 10 ... Loss: 0.00180434\n",
      "Train Epoch: 100 ... Batch: 20 ... Loss: 0.00185679\n",
      "Train Epoch: 100 ... Batch: 30 ... Loss: 0.00168742\n",
      "Train Epoch: 100 ... Batch: 40 ... Loss: 0.00177590\n",
      "Train Epoch: 100 ... Batch: 50 ... Loss: 0.00171366\n",
      "Train Epoch: 100 ... Batch: 60 ... Loss: 0.00210473\n",
      "Train Epoch: 100 ... Batch: 70 ... Loss: 0.00235559\n",
      "Train Epoch: 100 ... Batch: 80 ... Loss: 0.00197319\n",
      "Train Epoch: 100 ... Batch: 90 ... Loss: 0.00199138\n",
      "Train Epoch: 100 ... Batch: 100 ... Loss: 0.00174507\n",
      "Train Epoch: 100 ... Batch: 110 ... Loss: 0.00169992\n",
      "Train Epoch: 100 ... Batch: 120 ... Loss: 0.00187394\n",
      "Train Epoch: 100 ... Batch: 130 ... Loss: 0.00162417\n",
      "Train Epoch: 100 ... Batch: 140 ... Loss: 0.00225311\n",
      "Train Epoch: 100 ... Batch: 150 ... Loss: 0.00176801\n",
      "Train Epoch: 100 ... Batch: 160 ... Loss: 0.00199375\n",
      "Train Epoch: 100 ... Batch: 170 ... Loss: 0.00161621\n",
      "Train Epoch: 100 ... Batch: 180 ... Loss: 0.00209954\n",
      "Train Epoch: 100 ... Batch: 190 ... Loss: 0.00177159\n",
      "Train Epoch: 100 ... Batch: 200 ... Loss: 0.00152472\n",
      "------------------- Test set: Average loss: 467.8928 ... Samples: 810\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37874c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the model\n",
    "torch.save(model.state_dict(), 'fully_connected_network.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f460e",
   "metadata": {},
   "source": [
    "### test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3fa2855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAD6CAAAAAC6tr/wAAAFSUlEQVR4nO3d23baMBAFUJPV//9l9SGB2MYQLjaSjvZ+Km0XQdLxoNiDmSYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEpVS+xXwhFPtF9CGMk2LuSiXB99xPv3+cfb/ymQC2/Wv9gv4lLII5DmRs+Beonu6fjAt6vXyn5bP0a7S/CvcWfh4f+vqb8mdxfS0wxZj8Rx1pvP2u8fvYXz1lhMvdqTfy11pdzyf1XL8HM8P1vnf3Bp/7KIvBI6yTNNpqhXqhXO5PP28pmNsjPSPd6PARd+QNMp2Mr0yr557T3h5bVOVtO43JAzxvNVsMNQ37Tbvrw46YeHv632Epd6Geg/vTv/rQ+994f/W6wjL6hf/ri0W4ZmTdG+Nvtelf1R/4yvdbT4edT5LeXow3+/NQX8r/6SeBlhy6vRdj67JuzPR09q/oIvh5RbqO/5eGdG+q5ML7YPFepq6uX7frkajvW5PGtaqX4XHNTpnl0Dv0eWRY7la709No6u/jyYHJ873LRu+3n2eVI1tSI7stcjh0H9EM9G+7K6tG7uoXyObbWoaQf3lP07Nqq1Qc6C6GxKZ5jC1oi3UHOzz0V5/eJx6opfh66M/7XInD0Wbo33ouHUapE3JZftz0aZFwdn+wF5brKnh0GgLdSV7NZl07bBo6wapbfBkH3qGZPS5refxmQ9eoyOqdvB09Wy0Xcre0R5n5jq1vm3LB25JWMme0ba9blpuiLftOVoVu3HbnzdNDfxeVVusOzDWtzjsMVKx7sdm4c6M+74n/zLnKMlWGQpdtbeHtfy6CwW8eRs34srM9lt77TF+HcmXeR+fl4dTrppUR7skkCol4a9XbRkOlXJ54sVoyzWte+H4vN6KzJ9O6EN0X7lfifb+r4IG9Z7t5zYkUk03Hj80izsCj6bvuv131Z41Hkg2/bh3YLrDAh0X7j+izej6zfb6lRcXFZnpN9nzr8wd8AvseFCPCR/kW0Z5W3fp/uztLOFjVG0e11XlFm2e1kfCbUh4Wh+l0HVGntZH1XYWmxe1HnAns3nPIuGlobyLNrtYvP03EXAbEg5TN+GizdEWpyqWt69aPNr5SBBtalh+qmXxaDv860/arx9dHxcu2dCmq7iX7UeXBK/DLdqEWEfb1UgyXO1IRJtQok0o0SaUaBPqa5pqXzWCI6jahBJtIlzvPESbCOXquqNoE0q0CSXahPqaJu1RJFK1yaA9ilF8X410OZI4qjahRJtQok2o72g7+0ccVZsMekgYhWgTSrQJJdqE0h5FKFWbUL7Lhgw6/xiFaBNKtAkl2oTSHkUoVZtQok0EN0ZjGKJNhFv3/PORduKo2oT6ibazf6RRtcmgPYpRiDahRJtQok0o7VGEUrUJJdpE0B5FKD0kDEPVJpRoE0p7FKFUbUKJNhl0/jEK0SaUaBNKtAml849QqjahzqdM1G26pvOPYYg2oUSbCLf6tSGOzj9Cqdpk0B7FKESbUKJNKNEmlPYoQrkPCaFsSIigPYphiDahRJtQok2Em51/zv6RRtUmlGiTQecfo/iJtsuRpFG1CSXahDpH29k/wqjahBJtIuj8YxiiTSjRJpRoE0rnH6FUbSJcF2fRJpRoE0rnH6FUbUJpjyKUqk0o0SaC9iiGIdqEEm1CiTahtEcRStUmlGgTQecfwxBtQun8I5SqTSidf4RStQkl2kTQ+ccwRJtQok0o0SaUzj9Cqdok2LiaLtqE0kNCKFWbBEW/NsPQHkUoVZtQok0E7VEMQ7QJJdqEEm1CaY8ilKpNKNEmgc4/xqHzj1CqNgl0/jGO/yntzYJhb1jwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=730x250 at 0x17B7181F970>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## cut the image\n",
    "original = data_input[350].numpy().reshape((250,730))*255\n",
    "Image.fromarray(original.astype('uint8'), 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03d2de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() #evaluate model\n",
    "with torch.no_grad():\n",
    "    data = data_input[350]\n",
    "    #data = data[None,:,:,:]\n",
    "    data = data.to(device)\n",
    "    output = model(data)\n",
    "    output = output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2badd409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=730x250 at 0x17B701E8DC0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = output.numpy().reshape((250,730))*255\n",
    "Image.fromarray(predict.astype('uint8'), 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf5f329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
