{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbc884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b615489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e56064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad19ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d4cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbce90a",
   "metadata": {},
   "source": [
    "### read extracted feartures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4807be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_feat = np.load(\"extract_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "471e955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ex_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9de4511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 5\n",
    "\n",
    "def sliding_window(datas,steps=1,width=step):\n",
    "    win_set=[]\n",
    "    for i in tqdm(np.arange(0,len(datas),steps)):\n",
    "        temp=datas[i:i+width]\n",
    "        if temp.shape[0] == width:\n",
    "            win_set.append(temp)\n",
    "    return np.array(win_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11779cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed24f44947480bb2e29829338c6221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_input = sliding_window(ex_feat,steps=1,width=step)\n",
    "data_input = torch.tensor(data_input, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd326920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4042, 5, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acfbd4",
   "metadata": {},
   "source": [
    "### build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc0950c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm_encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=32, hidden_size=16)\n",
    "        self.lstm2 = nn.LSTM(input_size=16, hidden_size=16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #reshape x to fit the input requirement of lstm\n",
    "        x = x.permute(1, 0, 2)\n",
    "        output, hn = self.lstm1(x)\n",
    "        output, (hidden, cell) = self.lstm2(output)\n",
    "        # output include all timestep, while hidden just include the last timestep.\n",
    "        hidden = hidden.repeat((output.shape[0], 1, 1))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d961d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm_decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=16, hidden_size=32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # not need to reshape\n",
    "        \n",
    "        output, hn = self.lstm1(x)\n",
    "        #reshape output\n",
    "        output = output.permute(1, 0, 2)\n",
    "        ## output = torch.flip(output, dims=[1])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89882074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.Lstm_encoder = args[0]\n",
    "        self.Lstm_decoder = args[1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.Lstm_encoder(x)\n",
    "        output = self.Lstm_decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02d09a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net(Lstm_encoder(), Lstm_decoder())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad95ab",
   "metadata": {},
   "source": [
    "### train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a332884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \n",
    "    model.train() #trian model\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        ##calculate loss\n",
    "        #loss = 0\n",
    "        #for i in range(data.shape[0]):\n",
    "            #loss += F.mse_loss(output[i], data[i], reduction='sum')\n",
    "        #loss /= data.shape[0]\n",
    "        loss = F.mse_loss(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print result every 10 batch\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} ... Batch: {} ... Loss: {:.8f}'.format(epoch, batch_idx, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6ec3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval() #evaluate model\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            #calculate sum loss\n",
    "            test_loss += F.mse_loss(output, data, reduction='sum').item()\n",
    "    \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('------------------- Test set: Average loss: {:.4f} ... Samples: {}'.format(test_loss, len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656c331",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8888b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_, val_window_ = train_test_split(data_input, test_size=0.2, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d30aa586",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_window_, batch_size=16,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(val_window_, batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520b7350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd1dce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "345b4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51e0504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2aeb05ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 ... Batch: 0 ... Loss: 8124.04003906\n",
      "Train Epoch: 1 ... Batch: 10 ... Loss: 6601.99365234\n",
      "Train Epoch: 1 ... Batch: 20 ... Loss: 9207.70996094\n",
      "Train Epoch: 1 ... Batch: 30 ... Loss: 7069.19873047\n",
      "Train Epoch: 1 ... Batch: 40 ... Loss: 7608.19677734\n",
      "Train Epoch: 1 ... Batch: 50 ... Loss: 7527.05029297\n",
      "Train Epoch: 1 ... Batch: 60 ... Loss: 7438.39404297\n",
      "Train Epoch: 1 ... Batch: 70 ... Loss: 7318.56494141\n",
      "Train Epoch: 1 ... Batch: 80 ... Loss: 7073.29833984\n",
      "Train Epoch: 1 ... Batch: 90 ... Loss: 8958.78125000\n",
      "Train Epoch: 1 ... Batch: 100 ... Loss: 8402.92578125\n",
      "Train Epoch: 1 ... Batch: 110 ... Loss: 7716.16796875\n",
      "Train Epoch: 1 ... Batch: 120 ... Loss: 8945.07714844\n",
      "Train Epoch: 1 ... Batch: 130 ... Loss: 6334.67626953\n",
      "Train Epoch: 1 ... Batch: 140 ... Loss: 8864.29492188\n",
      "Train Epoch: 1 ... Batch: 150 ... Loss: 7453.33935547\n",
      "Train Epoch: 1 ... Batch: 160 ... Loss: 7787.67529297\n",
      "Train Epoch: 1 ... Batch: 170 ... Loss: 7178.70019531\n",
      "Train Epoch: 1 ... Batch: 180 ... Loss: 7795.48291016\n",
      "Train Epoch: 1 ... Batch: 190 ... Loss: 6778.17822266\n",
      "Train Epoch: 1 ... Batch: 200 ... Loss: 7385.90332031\n",
      "------------------- Test set: Average loss: 1218652.1372 ... Samples: 809\n",
      "Train Epoch: 2 ... Batch: 0 ... Loss: 7778.90771484\n",
      "Train Epoch: 2 ... Batch: 10 ... Loss: 7443.46337891\n",
      "Train Epoch: 2 ... Batch: 20 ... Loss: 8893.18359375\n",
      "Train Epoch: 2 ... Batch: 30 ... Loss: 7743.39794922\n",
      "Train Epoch: 2 ... Batch: 40 ... Loss: 8945.31542969\n",
      "Train Epoch: 2 ... Batch: 50 ... Loss: 7897.30175781\n",
      "Train Epoch: 2 ... Batch: 60 ... Loss: 8400.30761719\n",
      "Train Epoch: 2 ... Batch: 70 ... Loss: 7548.80712891\n",
      "Train Epoch: 2 ... Batch: 80 ... Loss: 7649.44628906\n",
      "Train Epoch: 2 ... Batch: 90 ... Loss: 7816.41406250\n",
      "Train Epoch: 2 ... Batch: 100 ... Loss: 7399.75341797\n",
      "Train Epoch: 2 ... Batch: 110 ... Loss: 8161.80468750\n",
      "Train Epoch: 2 ... Batch: 120 ... Loss: 7669.62500000\n",
      "Train Epoch: 2 ... Batch: 130 ... Loss: 8670.73925781\n",
      "Train Epoch: 2 ... Batch: 140 ... Loss: 9491.97558594\n",
      "Train Epoch: 2 ... Batch: 150 ... Loss: 7621.58447266\n",
      "Train Epoch: 2 ... Batch: 160 ... Loss: 6593.17919922\n",
      "Train Epoch: 2 ... Batch: 170 ... Loss: 8500.29101562\n",
      "Train Epoch: 2 ... Batch: 180 ... Loss: 7974.50146484\n",
      "Train Epoch: 2 ... Batch: 190 ... Loss: 7835.16796875\n",
      "Train Epoch: 2 ... Batch: 200 ... Loss: 7662.58203125\n",
      "------------------- Test set: Average loss: 1218644.3152 ... Samples: 809\n",
      "Train Epoch: 3 ... Batch: 0 ... Loss: 7658.46826172\n",
      "Train Epoch: 3 ... Batch: 10 ... Loss: 8510.27636719\n",
      "Train Epoch: 3 ... Batch: 20 ... Loss: 8301.24218750\n",
      "Train Epoch: 3 ... Batch: 30 ... Loss: 7248.10888672\n",
      "Train Epoch: 3 ... Batch: 40 ... Loss: 7097.93896484\n",
      "Train Epoch: 3 ... Batch: 50 ... Loss: 7299.79052734\n",
      "Train Epoch: 3 ... Batch: 60 ... Loss: 6777.55224609\n",
      "Train Epoch: 3 ... Batch: 70 ... Loss: 8006.37060547\n",
      "Train Epoch: 3 ... Batch: 80 ... Loss: 8136.48291016\n",
      "Train Epoch: 3 ... Batch: 90 ... Loss: 8507.13867188\n",
      "Train Epoch: 3 ... Batch: 100 ... Loss: 7028.21728516\n",
      "Train Epoch: 3 ... Batch: 110 ... Loss: 8840.76464844\n",
      "Train Epoch: 3 ... Batch: 120 ... Loss: 7515.97216797\n",
      "Train Epoch: 3 ... Batch: 130 ... Loss: 8033.47656250\n",
      "Train Epoch: 3 ... Batch: 140 ... Loss: 7909.25000000\n",
      "Train Epoch: 3 ... Batch: 150 ... Loss: 8486.66210938\n",
      "Train Epoch: 3 ... Batch: 160 ... Loss: 7450.15234375\n",
      "Train Epoch: 3 ... Batch: 170 ... Loss: 8541.28027344\n",
      "Train Epoch: 3 ... Batch: 180 ... Loss: 8481.72558594\n",
      "Train Epoch: 3 ... Batch: 190 ... Loss: 6924.84863281\n",
      "Train Epoch: 3 ... Batch: 200 ... Loss: 7190.65234375\n",
      "------------------- Test set: Average loss: 1218642.3016 ... Samples: 809\n",
      "Train Epoch: 4 ... Batch: 0 ... Loss: 8058.19677734\n",
      "Train Epoch: 4 ... Batch: 10 ... Loss: 8474.12011719\n",
      "Train Epoch: 4 ... Batch: 20 ... Loss: 6668.94384766\n",
      "Train Epoch: 4 ... Batch: 30 ... Loss: 7746.44775391\n",
      "Train Epoch: 4 ... Batch: 40 ... Loss: 7706.58447266\n",
      "Train Epoch: 4 ... Batch: 50 ... Loss: 8445.01269531\n",
      "Train Epoch: 4 ... Batch: 60 ... Loss: 7982.92578125\n",
      "Train Epoch: 4 ... Batch: 70 ... Loss: 7004.72900391\n",
      "Train Epoch: 4 ... Batch: 80 ... Loss: 8083.74560547\n",
      "Train Epoch: 4 ... Batch: 90 ... Loss: 7750.37109375\n",
      "Train Epoch: 4 ... Batch: 100 ... Loss: 8938.09765625\n",
      "Train Epoch: 4 ... Batch: 110 ... Loss: 7674.49707031\n",
      "Train Epoch: 4 ... Batch: 120 ... Loss: 8062.86816406\n",
      "Train Epoch: 4 ... Batch: 130 ... Loss: 7311.51318359\n",
      "Train Epoch: 4 ... Batch: 140 ... Loss: 7168.66796875\n",
      "Train Epoch: 4 ... Batch: 150 ... Loss: 6558.64306641\n",
      "Train Epoch: 4 ... Batch: 160 ... Loss: 8004.20410156\n",
      "Train Epoch: 4 ... Batch: 170 ... Loss: 8383.97949219\n",
      "Train Epoch: 4 ... Batch: 180 ... Loss: 7603.25341797\n",
      "Train Epoch: 4 ... Batch: 190 ... Loss: 6669.95410156\n",
      "Train Epoch: 4 ... Batch: 200 ... Loss: 7465.15625000\n",
      "------------------- Test set: Average loss: 1218637.2571 ... Samples: 809\n",
      "Train Epoch: 5 ... Batch: 0 ... Loss: 6685.10644531\n",
      "Train Epoch: 5 ... Batch: 10 ... Loss: 8001.29150391\n",
      "Train Epoch: 5 ... Batch: 20 ... Loss: 7144.01562500\n",
      "Train Epoch: 5 ... Batch: 30 ... Loss: 8998.97167969\n",
      "Train Epoch: 5 ... Batch: 40 ... Loss: 7414.72753906\n",
      "Train Epoch: 5 ... Batch: 50 ... Loss: 6913.59716797\n",
      "Train Epoch: 5 ... Batch: 60 ... Loss: 8136.80468750\n",
      "Train Epoch: 5 ... Batch: 70 ... Loss: 7626.28857422\n",
      "Train Epoch: 5 ... Batch: 80 ... Loss: 7973.71093750\n",
      "Train Epoch: 5 ... Batch: 90 ... Loss: 8634.26074219\n",
      "Train Epoch: 5 ... Batch: 100 ... Loss: 8086.51806641\n",
      "Train Epoch: 5 ... Batch: 110 ... Loss: 7582.78125000\n",
      "Train Epoch: 5 ... Batch: 120 ... Loss: 8753.55078125\n",
      "Train Epoch: 5 ... Batch: 130 ... Loss: 7934.46875000\n",
      "Train Epoch: 5 ... Batch: 140 ... Loss: 8479.08886719\n",
      "Train Epoch: 5 ... Batch: 150 ... Loss: 7124.47216797\n",
      "Train Epoch: 5 ... Batch: 160 ... Loss: 7928.37841797\n",
      "Train Epoch: 5 ... Batch: 170 ... Loss: 8793.66308594\n",
      "Train Epoch: 5 ... Batch: 180 ... Loss: 8256.76171875\n",
      "Train Epoch: 5 ... Batch: 190 ... Loss: 7051.85302734\n",
      "Train Epoch: 5 ... Batch: 200 ... Loss: 8840.39257812\n",
      "------------------- Test set: Average loss: 1218630.8467 ... Samples: 809\n",
      "Train Epoch: 6 ... Batch: 0 ... Loss: 7428.02490234\n",
      "Train Epoch: 6 ... Batch: 10 ... Loss: 6894.63623047\n",
      "Train Epoch: 6 ... Batch: 20 ... Loss: 7631.71630859\n",
      "Train Epoch: 6 ... Batch: 30 ... Loss: 7448.79296875\n",
      "Train Epoch: 6 ... Batch: 40 ... Loss: 7855.67529297\n",
      "Train Epoch: 6 ... Batch: 50 ... Loss: 9224.93945312\n",
      "Train Epoch: 6 ... Batch: 60 ... Loss: 7273.66162109\n",
      "Train Epoch: 6 ... Batch: 70 ... Loss: 7881.77441406\n",
      "Train Epoch: 6 ... Batch: 80 ... Loss: 8329.86425781\n",
      "Train Epoch: 6 ... Batch: 90 ... Loss: 8168.20654297\n",
      "Train Epoch: 6 ... Batch: 100 ... Loss: 7441.66552734\n",
      "Train Epoch: 6 ... Batch: 110 ... Loss: 7283.23583984\n",
      "Train Epoch: 6 ... Batch: 120 ... Loss: 7607.80322266\n",
      "Train Epoch: 6 ... Batch: 130 ... Loss: 7616.70556641\n",
      "Train Epoch: 6 ... Batch: 140 ... Loss: 8700.25097656\n",
      "Train Epoch: 6 ... Batch: 150 ... Loss: 9125.98535156\n",
      "Train Epoch: 6 ... Batch: 160 ... Loss: 7136.57519531\n",
      "Train Epoch: 6 ... Batch: 170 ... Loss: 7468.10009766\n",
      "Train Epoch: 6 ... Batch: 180 ... Loss: 7410.54833984\n",
      "Train Epoch: 6 ... Batch: 190 ... Loss: 7192.07763672\n",
      "Train Epoch: 6 ... Batch: 200 ... Loss: 7272.29687500\n",
      "------------------- Test set: Average loss: 1218631.1150 ... Samples: 809\n",
      "Train Epoch: 7 ... Batch: 0 ... Loss: 7651.98583984\n",
      "Train Epoch: 7 ... Batch: 10 ... Loss: 8790.93261719\n",
      "Train Epoch: 7 ... Batch: 20 ... Loss: 8266.12109375\n",
      "Train Epoch: 7 ... Batch: 30 ... Loss: 8230.66699219\n",
      "Train Epoch: 7 ... Batch: 40 ... Loss: 7310.01708984\n",
      "Train Epoch: 7 ... Batch: 50 ... Loss: 7141.94287109\n",
      "Train Epoch: 7 ... Batch: 60 ... Loss: 7801.60156250\n",
      "Train Epoch: 7 ... Batch: 70 ... Loss: 6448.04931641\n",
      "Train Epoch: 7 ... Batch: 80 ... Loss: 7313.95458984\n",
      "Train Epoch: 7 ... Batch: 90 ... Loss: 7830.84765625\n",
      "Train Epoch: 7 ... Batch: 100 ... Loss: 8541.57226562\n",
      "Train Epoch: 7 ... Batch: 110 ... Loss: 7836.14404297\n",
      "Train Epoch: 7 ... Batch: 120 ... Loss: 8283.98339844\n",
      "Train Epoch: 7 ... Batch: 130 ... Loss: 8419.83496094\n",
      "Train Epoch: 7 ... Batch: 140 ... Loss: 8134.85107422\n",
      "Train Epoch: 7 ... Batch: 150 ... Loss: 7703.60302734\n",
      "Train Epoch: 7 ... Batch: 160 ... Loss: 7843.05468750\n",
      "Train Epoch: 7 ... Batch: 170 ... Loss: 7900.17968750\n",
      "Train Epoch: 7 ... Batch: 180 ... Loss: 9644.37011719\n",
      "Train Epoch: 7 ... Batch: 190 ... Loss: 8698.14453125\n",
      "Train Epoch: 7 ... Batch: 200 ... Loss: 8355.32226562\n",
      "------------------- Test set: Average loss: 1218625.2262 ... Samples: 809\n",
      "Train Epoch: 8 ... Batch: 0 ... Loss: 8499.26562500\n",
      "Train Epoch: 8 ... Batch: 10 ... Loss: 7847.05615234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 ... Batch: 20 ... Loss: 9437.02246094\n",
      "Train Epoch: 8 ... Batch: 30 ... Loss: 6657.64990234\n",
      "Train Epoch: 8 ... Batch: 40 ... Loss: 7728.15380859\n",
      "Train Epoch: 8 ... Batch: 50 ... Loss: 6782.71875000\n",
      "Train Epoch: 8 ... Batch: 60 ... Loss: 7021.23437500\n",
      "Train Epoch: 8 ... Batch: 70 ... Loss: 7406.45312500\n",
      "Train Epoch: 8 ... Batch: 80 ... Loss: 7340.95703125\n",
      "Train Epoch: 8 ... Batch: 90 ... Loss: 7539.25927734\n",
      "Train Epoch: 8 ... Batch: 100 ... Loss: 8308.68066406\n",
      "Train Epoch: 8 ... Batch: 110 ... Loss: 7915.03271484\n",
      "Train Epoch: 8 ... Batch: 120 ... Loss: 8590.68261719\n",
      "Train Epoch: 8 ... Batch: 130 ... Loss: 8989.40332031\n",
      "Train Epoch: 8 ... Batch: 140 ... Loss: 7280.27880859\n",
      "Train Epoch: 8 ... Batch: 150 ... Loss: 9407.39355469\n",
      "Train Epoch: 8 ... Batch: 160 ... Loss: 7264.98437500\n",
      "Train Epoch: 8 ... Batch: 170 ... Loss: 8098.30175781\n",
      "Train Epoch: 8 ... Batch: 180 ... Loss: 7899.45019531\n",
      "Train Epoch: 8 ... Batch: 190 ... Loss: 7563.84765625\n",
      "Train Epoch: 8 ... Batch: 200 ... Loss: 7529.54638672\n",
      "------------------- Test set: Average loss: 1218627.5797 ... Samples: 809\n",
      "Train Epoch: 9 ... Batch: 0 ... Loss: 7109.18359375\n",
      "Train Epoch: 9 ... Batch: 10 ... Loss: 7827.42333984\n",
      "Train Epoch: 9 ... Batch: 20 ... Loss: 7156.00781250\n",
      "Train Epoch: 9 ... Batch: 30 ... Loss: 7260.39404297\n",
      "Train Epoch: 9 ... Batch: 40 ... Loss: 7800.23437500\n",
      "Train Epoch: 9 ... Batch: 50 ... Loss: 9121.48437500\n",
      "Train Epoch: 9 ... Batch: 60 ... Loss: 8266.53613281\n",
      "Train Epoch: 9 ... Batch: 70 ... Loss: 7285.16943359\n",
      "Train Epoch: 9 ... Batch: 80 ... Loss: 7125.32421875\n",
      "Train Epoch: 9 ... Batch: 90 ... Loss: 8651.01562500\n",
      "Train Epoch: 9 ... Batch: 100 ... Loss: 7594.18603516\n",
      "Train Epoch: 9 ... Batch: 110 ... Loss: 9245.07226562\n",
      "Train Epoch: 9 ... Batch: 120 ... Loss: 8044.53369141\n",
      "Train Epoch: 9 ... Batch: 130 ... Loss: 8874.60937500\n",
      "Train Epoch: 9 ... Batch: 140 ... Loss: 8120.60009766\n",
      "Train Epoch: 9 ... Batch: 150 ... Loss: 8217.93457031\n",
      "Train Epoch: 9 ... Batch: 160 ... Loss: 7970.10302734\n",
      "Train Epoch: 9 ... Batch: 170 ... Loss: 6583.81591797\n",
      "Train Epoch: 9 ... Batch: 180 ... Loss: 7513.28662109\n",
      "Train Epoch: 9 ... Batch: 190 ... Loss: 7310.37060547\n",
      "Train Epoch: 9 ... Batch: 200 ... Loss: 7585.06884766\n",
      "------------------- Test set: Average loss: 1218625.8096 ... Samples: 809\n",
      "Train Epoch: 10 ... Batch: 0 ... Loss: 8912.71386719\n",
      "Train Epoch: 10 ... Batch: 10 ... Loss: 7990.35400391\n",
      "Train Epoch: 10 ... Batch: 20 ... Loss: 7083.64550781\n",
      "Train Epoch: 10 ... Batch: 30 ... Loss: 7454.38623047\n",
      "Train Epoch: 10 ... Batch: 40 ... Loss: 7706.99853516\n",
      "Train Epoch: 10 ... Batch: 50 ... Loss: 6977.62841797\n",
      "Train Epoch: 10 ... Batch: 60 ... Loss: 7268.67333984\n",
      "Train Epoch: 10 ... Batch: 70 ... Loss: 7981.23388672\n",
      "Train Epoch: 10 ... Batch: 80 ... Loss: 7890.13818359\n",
      "Train Epoch: 10 ... Batch: 90 ... Loss: 7742.03906250\n",
      "Train Epoch: 10 ... Batch: 100 ... Loss: 8826.31250000\n",
      "Train Epoch: 10 ... Batch: 110 ... Loss: 7671.83203125\n",
      "Train Epoch: 10 ... Batch: 120 ... Loss: 7191.53759766\n",
      "Train Epoch: 10 ... Batch: 130 ... Loss: 7757.23046875\n",
      "Train Epoch: 10 ... Batch: 140 ... Loss: 8163.25927734\n",
      "Train Epoch: 10 ... Batch: 150 ... Loss: 8180.24072266\n",
      "Train Epoch: 10 ... Batch: 160 ... Loss: 7700.19775391\n",
      "Train Epoch: 10 ... Batch: 170 ... Loss: 8603.06640625\n",
      "Train Epoch: 10 ... Batch: 180 ... Loss: 7672.03125000\n",
      "Train Epoch: 10 ... Batch: 190 ... Loss: 9271.65625000\n",
      "Train Epoch: 10 ... Batch: 200 ... Loss: 9163.00000000\n",
      "------------------- Test set: Average loss: 1218624.7095 ... Samples: 809\n",
      "Train Epoch: 11 ... Batch: 0 ... Loss: 7858.02294922\n",
      "Train Epoch: 11 ... Batch: 10 ... Loss: 8823.97265625\n",
      "Train Epoch: 11 ... Batch: 20 ... Loss: 7143.87890625\n",
      "Train Epoch: 11 ... Batch: 30 ... Loss: 8053.54248047\n",
      "Train Epoch: 11 ... Batch: 40 ... Loss: 8515.83105469\n",
      "Train Epoch: 11 ... Batch: 50 ... Loss: 6968.73291016\n",
      "Train Epoch: 11 ... Batch: 60 ... Loss: 8062.38427734\n",
      "Train Epoch: 11 ... Batch: 70 ... Loss: 7862.84082031\n",
      "Train Epoch: 11 ... Batch: 80 ... Loss: 7060.29394531\n",
      "Train Epoch: 11 ... Batch: 90 ... Loss: 8384.52636719\n",
      "Train Epoch: 11 ... Batch: 100 ... Loss: 7561.91406250\n",
      "Train Epoch: 11 ... Batch: 110 ... Loss: 6089.58349609\n",
      "Train Epoch: 11 ... Batch: 120 ... Loss: 6859.01806641\n",
      "Train Epoch: 11 ... Batch: 130 ... Loss: 6625.76513672\n",
      "Train Epoch: 11 ... Batch: 140 ... Loss: 6833.29248047\n",
      "Train Epoch: 11 ... Batch: 150 ... Loss: 8556.64355469\n",
      "Train Epoch: 11 ... Batch: 160 ... Loss: 7477.29150391\n",
      "Train Epoch: 11 ... Batch: 170 ... Loss: 7825.21240234\n",
      "Train Epoch: 11 ... Batch: 180 ... Loss: 8244.97363281\n",
      "Train Epoch: 11 ... Batch: 190 ... Loss: 8239.05468750\n",
      "Train Epoch: 11 ... Batch: 200 ... Loss: 8518.11132812\n",
      "------------------- Test set: Average loss: 1218624.9716 ... Samples: 809\n",
      "Train Epoch: 12 ... Batch: 0 ... Loss: 8120.62841797\n",
      "Train Epoch: 12 ... Batch: 10 ... Loss: 6831.11865234\n",
      "Train Epoch: 12 ... Batch: 20 ... Loss: 8328.90234375\n",
      "Train Epoch: 12 ... Batch: 30 ... Loss: 6735.83984375\n",
      "Train Epoch: 12 ... Batch: 40 ... Loss: 8102.92333984\n",
      "Train Epoch: 12 ... Batch: 50 ... Loss: 8250.62500000\n",
      "Train Epoch: 12 ... Batch: 60 ... Loss: 8319.20019531\n",
      "Train Epoch: 12 ... Batch: 70 ... Loss: 8082.01269531\n",
      "Train Epoch: 12 ... Batch: 80 ... Loss: 7655.47216797\n",
      "Train Epoch: 12 ... Batch: 90 ... Loss: 8169.15625000\n",
      "Train Epoch: 12 ... Batch: 100 ... Loss: 7954.71240234\n",
      "Train Epoch: 12 ... Batch: 110 ... Loss: 8880.60156250\n",
      "Train Epoch: 12 ... Batch: 120 ... Loss: 7942.63769531\n",
      "Train Epoch: 12 ... Batch: 130 ... Loss: 8336.95312500\n",
      "Train Epoch: 12 ... Batch: 140 ... Loss: 8109.47802734\n",
      "Train Epoch: 12 ... Batch: 150 ... Loss: 7017.50244141\n",
      "Train Epoch: 12 ... Batch: 160 ... Loss: 7086.32177734\n",
      "Train Epoch: 12 ... Batch: 170 ... Loss: 8884.60839844\n",
      "Train Epoch: 12 ... Batch: 180 ... Loss: 8142.56787109\n",
      "Train Epoch: 12 ... Batch: 190 ... Loss: 7829.67578125\n",
      "Train Epoch: 12 ... Batch: 200 ... Loss: 7137.44921875\n",
      "------------------- Test set: Average loss: 1218627.5983 ... Samples: 809\n",
      "Train Epoch: 13 ... Batch: 0 ... Loss: 7861.62207031\n",
      "Train Epoch: 13 ... Batch: 10 ... Loss: 8726.45605469\n",
      "Train Epoch: 13 ... Batch: 20 ... Loss: 6512.85400391\n",
      "Train Epoch: 13 ... Batch: 30 ... Loss: 7518.67822266\n",
      "Train Epoch: 13 ... Batch: 40 ... Loss: 8329.77539062\n",
      "Train Epoch: 13 ... Batch: 50 ... Loss: 8282.33593750\n",
      "Train Epoch: 13 ... Batch: 60 ... Loss: 7007.81884766\n",
      "Train Epoch: 13 ... Batch: 70 ... Loss: 7587.43847656\n",
      "Train Epoch: 13 ... Batch: 80 ... Loss: 6857.34619141\n",
      "Train Epoch: 13 ... Batch: 90 ... Loss: 8810.31933594\n",
      "Train Epoch: 13 ... Batch: 100 ... Loss: 7378.45947266\n",
      "Train Epoch: 13 ... Batch: 110 ... Loss: 8848.71777344\n",
      "Train Epoch: 13 ... Batch: 120 ... Loss: 7671.71191406\n",
      "Train Epoch: 13 ... Batch: 130 ... Loss: 7459.78613281\n",
      "Train Epoch: 13 ... Batch: 140 ... Loss: 7344.32031250\n",
      "Train Epoch: 13 ... Batch: 150 ... Loss: 7826.70068359\n",
      "Train Epoch: 13 ... Batch: 160 ... Loss: 8164.72509766\n",
      "Train Epoch: 13 ... Batch: 170 ... Loss: 8592.35058594\n",
      "Train Epoch: 13 ... Batch: 180 ... Loss: 7594.45263672\n",
      "Train Epoch: 13 ... Batch: 190 ... Loss: 8341.62792969\n",
      "Train Epoch: 13 ... Batch: 200 ... Loss: 7898.40966797\n",
      "------------------- Test set: Average loss: 1218626.6489 ... Samples: 809\n",
      "Train Epoch: 14 ... Batch: 0 ... Loss: 7550.15087891\n",
      "Train Epoch: 14 ... Batch: 10 ... Loss: 7556.26416016\n",
      "Train Epoch: 14 ... Batch: 20 ... Loss: 8637.39746094\n",
      "Train Epoch: 14 ... Batch: 30 ... Loss: 8169.75341797\n",
      "Train Epoch: 14 ... Batch: 40 ... Loss: 7723.80029297\n",
      "Train Epoch: 14 ... Batch: 50 ... Loss: 6764.25146484\n",
      "Train Epoch: 14 ... Batch: 60 ... Loss: 8247.60156250\n",
      "Train Epoch: 14 ... Batch: 70 ... Loss: 8503.39355469\n",
      "Train Epoch: 14 ... Batch: 80 ... Loss: 6259.54003906\n",
      "Train Epoch: 14 ... Batch: 90 ... Loss: 7796.17333984\n",
      "Train Epoch: 14 ... Batch: 100 ... Loss: 8593.85156250\n",
      "Train Epoch: 14 ... Batch: 110 ... Loss: 7731.44140625\n",
      "Train Epoch: 14 ... Batch: 120 ... Loss: 7623.67822266\n",
      "Train Epoch: 14 ... Batch: 130 ... Loss: 8125.36718750\n",
      "Train Epoch: 14 ... Batch: 140 ... Loss: 10233.35156250\n",
      "Train Epoch: 14 ... Batch: 150 ... Loss: 8655.05859375\n",
      "Train Epoch: 14 ... Batch: 160 ... Loss: 7382.73291016\n",
      "Train Epoch: 14 ... Batch: 170 ... Loss: 7736.31738281\n",
      "Train Epoch: 14 ... Batch: 180 ... Loss: 7642.80175781\n",
      "Train Epoch: 14 ... Batch: 190 ... Loss: 7112.13037109\n",
      "Train Epoch: 14 ... Batch: 200 ... Loss: 8984.95507812\n",
      "------------------- Test set: Average loss: 1218623.1323 ... Samples: 809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 ... Batch: 0 ... Loss: 7814.58154297\n",
      "Train Epoch: 15 ... Batch: 10 ... Loss: 7184.00781250\n",
      "Train Epoch: 15 ... Batch: 20 ... Loss: 8043.36474609\n",
      "Train Epoch: 15 ... Batch: 30 ... Loss: 6789.99951172\n",
      "Train Epoch: 15 ... Batch: 40 ... Loss: 7734.59521484\n",
      "Train Epoch: 15 ... Batch: 50 ... Loss: 7928.39941406\n",
      "Train Epoch: 15 ... Batch: 60 ... Loss: 8078.75537109\n",
      "Train Epoch: 15 ... Batch: 70 ... Loss: 7537.05322266\n",
      "Train Epoch: 15 ... Batch: 80 ... Loss: 6976.68750000\n",
      "Train Epoch: 15 ... Batch: 90 ... Loss: 8080.70068359\n",
      "Train Epoch: 15 ... Batch: 100 ... Loss: 7440.23437500\n",
      "Train Epoch: 15 ... Batch: 110 ... Loss: 7821.74365234\n",
      "Train Epoch: 15 ... Batch: 120 ... Loss: 7403.46044922\n",
      "Train Epoch: 15 ... Batch: 130 ... Loss: 7004.55468750\n",
      "Train Epoch: 15 ... Batch: 140 ... Loss: 7478.53271484\n",
      "Train Epoch: 15 ... Batch: 150 ... Loss: 7066.39697266\n",
      "Train Epoch: 15 ... Batch: 160 ... Loss: 7633.39404297\n",
      "Train Epoch: 15 ... Batch: 170 ... Loss: 7975.81787109\n",
      "Train Epoch: 15 ... Batch: 180 ... Loss: 8053.65478516\n",
      "Train Epoch: 15 ... Batch: 190 ... Loss: 7362.81103516\n",
      "Train Epoch: 15 ... Batch: 200 ... Loss: 8046.42919922\n",
      "------------------- Test set: Average loss: 1218623.4685 ... Samples: 809\n",
      "Train Epoch: 16 ... Batch: 0 ... Loss: 8189.75488281\n",
      "Train Epoch: 16 ... Batch: 10 ... Loss: 6629.24755859\n",
      "Train Epoch: 16 ... Batch: 20 ... Loss: 7830.41552734\n",
      "Train Epoch: 16 ... Batch: 30 ... Loss: 6915.39697266\n",
      "Train Epoch: 16 ... Batch: 40 ... Loss: 9279.45214844\n",
      "Train Epoch: 16 ... Batch: 50 ... Loss: 8685.98730469\n",
      "Train Epoch: 16 ... Batch: 60 ... Loss: 7608.90478516\n",
      "Train Epoch: 16 ... Batch: 70 ... Loss: 8124.36718750\n",
      "Train Epoch: 16 ... Batch: 80 ... Loss: 7689.26562500\n",
      "Train Epoch: 16 ... Batch: 90 ... Loss: 7171.34082031\n",
      "Train Epoch: 16 ... Batch: 100 ... Loss: 8051.45800781\n",
      "Train Epoch: 16 ... Batch: 110 ... Loss: 7217.58300781\n",
      "Train Epoch: 16 ... Batch: 120 ... Loss: 9010.17968750\n",
      "Train Epoch: 16 ... Batch: 130 ... Loss: 7247.60791016\n",
      "Train Epoch: 16 ... Batch: 140 ... Loss: 8271.18945312\n",
      "Train Epoch: 16 ... Batch: 150 ... Loss: 9270.17968750\n",
      "Train Epoch: 16 ... Batch: 160 ... Loss: 7174.81250000\n",
      "Train Epoch: 16 ... Batch: 170 ... Loss: 7289.60156250\n",
      "Train Epoch: 16 ... Batch: 180 ... Loss: 8145.87841797\n",
      "Train Epoch: 16 ... Batch: 190 ... Loss: 7542.92138672\n",
      "Train Epoch: 16 ... Batch: 200 ... Loss: 9120.16699219\n",
      "------------------- Test set: Average loss: 1218624.7651 ... Samples: 809\n",
      "Train Epoch: 17 ... Batch: 0 ... Loss: 8012.31591797\n",
      "Train Epoch: 17 ... Batch: 10 ... Loss: 8511.66894531\n",
      "Train Epoch: 17 ... Batch: 20 ... Loss: 7600.57958984\n",
      "Train Epoch: 17 ... Batch: 30 ... Loss: 9196.17382812\n",
      "Train Epoch: 17 ... Batch: 40 ... Loss: 7741.14941406\n",
      "Train Epoch: 17 ... Batch: 50 ... Loss: 7456.07763672\n",
      "Train Epoch: 17 ... Batch: 60 ... Loss: 7655.81347656\n",
      "Train Epoch: 17 ... Batch: 70 ... Loss: 7246.37744141\n",
      "Train Epoch: 17 ... Batch: 80 ... Loss: 8017.95654297\n",
      "Train Epoch: 17 ... Batch: 90 ... Loss: 8124.14990234\n",
      "Train Epoch: 17 ... Batch: 100 ... Loss: 7579.04541016\n",
      "Train Epoch: 17 ... Batch: 110 ... Loss: 8425.68652344\n",
      "Train Epoch: 17 ... Batch: 120 ... Loss: 8070.59863281\n",
      "Train Epoch: 17 ... Batch: 130 ... Loss: 8285.34082031\n",
      "Train Epoch: 17 ... Batch: 140 ... Loss: 7893.88916016\n",
      "Train Epoch: 17 ... Batch: 150 ... Loss: 8494.78613281\n",
      "Train Epoch: 17 ... Batch: 160 ... Loss: 7137.02685547\n",
      "Train Epoch: 17 ... Batch: 170 ... Loss: 7636.16357422\n",
      "Train Epoch: 17 ... Batch: 180 ... Loss: 7021.81396484\n",
      "Train Epoch: 17 ... Batch: 190 ... Loss: 7745.25488281\n",
      "Train Epoch: 17 ... Batch: 200 ... Loss: 6724.90185547\n",
      "------------------- Test set: Average loss: 1218625.0828 ... Samples: 809\n",
      "Train Epoch: 18 ... Batch: 0 ... Loss: 7552.48925781\n",
      "Train Epoch: 18 ... Batch: 10 ... Loss: 8741.64355469\n",
      "Train Epoch: 18 ... Batch: 20 ... Loss: 7787.38671875\n",
      "Train Epoch: 18 ... Batch: 30 ... Loss: 8192.63183594\n",
      "Train Epoch: 18 ... Batch: 40 ... Loss: 6836.72509766\n",
      "Train Epoch: 18 ... Batch: 50 ... Loss: 7578.90625000\n",
      "Train Epoch: 18 ... Batch: 60 ... Loss: 7733.69384766\n",
      "Train Epoch: 18 ... Batch: 70 ... Loss: 8106.68066406\n",
      "Train Epoch: 18 ... Batch: 80 ... Loss: 9138.75292969\n",
      "Train Epoch: 18 ... Batch: 90 ... Loss: 8779.46191406\n",
      "Train Epoch: 18 ... Batch: 100 ... Loss: 8044.60498047\n",
      "Train Epoch: 18 ... Batch: 110 ... Loss: 8036.79150391\n",
      "Train Epoch: 18 ... Batch: 120 ... Loss: 7731.31250000\n",
      "Train Epoch: 18 ... Batch: 130 ... Loss: 8419.71386719\n",
      "Train Epoch: 18 ... Batch: 140 ... Loss: 8330.81835938\n",
      "Train Epoch: 18 ... Batch: 150 ... Loss: 7119.00732422\n",
      "Train Epoch: 18 ... Batch: 160 ... Loss: 6788.45654297\n",
      "Train Epoch: 18 ... Batch: 170 ... Loss: 8661.53417969\n",
      "Train Epoch: 18 ... Batch: 180 ... Loss: 7832.19091797\n",
      "Train Epoch: 18 ... Batch: 190 ... Loss: 8375.46289062\n",
      "Train Epoch: 18 ... Batch: 200 ... Loss: 8389.65527344\n",
      "------------------- Test set: Average loss: 1218624.4326 ... Samples: 809\n",
      "Train Epoch: 19 ... Batch: 0 ... Loss: 7163.69531250\n",
      "Train Epoch: 19 ... Batch: 10 ... Loss: 7010.71875000\n",
      "Train Epoch: 19 ... Batch: 20 ... Loss: 7994.43896484\n",
      "Train Epoch: 19 ... Batch: 30 ... Loss: 8362.68945312\n",
      "Train Epoch: 19 ... Batch: 40 ... Loss: 7631.85107422\n",
      "Train Epoch: 19 ... Batch: 50 ... Loss: 8840.17675781\n",
      "Train Epoch: 19 ... Batch: 60 ... Loss: 8474.54882812\n",
      "Train Epoch: 19 ... Batch: 70 ... Loss: 7846.25341797\n",
      "Train Epoch: 19 ... Batch: 80 ... Loss: 8253.38378906\n",
      "Train Epoch: 19 ... Batch: 90 ... Loss: 7821.16552734\n",
      "Train Epoch: 19 ... Batch: 100 ... Loss: 7063.51562500\n",
      "Train Epoch: 19 ... Batch: 110 ... Loss: 7674.07177734\n",
      "Train Epoch: 19 ... Batch: 120 ... Loss: 7294.30175781\n",
      "Train Epoch: 19 ... Batch: 130 ... Loss: 7925.65771484\n",
      "Train Epoch: 19 ... Batch: 140 ... Loss: 9086.77832031\n",
      "Train Epoch: 19 ... Batch: 150 ... Loss: 8470.54687500\n",
      "Train Epoch: 19 ... Batch: 160 ... Loss: 9122.58593750\n",
      "Train Epoch: 19 ... Batch: 170 ... Loss: 7460.24218750\n",
      "Train Epoch: 19 ... Batch: 180 ... Loss: 7516.74560547\n",
      "Train Epoch: 19 ... Batch: 190 ... Loss: 8008.62597656\n",
      "Train Epoch: 19 ... Batch: 200 ... Loss: 6661.15722656\n",
      "------------------- Test set: Average loss: 1218622.6316 ... Samples: 809\n",
      "Train Epoch: 20 ... Batch: 0 ... Loss: 6199.61328125\n",
      "Train Epoch: 20 ... Batch: 10 ... Loss: 8964.83496094\n",
      "Train Epoch: 20 ... Batch: 20 ... Loss: 6769.50781250\n",
      "Train Epoch: 20 ... Batch: 30 ... Loss: 8236.89062500\n",
      "Train Epoch: 20 ... Batch: 40 ... Loss: 8367.25781250\n",
      "Train Epoch: 20 ... Batch: 50 ... Loss: 8715.10839844\n",
      "Train Epoch: 20 ... Batch: 60 ... Loss: 7311.11865234\n",
      "Train Epoch: 20 ... Batch: 70 ... Loss: 8287.47656250\n",
      "Train Epoch: 20 ... Batch: 80 ... Loss: 7597.59863281\n",
      "Train Epoch: 20 ... Batch: 90 ... Loss: 7564.73193359\n",
      "Train Epoch: 20 ... Batch: 100 ... Loss: 9195.33789062\n",
      "Train Epoch: 20 ... Batch: 110 ... Loss: 8392.64746094\n",
      "Train Epoch: 20 ... Batch: 120 ... Loss: 7484.39843750\n",
      "Train Epoch: 20 ... Batch: 130 ... Loss: 6358.42041016\n",
      "Train Epoch: 20 ... Batch: 140 ... Loss: 6804.35791016\n",
      "Train Epoch: 20 ... Batch: 150 ... Loss: 7272.47802734\n",
      "Train Epoch: 20 ... Batch: 160 ... Loss: 8302.74121094\n",
      "Train Epoch: 20 ... Batch: 170 ... Loss: 7166.32031250\n",
      "Train Epoch: 20 ... Batch: 180 ... Loss: 8497.45996094\n",
      "Train Epoch: 20 ... Batch: 190 ... Loss: 8659.72167969\n",
      "Train Epoch: 20 ... Batch: 200 ... Loss: 8337.92968750\n",
      "------------------- Test set: Average loss: 1218618.8331 ... Samples: 809\n",
      "Train Epoch: 21 ... Batch: 0 ... Loss: 7695.17041016\n",
      "Train Epoch: 21 ... Batch: 10 ... Loss: 8408.58105469\n",
      "Train Epoch: 21 ... Batch: 20 ... Loss: 8426.17480469\n",
      "Train Epoch: 21 ... Batch: 30 ... Loss: 9885.06933594\n",
      "Train Epoch: 21 ... Batch: 40 ... Loss: 8151.58300781\n",
      "Train Epoch: 21 ... Batch: 50 ... Loss: 7470.74169922\n",
      "Train Epoch: 21 ... Batch: 60 ... Loss: 7689.68359375\n",
      "Train Epoch: 21 ... Batch: 70 ... Loss: 7547.50146484\n",
      "Train Epoch: 21 ... Batch: 80 ... Loss: 7326.10009766\n",
      "Train Epoch: 21 ... Batch: 90 ... Loss: 7987.81591797\n",
      "Train Epoch: 21 ... Batch: 100 ... Loss: 8272.52636719\n",
      "Train Epoch: 21 ... Batch: 110 ... Loss: 7931.72802734\n",
      "Train Epoch: 21 ... Batch: 120 ... Loss: 7876.75341797\n",
      "Train Epoch: 21 ... Batch: 130 ... Loss: 8154.30615234\n",
      "Train Epoch: 21 ... Batch: 140 ... Loss: 6510.43066406\n",
      "Train Epoch: 21 ... Batch: 150 ... Loss: 9006.65332031\n",
      "Train Epoch: 21 ... Batch: 160 ... Loss: 7873.52978516\n",
      "Train Epoch: 21 ... Batch: 170 ... Loss: 7882.63916016\n",
      "Train Epoch: 21 ... Batch: 180 ... Loss: 7116.11718750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21 ... Batch: 190 ... Loss: 7197.08740234\n",
      "Train Epoch: 21 ... Batch: 200 ... Loss: 8315.19238281\n",
      "------------------- Test set: Average loss: 1218619.0507 ... Samples: 809\n",
      "Train Epoch: 22 ... Batch: 0 ... Loss: 7831.58203125\n",
      "Train Epoch: 22 ... Batch: 10 ... Loss: 7723.28369141\n",
      "Train Epoch: 22 ... Batch: 20 ... Loss: 8742.20996094\n",
      "Train Epoch: 22 ... Batch: 30 ... Loss: 7547.58300781\n",
      "Train Epoch: 22 ... Batch: 40 ... Loss: 7762.93750000\n",
      "Train Epoch: 22 ... Batch: 50 ... Loss: 8126.70166016\n",
      "Train Epoch: 22 ... Batch: 60 ... Loss: 8041.93310547\n",
      "Train Epoch: 22 ... Batch: 70 ... Loss: 7403.96875000\n",
      "Train Epoch: 22 ... Batch: 80 ... Loss: 6053.90771484\n",
      "Train Epoch: 22 ... Batch: 90 ... Loss: 7206.35156250\n",
      "Train Epoch: 22 ... Batch: 100 ... Loss: 7668.54541016\n",
      "Train Epoch: 22 ... Batch: 110 ... Loss: 7303.29052734\n",
      "Train Epoch: 22 ... Batch: 120 ... Loss: 8398.14746094\n",
      "Train Epoch: 22 ... Batch: 130 ... Loss: 8478.76464844\n",
      "Train Epoch: 22 ... Batch: 140 ... Loss: 7193.12060547\n",
      "Train Epoch: 22 ... Batch: 150 ... Loss: 8030.56250000\n",
      "Train Epoch: 22 ... Batch: 160 ... Loss: 7657.07812500\n",
      "Train Epoch: 22 ... Batch: 170 ... Loss: 7052.60693359\n",
      "Train Epoch: 22 ... Batch: 180 ... Loss: 7666.06103516\n",
      "Train Epoch: 22 ... Batch: 190 ... Loss: 7163.44531250\n",
      "Train Epoch: 22 ... Batch: 200 ... Loss: 8794.92675781\n",
      "------------------- Test set: Average loss: 1218619.4895 ... Samples: 809\n",
      "Train Epoch: 23 ... Batch: 0 ... Loss: 8112.90625000\n",
      "Train Epoch: 23 ... Batch: 10 ... Loss: 8931.90820312\n",
      "Train Epoch: 23 ... Batch: 20 ... Loss: 8824.72949219\n",
      "Train Epoch: 23 ... Batch: 30 ... Loss: 8112.11718750\n",
      "Train Epoch: 23 ... Batch: 40 ... Loss: 8314.81445312\n",
      "Train Epoch: 23 ... Batch: 50 ... Loss: 9000.33886719\n",
      "Train Epoch: 23 ... Batch: 60 ... Loss: 6482.71826172\n",
      "Train Epoch: 23 ... Batch: 70 ... Loss: 7918.34472656\n",
      "Train Epoch: 23 ... Batch: 80 ... Loss: 7820.07031250\n",
      "Train Epoch: 23 ... Batch: 90 ... Loss: 8802.28027344\n",
      "Train Epoch: 23 ... Batch: 100 ... Loss: 7568.23583984\n",
      "Train Epoch: 23 ... Batch: 110 ... Loss: 7445.53271484\n",
      "Train Epoch: 23 ... Batch: 120 ... Loss: 8492.33496094\n",
      "Train Epoch: 23 ... Batch: 130 ... Loss: 8218.30175781\n",
      "Train Epoch: 23 ... Batch: 140 ... Loss: 7240.12500000\n",
      "Train Epoch: 23 ... Batch: 150 ... Loss: 7391.51708984\n",
      "Train Epoch: 23 ... Batch: 160 ... Loss: 7532.74169922\n",
      "Train Epoch: 23 ... Batch: 170 ... Loss: 8287.11914062\n",
      "Train Epoch: 23 ... Batch: 180 ... Loss: 8216.03808594\n",
      "Train Epoch: 23 ... Batch: 190 ... Loss: 8111.79833984\n",
      "Train Epoch: 23 ... Batch: 200 ... Loss: 9108.97949219\n",
      "------------------- Test set: Average loss: 1218620.6774 ... Samples: 809\n",
      "Train Epoch: 24 ... Batch: 0 ... Loss: 7437.99365234\n",
      "Train Epoch: 24 ... Batch: 10 ... Loss: 8935.18652344\n",
      "Train Epoch: 24 ... Batch: 20 ... Loss: 7363.91748047\n",
      "Train Epoch: 24 ... Batch: 30 ... Loss: 7321.65625000\n",
      "Train Epoch: 24 ... Batch: 40 ... Loss: 9231.96386719\n",
      "Train Epoch: 24 ... Batch: 50 ... Loss: 7632.53759766\n",
      "Train Epoch: 24 ... Batch: 60 ... Loss: 7567.13378906\n",
      "Train Epoch: 24 ... Batch: 70 ... Loss: 7680.41503906\n",
      "Train Epoch: 24 ... Batch: 80 ... Loss: 7032.05419922\n",
      "Train Epoch: 24 ... Batch: 90 ... Loss: 8716.72949219\n",
      "Train Epoch: 24 ... Batch: 100 ... Loss: 8218.57519531\n",
      "Train Epoch: 24 ... Batch: 110 ... Loss: 8496.07519531\n",
      "Train Epoch: 24 ... Batch: 120 ... Loss: 8442.63476562\n",
      "Train Epoch: 24 ... Batch: 130 ... Loss: 7067.92529297\n",
      "Train Epoch: 24 ... Batch: 140 ... Loss: 8395.72265625\n",
      "Train Epoch: 24 ... Batch: 150 ... Loss: 8013.67529297\n",
      "Train Epoch: 24 ... Batch: 160 ... Loss: 7052.20166016\n",
      "Train Epoch: 24 ... Batch: 170 ... Loss: 7910.69873047\n",
      "Train Epoch: 24 ... Batch: 180 ... Loss: 7077.15869141\n",
      "Train Epoch: 24 ... Batch: 190 ... Loss: 6746.88525391\n",
      "Train Epoch: 24 ... Batch: 200 ... Loss: 7802.05810547\n",
      "------------------- Test set: Average loss: 1218619.2237 ... Samples: 809\n",
      "Train Epoch: 25 ... Batch: 0 ... Loss: 8456.56933594\n",
      "Train Epoch: 25 ... Batch: 10 ... Loss: 8472.19824219\n",
      "Train Epoch: 25 ... Batch: 20 ... Loss: 8345.36914062\n",
      "Train Epoch: 25 ... Batch: 30 ... Loss: 6810.07958984\n",
      "Train Epoch: 25 ... Batch: 40 ... Loss: 7396.02197266\n",
      "Train Epoch: 25 ... Batch: 50 ... Loss: 7535.42333984\n",
      "Train Epoch: 25 ... Batch: 60 ... Loss: 7473.89550781\n",
      "Train Epoch: 25 ... Batch: 70 ... Loss: 9061.13964844\n",
      "Train Epoch: 25 ... Batch: 80 ... Loss: 8178.59863281\n",
      "Train Epoch: 25 ... Batch: 90 ... Loss: 6674.01171875\n",
      "Train Epoch: 25 ... Batch: 100 ... Loss: 9135.72558594\n",
      "Train Epoch: 25 ... Batch: 110 ... Loss: 8392.69433594\n",
      "Train Epoch: 25 ... Batch: 120 ... Loss: 6680.43896484\n",
      "Train Epoch: 25 ... Batch: 130 ... Loss: 7236.19628906\n",
      "Train Epoch: 25 ... Batch: 140 ... Loss: 6781.68310547\n",
      "Train Epoch: 25 ... Batch: 150 ... Loss: 9083.80371094\n",
      "Train Epoch: 25 ... Batch: 160 ... Loss: 8741.74609375\n",
      "Train Epoch: 25 ... Batch: 170 ... Loss: 8280.84082031\n",
      "Train Epoch: 25 ... Batch: 180 ... Loss: 8192.62109375\n",
      "Train Epoch: 25 ... Batch: 190 ... Loss: 7693.02050781\n",
      "Train Epoch: 25 ... Batch: 200 ... Loss: 7436.01806641\n",
      "------------------- Test set: Average loss: 1218618.0655 ... Samples: 809\n",
      "Train Epoch: 26 ... Batch: 0 ... Loss: 6726.17822266\n",
      "Train Epoch: 26 ... Batch: 10 ... Loss: 7080.25244141\n",
      "Train Epoch: 26 ... Batch: 20 ... Loss: 6316.53808594\n",
      "Train Epoch: 26 ... Batch: 30 ... Loss: 8081.91894531\n",
      "Train Epoch: 26 ... Batch: 40 ... Loss: 7907.04687500\n",
      "Train Epoch: 26 ... Batch: 50 ... Loss: 6429.32763672\n",
      "Train Epoch: 26 ... Batch: 60 ... Loss: 7441.23925781\n",
      "Train Epoch: 26 ... Batch: 70 ... Loss: 8043.27734375\n",
      "Train Epoch: 26 ... Batch: 80 ... Loss: 8723.66601562\n",
      "Train Epoch: 26 ... Batch: 90 ... Loss: 8010.78759766\n",
      "Train Epoch: 26 ... Batch: 100 ... Loss: 8734.02246094\n",
      "Train Epoch: 26 ... Batch: 110 ... Loss: 7964.36279297\n",
      "Train Epoch: 26 ... Batch: 120 ... Loss: 8358.78710938\n",
      "Train Epoch: 26 ... Batch: 130 ... Loss: 8547.71875000\n",
      "Train Epoch: 26 ... Batch: 140 ... Loss: 9011.09277344\n",
      "Train Epoch: 26 ... Batch: 150 ... Loss: 8435.83691406\n",
      "Train Epoch: 26 ... Batch: 160 ... Loss: 7640.14794922\n",
      "Train Epoch: 26 ... Batch: 170 ... Loss: 8102.79150391\n",
      "Train Epoch: 26 ... Batch: 180 ... Loss: 8221.55664062\n",
      "Train Epoch: 26 ... Batch: 190 ... Loss: 7772.70312500\n",
      "Train Epoch: 26 ... Batch: 200 ... Loss: 8621.33886719\n",
      "------------------- Test set: Average loss: 1218616.2126 ... Samples: 809\n",
      "Train Epoch: 27 ... Batch: 0 ... Loss: 9228.92968750\n",
      "Train Epoch: 27 ... Batch: 10 ... Loss: 7958.27490234\n",
      "Train Epoch: 27 ... Batch: 20 ... Loss: 7337.47607422\n",
      "Train Epoch: 27 ... Batch: 30 ... Loss: 7384.68115234\n",
      "Train Epoch: 27 ... Batch: 40 ... Loss: 8369.64550781\n",
      "Train Epoch: 27 ... Batch: 50 ... Loss: 7285.48583984\n",
      "Train Epoch: 27 ... Batch: 60 ... Loss: 7573.88623047\n",
      "Train Epoch: 27 ... Batch: 70 ... Loss: 8050.64404297\n",
      "Train Epoch: 27 ... Batch: 80 ... Loss: 8154.24462891\n",
      "Train Epoch: 27 ... Batch: 90 ... Loss: 9279.13183594\n",
      "Train Epoch: 27 ... Batch: 100 ... Loss: 8262.82714844\n",
      "Train Epoch: 27 ... Batch: 110 ... Loss: 6133.03906250\n",
      "Train Epoch: 27 ... Batch: 120 ... Loss: 7712.50927734\n",
      "Train Epoch: 27 ... Batch: 130 ... Loss: 7178.46728516\n",
      "Train Epoch: 27 ... Batch: 140 ... Loss: 7859.41552734\n",
      "Train Epoch: 27 ... Batch: 150 ... Loss: 6943.35156250\n",
      "Train Epoch: 27 ... Batch: 160 ... Loss: 8214.68261719\n",
      "Train Epoch: 27 ... Batch: 170 ... Loss: 9167.77539062\n",
      "Train Epoch: 27 ... Batch: 180 ... Loss: 6977.46484375\n",
      "Train Epoch: 27 ... Batch: 190 ... Loss: 6389.14599609\n",
      "Train Epoch: 27 ... Batch: 200 ... Loss: 7280.17919922\n",
      "------------------- Test set: Average loss: 1218615.5451 ... Samples: 809\n",
      "Train Epoch: 28 ... Batch: 0 ... Loss: 8559.48925781\n",
      "Train Epoch: 28 ... Batch: 10 ... Loss: 7110.70703125\n",
      "Train Epoch: 28 ... Batch: 20 ... Loss: 7479.82666016\n",
      "Train Epoch: 28 ... Batch: 30 ... Loss: 7092.60009766\n",
      "Train Epoch: 28 ... Batch: 40 ... Loss: 7908.03076172\n",
      "Train Epoch: 28 ... Batch: 50 ... Loss: 7740.95019531\n",
      "Train Epoch: 28 ... Batch: 60 ... Loss: 7547.01318359\n",
      "Train Epoch: 28 ... Batch: 70 ... Loss: 7960.81591797\n",
      "Train Epoch: 28 ... Batch: 80 ... Loss: 8149.14208984\n",
      "Train Epoch: 28 ... Batch: 90 ... Loss: 7302.26416016\n",
      "Train Epoch: 28 ... Batch: 100 ... Loss: 7434.85888672\n",
      "Train Epoch: 28 ... Batch: 110 ... Loss: 8670.31933594\n",
      "Train Epoch: 28 ... Batch: 120 ... Loss: 8389.43066406\n",
      "Train Epoch: 28 ... Batch: 130 ... Loss: 7494.76708984\n",
      "Train Epoch: 28 ... Batch: 140 ... Loss: 8274.72949219\n",
      "Train Epoch: 28 ... Batch: 150 ... Loss: 7547.25537109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 ... Batch: 160 ... Loss: 8467.97949219\n",
      "Train Epoch: 28 ... Batch: 170 ... Loss: 7261.39404297\n",
      "Train Epoch: 28 ... Batch: 180 ... Loss: 8108.70166016\n",
      "Train Epoch: 28 ... Batch: 190 ... Loss: 8130.60937500\n",
      "Train Epoch: 28 ... Batch: 200 ... Loss: 8002.89404297\n",
      "------------------- Test set: Average loss: 1218615.5847 ... Samples: 809\n",
      "Train Epoch: 29 ... Batch: 0 ... Loss: 8570.84082031\n",
      "Train Epoch: 29 ... Batch: 10 ... Loss: 7179.55615234\n",
      "Train Epoch: 29 ... Batch: 20 ... Loss: 8232.60058594\n",
      "Train Epoch: 29 ... Batch: 30 ... Loss: 7638.56103516\n",
      "Train Epoch: 29 ... Batch: 40 ... Loss: 7058.30468750\n",
      "Train Epoch: 29 ... Batch: 50 ... Loss: 7703.33349609\n",
      "Train Epoch: 29 ... Batch: 60 ... Loss: 7505.48388672\n",
      "Train Epoch: 29 ... Batch: 70 ... Loss: 7931.75927734\n",
      "Train Epoch: 29 ... Batch: 80 ... Loss: 8002.72656250\n",
      "Train Epoch: 29 ... Batch: 90 ... Loss: 7640.08447266\n",
      "Train Epoch: 29 ... Batch: 100 ... Loss: 10170.22558594\n",
      "Train Epoch: 29 ... Batch: 110 ... Loss: 7035.53271484\n",
      "Train Epoch: 29 ... Batch: 120 ... Loss: 7822.10498047\n",
      "Train Epoch: 29 ... Batch: 130 ... Loss: 9488.94238281\n",
      "Train Epoch: 29 ... Batch: 140 ... Loss: 8106.83447266\n",
      "Train Epoch: 29 ... Batch: 150 ... Loss: 7598.74609375\n",
      "Train Epoch: 29 ... Batch: 160 ... Loss: 7583.45849609\n",
      "Train Epoch: 29 ... Batch: 170 ... Loss: 8548.31542969\n",
      "Train Epoch: 29 ... Batch: 180 ... Loss: 7664.11865234\n",
      "Train Epoch: 29 ... Batch: 190 ... Loss: 7365.97021484\n",
      "Train Epoch: 29 ... Batch: 200 ... Loss: 7860.37744141\n",
      "------------------- Test set: Average loss: 1218615.1990 ... Samples: 809\n",
      "Train Epoch: 30 ... Batch: 0 ... Loss: 8357.16406250\n",
      "Train Epoch: 30 ... Batch: 10 ... Loss: 7823.14697266\n",
      "Train Epoch: 30 ... Batch: 20 ... Loss: 8422.65136719\n",
      "Train Epoch: 30 ... Batch: 30 ... Loss: 8436.05957031\n",
      "Train Epoch: 30 ... Batch: 40 ... Loss: 8682.89746094\n",
      "Train Epoch: 30 ... Batch: 50 ... Loss: 7732.91259766\n",
      "Train Epoch: 30 ... Batch: 60 ... Loss: 7188.22656250\n",
      "Train Epoch: 30 ... Batch: 70 ... Loss: 7944.73144531\n",
      "Train Epoch: 30 ... Batch: 80 ... Loss: 8438.08789062\n",
      "Train Epoch: 30 ... Batch: 90 ... Loss: 7929.33203125\n",
      "Train Epoch: 30 ... Batch: 100 ... Loss: 7421.28906250\n",
      "Train Epoch: 30 ... Batch: 110 ... Loss: 7864.04687500\n",
      "Train Epoch: 30 ... Batch: 120 ... Loss: 6966.16796875\n",
      "Train Epoch: 30 ... Batch: 130 ... Loss: 6984.14404297\n",
      "Train Epoch: 30 ... Batch: 140 ... Loss: 8890.25976562\n",
      "Train Epoch: 30 ... Batch: 150 ... Loss: 7755.54003906\n",
      "Train Epoch: 30 ... Batch: 160 ... Loss: 7176.41552734\n",
      "Train Epoch: 30 ... Batch: 170 ... Loss: 7651.61035156\n",
      "Train Epoch: 30 ... Batch: 180 ... Loss: 8618.90039062\n",
      "Train Epoch: 30 ... Batch: 190 ... Loss: 7554.57812500\n",
      "Train Epoch: 30 ... Batch: 200 ... Loss: 7535.84375000\n",
      "------------------- Test set: Average loss: 1218616.3671 ... Samples: 809\n",
      "Train Epoch: 31 ... Batch: 0 ... Loss: 8211.78613281\n",
      "Train Epoch: 31 ... Batch: 10 ... Loss: 8557.61621094\n",
      "Train Epoch: 31 ... Batch: 20 ... Loss: 7830.57031250\n",
      "Train Epoch: 31 ... Batch: 30 ... Loss: 7064.86425781\n",
      "Train Epoch: 31 ... Batch: 40 ... Loss: 7684.43505859\n",
      "Train Epoch: 31 ... Batch: 50 ... Loss: 7937.31396484\n",
      "Train Epoch: 31 ... Batch: 60 ... Loss: 7292.71728516\n",
      "Train Epoch: 31 ... Batch: 70 ... Loss: 8547.10351562\n",
      "Train Epoch: 31 ... Batch: 80 ... Loss: 8700.21582031\n",
      "Train Epoch: 31 ... Batch: 90 ... Loss: 9367.58300781\n",
      "Train Epoch: 31 ... Batch: 100 ... Loss: 6768.34619141\n",
      "Train Epoch: 31 ... Batch: 110 ... Loss: 8158.00390625\n",
      "Train Epoch: 31 ... Batch: 120 ... Loss: 7498.57177734\n",
      "Train Epoch: 31 ... Batch: 130 ... Loss: 8029.27685547\n",
      "Train Epoch: 31 ... Batch: 140 ... Loss: 8304.73730469\n",
      "Train Epoch: 31 ... Batch: 150 ... Loss: 7235.07910156\n",
      "Train Epoch: 31 ... Batch: 160 ... Loss: 8849.23437500\n",
      "Train Epoch: 31 ... Batch: 170 ... Loss: 6598.63037109\n",
      "Train Epoch: 31 ... Batch: 180 ... Loss: 6772.60400391\n",
      "Train Epoch: 31 ... Batch: 190 ... Loss: 7249.31494141\n",
      "Train Epoch: 31 ... Batch: 200 ... Loss: 7894.48291016\n",
      "------------------- Test set: Average loss: 1218614.1335 ... Samples: 809\n",
      "Train Epoch: 32 ... Batch: 0 ... Loss: 7921.72802734\n",
      "Train Epoch: 32 ... Batch: 10 ... Loss: 7393.35302734\n",
      "Train Epoch: 32 ... Batch: 20 ... Loss: 8380.20898438\n",
      "Train Epoch: 32 ... Batch: 30 ... Loss: 8010.40234375\n",
      "Train Epoch: 32 ... Batch: 40 ... Loss: 8718.88183594\n",
      "Train Epoch: 32 ... Batch: 50 ... Loss: 7592.97509766\n",
      "Train Epoch: 32 ... Batch: 60 ... Loss: 7677.37841797\n",
      "Train Epoch: 32 ... Batch: 70 ... Loss: 7815.35302734\n",
      "Train Epoch: 32 ... Batch: 80 ... Loss: 8143.10888672\n",
      "Train Epoch: 32 ... Batch: 90 ... Loss: 7552.83593750\n",
      "Train Epoch: 32 ... Batch: 100 ... Loss: 7160.73388672\n",
      "Train Epoch: 32 ... Batch: 110 ... Loss: 7474.22998047\n",
      "Train Epoch: 32 ... Batch: 120 ... Loss: 7588.49169922\n",
      "Train Epoch: 32 ... Batch: 130 ... Loss: 8149.07519531\n",
      "Train Epoch: 32 ... Batch: 140 ... Loss: 7749.88281250\n",
      "Train Epoch: 32 ... Batch: 150 ... Loss: 6470.33984375\n",
      "Train Epoch: 32 ... Batch: 160 ... Loss: 8112.41406250\n",
      "Train Epoch: 32 ... Batch: 170 ... Loss: 6849.56738281\n",
      "Train Epoch: 32 ... Batch: 180 ... Loss: 6868.60888672\n",
      "Train Epoch: 32 ... Batch: 190 ... Loss: 8980.39746094\n",
      "Train Epoch: 32 ... Batch: 200 ... Loss: 7744.09228516\n",
      "------------------- Test set: Average loss: 1218613.8455 ... Samples: 809\n",
      "Train Epoch: 33 ... Batch: 0 ... Loss: 7440.55078125\n",
      "Train Epoch: 33 ... Batch: 10 ... Loss: 8472.28320312\n",
      "Train Epoch: 33 ... Batch: 20 ... Loss: 7429.40869141\n",
      "Train Epoch: 33 ... Batch: 30 ... Loss: 7183.66650391\n",
      "Train Epoch: 33 ... Batch: 40 ... Loss: 6778.33300781\n",
      "Train Epoch: 33 ... Batch: 50 ... Loss: 7980.68115234\n",
      "Train Epoch: 33 ... Batch: 60 ... Loss: 7141.99365234\n",
      "Train Epoch: 33 ... Batch: 70 ... Loss: 8132.84228516\n",
      "Train Epoch: 33 ... Batch: 80 ... Loss: 6923.58203125\n",
      "Train Epoch: 33 ... Batch: 90 ... Loss: 6260.32275391\n",
      "Train Epoch: 33 ... Batch: 100 ... Loss: 7679.76513672\n",
      "Train Epoch: 33 ... Batch: 110 ... Loss: 6856.11718750\n",
      "Train Epoch: 33 ... Batch: 120 ... Loss: 8739.19042969\n",
      "Train Epoch: 33 ... Batch: 130 ... Loss: 7830.79785156\n",
      "Train Epoch: 33 ... Batch: 140 ... Loss: 8523.63378906\n",
      "Train Epoch: 33 ... Batch: 150 ... Loss: 7797.74218750\n",
      "Train Epoch: 33 ... Batch: 160 ... Loss: 6945.90966797\n",
      "Train Epoch: 33 ... Batch: 170 ... Loss: 8032.76708984\n",
      "Train Epoch: 33 ... Batch: 180 ... Loss: 8118.83935547\n",
      "Train Epoch: 33 ... Batch: 190 ... Loss: 7787.71875000\n",
      "Train Epoch: 33 ... Batch: 200 ... Loss: 7688.56347656\n",
      "------------------- Test set: Average loss: 1218612.0544 ... Samples: 809\n",
      "Train Epoch: 34 ... Batch: 0 ... Loss: 7160.08300781\n",
      "Train Epoch: 34 ... Batch: 10 ... Loss: 7652.13427734\n",
      "Train Epoch: 34 ... Batch: 20 ... Loss: 8329.72363281\n",
      "Train Epoch: 34 ... Batch: 30 ... Loss: 7848.24853516\n",
      "Train Epoch: 34 ... Batch: 40 ... Loss: 8178.54931641\n",
      "Train Epoch: 34 ... Batch: 50 ... Loss: 7107.26269531\n",
      "Train Epoch: 34 ... Batch: 60 ... Loss: 6504.52490234\n",
      "Train Epoch: 34 ... Batch: 70 ... Loss: 7268.70654297\n",
      "Train Epoch: 34 ... Batch: 80 ... Loss: 7673.32031250\n",
      "Train Epoch: 34 ... Batch: 90 ... Loss: 7099.38281250\n",
      "Train Epoch: 34 ... Batch: 100 ... Loss: 8604.50683594\n",
      "Train Epoch: 34 ... Batch: 110 ... Loss: 8729.19531250\n",
      "Train Epoch: 34 ... Batch: 120 ... Loss: 6990.77587891\n",
      "Train Epoch: 34 ... Batch: 130 ... Loss: 7712.42529297\n",
      "Train Epoch: 34 ... Batch: 140 ... Loss: 7766.73291016\n",
      "Train Epoch: 34 ... Batch: 150 ... Loss: 6701.25244141\n",
      "Train Epoch: 34 ... Batch: 160 ... Loss: 9232.88378906\n",
      "Train Epoch: 34 ... Batch: 170 ... Loss: 8487.55371094\n",
      "Train Epoch: 34 ... Batch: 180 ... Loss: 7379.85888672\n",
      "Train Epoch: 34 ... Batch: 190 ... Loss: 9401.94335938\n",
      "Train Epoch: 34 ... Batch: 200 ... Loss: 8194.72656250\n",
      "------------------- Test set: Average loss: 1218611.3140 ... Samples: 809\n",
      "Train Epoch: 35 ... Batch: 0 ... Loss: 8083.28857422\n",
      "Train Epoch: 35 ... Batch: 10 ... Loss: 6563.68603516\n",
      "Train Epoch: 35 ... Batch: 20 ... Loss: 7290.98535156\n",
      "Train Epoch: 35 ... Batch: 30 ... Loss: 8014.75244141\n",
      "Train Epoch: 35 ... Batch: 40 ... Loss: 6398.77050781\n",
      "Train Epoch: 35 ... Batch: 50 ... Loss: 7568.70312500\n",
      "Train Epoch: 35 ... Batch: 60 ... Loss: 8496.26074219\n",
      "Train Epoch: 35 ... Batch: 70 ... Loss: 7779.84863281\n",
      "Train Epoch: 35 ... Batch: 80 ... Loss: 7244.53759766\n",
      "Train Epoch: 35 ... Batch: 90 ... Loss: 8365.77636719\n",
      "Train Epoch: 35 ... Batch: 100 ... Loss: 7672.36718750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35 ... Batch: 110 ... Loss: 7866.81591797\n",
      "Train Epoch: 35 ... Batch: 120 ... Loss: 9184.68164062\n",
      "Train Epoch: 35 ... Batch: 130 ... Loss: 7140.65332031\n",
      "Train Epoch: 35 ... Batch: 140 ... Loss: 7313.20410156\n",
      "Train Epoch: 35 ... Batch: 150 ... Loss: 8277.00585938\n",
      "Train Epoch: 35 ... Batch: 160 ... Loss: 7295.03613281\n",
      "Train Epoch: 35 ... Batch: 170 ... Loss: 8241.69824219\n",
      "Train Epoch: 35 ... Batch: 180 ... Loss: 8793.54199219\n",
      "Train Epoch: 35 ... Batch: 190 ... Loss: 6981.63525391\n",
      "Train Epoch: 35 ... Batch: 200 ... Loss: 7031.50488281\n",
      "------------------- Test set: Average loss: 1218611.9604 ... Samples: 809\n",
      "Train Epoch: 36 ... Batch: 0 ... Loss: 9329.10156250\n",
      "Train Epoch: 36 ... Batch: 10 ... Loss: 6997.87988281\n",
      "Train Epoch: 36 ... Batch: 20 ... Loss: 7351.46875000\n",
      "Train Epoch: 36 ... Batch: 30 ... Loss: 8475.75195312\n",
      "Train Epoch: 36 ... Batch: 40 ... Loss: 7037.12353516\n",
      "Train Epoch: 36 ... Batch: 50 ... Loss: 8583.38574219\n",
      "Train Epoch: 36 ... Batch: 60 ... Loss: 8700.85937500\n",
      "Train Epoch: 36 ... Batch: 70 ... Loss: 7818.73779297\n",
      "Train Epoch: 36 ... Batch: 80 ... Loss: 6763.10644531\n",
      "Train Epoch: 36 ... Batch: 90 ... Loss: 7482.88281250\n",
      "Train Epoch: 36 ... Batch: 100 ... Loss: 7791.88671875\n",
      "Train Epoch: 36 ... Batch: 110 ... Loss: 8163.99707031\n",
      "Train Epoch: 36 ... Batch: 120 ... Loss: 7595.07128906\n",
      "Train Epoch: 36 ... Batch: 130 ... Loss: 7870.22363281\n",
      "Train Epoch: 36 ... Batch: 140 ... Loss: 8991.38671875\n",
      "Train Epoch: 36 ... Batch: 150 ... Loss: 7026.17333984\n",
      "Train Epoch: 36 ... Batch: 160 ... Loss: 6902.69091797\n",
      "Train Epoch: 36 ... Batch: 170 ... Loss: 8827.29199219\n",
      "Train Epoch: 36 ... Batch: 180 ... Loss: 8699.45507812\n",
      "Train Epoch: 36 ... Batch: 190 ... Loss: 9474.35058594\n",
      "Train Epoch: 36 ... Batch: 200 ... Loss: 8750.01757812\n",
      "------------------- Test set: Average loss: 1218610.2831 ... Samples: 809\n",
      "Train Epoch: 37 ... Batch: 0 ... Loss: 7822.18896484\n",
      "Train Epoch: 37 ... Batch: 10 ... Loss: 7482.39404297\n",
      "Train Epoch: 37 ... Batch: 20 ... Loss: 7798.20263672\n",
      "Train Epoch: 37 ... Batch: 30 ... Loss: 8176.67529297\n",
      "Train Epoch: 37 ... Batch: 40 ... Loss: 8344.64648438\n",
      "Train Epoch: 37 ... Batch: 50 ... Loss: 7710.11083984\n",
      "Train Epoch: 37 ... Batch: 60 ... Loss: 9071.40429688\n",
      "Train Epoch: 37 ... Batch: 70 ... Loss: 6164.82812500\n",
      "Train Epoch: 37 ... Batch: 80 ... Loss: 6600.64013672\n",
      "Train Epoch: 37 ... Batch: 90 ... Loss: 7361.88378906\n",
      "Train Epoch: 37 ... Batch: 100 ... Loss: 8235.94531250\n",
      "Train Epoch: 37 ... Batch: 110 ... Loss: 8936.14160156\n",
      "Train Epoch: 37 ... Batch: 120 ... Loss: 8405.06933594\n",
      "Train Epoch: 37 ... Batch: 130 ... Loss: 8207.78125000\n",
      "Train Epoch: 37 ... Batch: 140 ... Loss: 7560.57275391\n",
      "Train Epoch: 37 ... Batch: 150 ... Loss: 8609.03808594\n",
      "Train Epoch: 37 ... Batch: 160 ... Loss: 7642.19091797\n",
      "Train Epoch: 37 ... Batch: 170 ... Loss: 7716.52294922\n",
      "Train Epoch: 37 ... Batch: 180 ... Loss: 8295.28613281\n",
      "Train Epoch: 37 ... Batch: 190 ... Loss: 6900.11279297\n",
      "Train Epoch: 37 ... Batch: 200 ... Loss: 7754.55078125\n",
      "------------------- Test set: Average loss: 1218609.1817 ... Samples: 809\n",
      "Train Epoch: 38 ... Batch: 0 ... Loss: 7525.67333984\n",
      "Train Epoch: 38 ... Batch: 10 ... Loss: 7735.07373047\n",
      "Train Epoch: 38 ... Batch: 20 ... Loss: 8917.41308594\n",
      "Train Epoch: 38 ... Batch: 30 ... Loss: 9002.70019531\n",
      "Train Epoch: 38 ... Batch: 40 ... Loss: 6981.03271484\n",
      "Train Epoch: 38 ... Batch: 50 ... Loss: 9356.65332031\n",
      "Train Epoch: 38 ... Batch: 60 ... Loss: 7706.44140625\n",
      "Train Epoch: 38 ... Batch: 70 ... Loss: 8461.63183594\n",
      "Train Epoch: 38 ... Batch: 80 ... Loss: 8140.10107422\n",
      "Train Epoch: 38 ... Batch: 90 ... Loss: 8181.42529297\n",
      "Train Epoch: 38 ... Batch: 100 ... Loss: 7189.63427734\n",
      "Train Epoch: 38 ... Batch: 110 ... Loss: 7500.37500000\n",
      "Train Epoch: 38 ... Batch: 120 ... Loss: 8325.00683594\n",
      "Train Epoch: 38 ... Batch: 130 ... Loss: 7133.02050781\n",
      "Train Epoch: 38 ... Batch: 140 ... Loss: 6789.98046875\n",
      "Train Epoch: 38 ... Batch: 150 ... Loss: 8649.61718750\n",
      "Train Epoch: 38 ... Batch: 160 ... Loss: 8105.51318359\n",
      "Train Epoch: 38 ... Batch: 170 ... Loss: 6964.21875000\n",
      "Train Epoch: 38 ... Batch: 180 ... Loss: 8875.50585938\n",
      "Train Epoch: 38 ... Batch: 190 ... Loss: 6996.85791016\n",
      "Train Epoch: 38 ... Batch: 200 ... Loss: 7683.97802734\n",
      "------------------- Test set: Average loss: 1218609.3993 ... Samples: 809\n",
      "Train Epoch: 39 ... Batch: 0 ... Loss: 8894.59570312\n",
      "Train Epoch: 39 ... Batch: 10 ... Loss: 7578.40234375\n",
      "Train Epoch: 39 ... Batch: 20 ... Loss: 8766.17187500\n",
      "Train Epoch: 39 ... Batch: 30 ... Loss: 7831.35791016\n",
      "Train Epoch: 39 ... Batch: 40 ... Loss: 7976.03613281\n",
      "Train Epoch: 39 ... Batch: 50 ... Loss: 7262.10498047\n",
      "Train Epoch: 39 ... Batch: 60 ... Loss: 6901.53076172\n",
      "Train Epoch: 39 ... Batch: 70 ... Loss: 8393.14355469\n",
      "Train Epoch: 39 ... Batch: 80 ... Loss: 8921.40625000\n",
      "Train Epoch: 39 ... Batch: 90 ... Loss: 6206.33056641\n",
      "Train Epoch: 39 ... Batch: 100 ... Loss: 6950.43603516\n",
      "Train Epoch: 39 ... Batch: 110 ... Loss: 7732.93896484\n",
      "Train Epoch: 39 ... Batch: 120 ... Loss: 7954.91650391\n",
      "Train Epoch: 39 ... Batch: 130 ... Loss: 7382.98974609\n",
      "Train Epoch: 39 ... Batch: 140 ... Loss: 8365.98144531\n",
      "Train Epoch: 39 ... Batch: 150 ... Loss: 8417.06933594\n",
      "Train Epoch: 39 ... Batch: 160 ... Loss: 6992.86816406\n",
      "Train Epoch: 39 ... Batch: 170 ... Loss: 6667.82763672\n",
      "Train Epoch: 39 ... Batch: 180 ... Loss: 7298.05957031\n",
      "Train Epoch: 39 ... Batch: 190 ... Loss: 8045.02099609\n",
      "Train Epoch: 39 ... Batch: 200 ... Loss: 8158.56250000\n",
      "------------------- Test set: Average loss: 1218607.2349 ... Samples: 809\n",
      "Train Epoch: 40 ... Batch: 0 ... Loss: 7825.53515625\n",
      "Train Epoch: 40 ... Batch: 10 ... Loss: 7490.98388672\n",
      "Train Epoch: 40 ... Batch: 20 ... Loss: 9013.22167969\n",
      "Train Epoch: 40 ... Batch: 30 ... Loss: 7929.08544922\n",
      "Train Epoch: 40 ... Batch: 40 ... Loss: 8385.48339844\n",
      "Train Epoch: 40 ... Batch: 50 ... Loss: 7924.33056641\n",
      "Train Epoch: 40 ... Batch: 60 ... Loss: 7822.32031250\n",
      "Train Epoch: 40 ... Batch: 70 ... Loss: 8156.61425781\n",
      "Train Epoch: 40 ... Batch: 80 ... Loss: 7279.33154297\n",
      "Train Epoch: 40 ... Batch: 90 ... Loss: 7306.58593750\n",
      "Train Epoch: 40 ... Batch: 100 ... Loss: 7607.16406250\n",
      "Train Epoch: 40 ... Batch: 110 ... Loss: 8085.33349609\n",
      "Train Epoch: 40 ... Batch: 120 ... Loss: 7732.12988281\n",
      "Train Epoch: 40 ... Batch: 130 ... Loss: 8094.64404297\n",
      "Train Epoch: 40 ... Batch: 140 ... Loss: 8549.59570312\n",
      "Train Epoch: 40 ... Batch: 150 ... Loss: 8430.56933594\n",
      "Train Epoch: 40 ... Batch: 160 ... Loss: 9600.59082031\n",
      "Train Epoch: 40 ... Batch: 170 ... Loss: 7233.97753906\n",
      "Train Epoch: 40 ... Batch: 180 ... Loss: 8251.78417969\n",
      "Train Epoch: 40 ... Batch: 190 ... Loss: 7894.73193359\n",
      "Train Epoch: 40 ... Batch: 200 ... Loss: 7833.40380859\n",
      "------------------- Test set: Average loss: 1218607.6230 ... Samples: 809\n",
      "Train Epoch: 41 ... Batch: 0 ... Loss: 6529.41259766\n",
      "Train Epoch: 41 ... Batch: 10 ... Loss: 7024.24365234\n",
      "Train Epoch: 41 ... Batch: 20 ... Loss: 7424.24707031\n",
      "Train Epoch: 41 ... Batch: 30 ... Loss: 7330.26269531\n",
      "Train Epoch: 41 ... Batch: 40 ... Loss: 8146.58056641\n",
      "Train Epoch: 41 ... Batch: 50 ... Loss: 10042.41015625\n",
      "Train Epoch: 41 ... Batch: 60 ... Loss: 7924.72216797\n",
      "Train Epoch: 41 ... Batch: 70 ... Loss: 7257.51708984\n",
      "Train Epoch: 41 ... Batch: 80 ... Loss: 9174.41601562\n",
      "Train Epoch: 41 ... Batch: 90 ... Loss: 7468.43115234\n",
      "Train Epoch: 41 ... Batch: 100 ... Loss: 7448.29003906\n",
      "Train Epoch: 41 ... Batch: 110 ... Loss: 7556.60644531\n",
      "Train Epoch: 41 ... Batch: 120 ... Loss: 7715.13916016\n",
      "Train Epoch: 41 ... Batch: 130 ... Loss: 8482.01171875\n",
      "Train Epoch: 41 ... Batch: 140 ... Loss: 8704.53125000\n",
      "Train Epoch: 41 ... Batch: 150 ... Loss: 8084.82958984\n",
      "Train Epoch: 41 ... Batch: 160 ... Loss: 7452.73779297\n",
      "Train Epoch: 41 ... Batch: 170 ... Loss: 7606.56103516\n",
      "Train Epoch: 41 ... Batch: 180 ... Loss: 8336.99218750\n",
      "Train Epoch: 41 ... Batch: 190 ... Loss: 7453.76123047\n",
      "Train Epoch: 41 ... Batch: 200 ... Loss: 7943.40185547\n",
      "------------------- Test set: Average loss: 1218606.5192 ... Samples: 809\n",
      "Train Epoch: 42 ... Batch: 0 ... Loss: 9035.96386719\n",
      "Train Epoch: 42 ... Batch: 10 ... Loss: 8335.45312500\n",
      "Train Epoch: 42 ... Batch: 20 ... Loss: 7639.16503906\n",
      "Train Epoch: 42 ... Batch: 30 ... Loss: 8599.50097656\n",
      "Train Epoch: 42 ... Batch: 40 ... Loss: 7934.23437500\n",
      "Train Epoch: 42 ... Batch: 50 ... Loss: 8053.54052734\n",
      "Train Epoch: 42 ... Batch: 60 ... Loss: 7084.29638672\n",
      "Train Epoch: 42 ... Batch: 70 ... Loss: 7470.42968750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42 ... Batch: 80 ... Loss: 6779.08300781\n",
      "Train Epoch: 42 ... Batch: 90 ... Loss: 7448.05029297\n",
      "Train Epoch: 42 ... Batch: 100 ... Loss: 9780.87011719\n",
      "Train Epoch: 42 ... Batch: 110 ... Loss: 8187.89550781\n",
      "Train Epoch: 42 ... Batch: 120 ... Loss: 7828.19921875\n",
      "Train Epoch: 42 ... Batch: 130 ... Loss: 8823.18750000\n",
      "Train Epoch: 42 ... Batch: 140 ... Loss: 7261.08691406\n",
      "Train Epoch: 42 ... Batch: 150 ... Loss: 8055.06494141\n",
      "Train Epoch: 42 ... Batch: 160 ... Loss: 9217.72949219\n",
      "Train Epoch: 42 ... Batch: 170 ... Loss: 7621.24707031\n",
      "Train Epoch: 42 ... Batch: 180 ... Loss: 7647.70166016\n",
      "Train Epoch: 42 ... Batch: 190 ... Loss: 8286.01855469\n",
      "Train Epoch: 42 ... Batch: 200 ... Loss: 8726.26074219\n",
      "------------------- Test set: Average loss: 1218604.5822 ... Samples: 809\n",
      "Train Epoch: 43 ... Batch: 0 ... Loss: 6254.96191406\n",
      "Train Epoch: 43 ... Batch: 10 ... Loss: 7422.30468750\n",
      "Train Epoch: 43 ... Batch: 20 ... Loss: 8236.83886719\n",
      "Train Epoch: 43 ... Batch: 30 ... Loss: 7765.99218750\n",
      "Train Epoch: 43 ... Batch: 40 ... Loss: 7525.19873047\n",
      "Train Epoch: 43 ... Batch: 50 ... Loss: 8894.70800781\n",
      "Train Epoch: 43 ... Batch: 60 ... Loss: 7503.21728516\n",
      "Train Epoch: 43 ... Batch: 70 ... Loss: 7571.67529297\n",
      "Train Epoch: 43 ... Batch: 80 ... Loss: 7699.43115234\n",
      "Train Epoch: 43 ... Batch: 90 ... Loss: 6370.89794922\n",
      "Train Epoch: 43 ... Batch: 100 ... Loss: 8621.12304688\n",
      "Train Epoch: 43 ... Batch: 110 ... Loss: 7021.18896484\n",
      "Train Epoch: 43 ... Batch: 120 ... Loss: 8262.46191406\n",
      "Train Epoch: 43 ... Batch: 130 ... Loss: 6953.31396484\n",
      "Train Epoch: 43 ... Batch: 140 ... Loss: 7949.00097656\n",
      "Train Epoch: 43 ... Batch: 150 ... Loss: 8329.42773438\n",
      "Train Epoch: 43 ... Batch: 160 ... Loss: 8809.15527344\n",
      "Train Epoch: 43 ... Batch: 170 ... Loss: 8288.94042969\n",
      "Train Epoch: 43 ... Batch: 180 ... Loss: 8186.29150391\n",
      "Train Epoch: 43 ... Batch: 190 ... Loss: 8592.82421875\n",
      "Train Epoch: 43 ... Batch: 200 ... Loss: 6876.20410156\n",
      "------------------- Test set: Average loss: 1218603.6230 ... Samples: 809\n",
      "Train Epoch: 44 ... Batch: 0 ... Loss: 8270.57324219\n",
      "Train Epoch: 44 ... Batch: 10 ... Loss: 7132.96728516\n",
      "Train Epoch: 44 ... Batch: 20 ... Loss: 7681.47607422\n",
      "Train Epoch: 44 ... Batch: 30 ... Loss: 7343.62988281\n",
      "Train Epoch: 44 ... Batch: 40 ... Loss: 8217.02246094\n",
      "Train Epoch: 44 ... Batch: 50 ... Loss: 6561.46875000\n",
      "Train Epoch: 44 ... Batch: 60 ... Loss: 8448.28222656\n",
      "Train Epoch: 44 ... Batch: 70 ... Loss: 8122.60546875\n",
      "Train Epoch: 44 ... Batch: 80 ... Loss: 7859.97802734\n",
      "Train Epoch: 44 ... Batch: 90 ... Loss: 8161.37890625\n",
      "Train Epoch: 44 ... Batch: 100 ... Loss: 7331.70410156\n",
      "Train Epoch: 44 ... Batch: 110 ... Loss: 7290.41748047\n",
      "Train Epoch: 44 ... Batch: 120 ... Loss: 7443.13281250\n",
      "Train Epoch: 44 ... Batch: 130 ... Loss: 8460.74902344\n",
      "Train Epoch: 44 ... Batch: 140 ... Loss: 6956.67529297\n",
      "Train Epoch: 44 ... Batch: 150 ... Loss: 7296.26953125\n",
      "Train Epoch: 44 ... Batch: 160 ... Loss: 8812.57226562\n",
      "Train Epoch: 44 ... Batch: 170 ... Loss: 7072.13769531\n",
      "Train Epoch: 44 ... Batch: 180 ... Loss: 7525.96240234\n",
      "Train Epoch: 44 ... Batch: 190 ... Loss: 8268.92382812\n",
      "Train Epoch: 44 ... Batch: 200 ... Loss: 7714.01708984\n",
      "------------------- Test set: Average loss: 1218602.8504 ... Samples: 809\n",
      "Train Epoch: 45 ... Batch: 0 ... Loss: 7692.80957031\n",
      "Train Epoch: 45 ... Batch: 10 ... Loss: 8920.04687500\n",
      "Train Epoch: 45 ... Batch: 20 ... Loss: 7707.23583984\n",
      "Train Epoch: 45 ... Batch: 30 ... Loss: 7108.06347656\n",
      "Train Epoch: 45 ... Batch: 40 ... Loss: 7441.84472656\n",
      "Train Epoch: 45 ... Batch: 50 ... Loss: 8243.21582031\n",
      "Train Epoch: 45 ... Batch: 60 ... Loss: 7587.40625000\n",
      "Train Epoch: 45 ... Batch: 70 ... Loss: 8959.07910156\n",
      "Train Epoch: 45 ... Batch: 80 ... Loss: 7895.66894531\n",
      "Train Epoch: 45 ... Batch: 90 ... Loss: 7938.13281250\n",
      "Train Epoch: 45 ... Batch: 100 ... Loss: 6966.21191406\n",
      "Train Epoch: 45 ... Batch: 110 ... Loss: 7640.28906250\n",
      "Train Epoch: 45 ... Batch: 120 ... Loss: 10126.85742188\n",
      "Train Epoch: 45 ... Batch: 130 ... Loss: 7507.27832031\n",
      "Train Epoch: 45 ... Batch: 140 ... Loss: 7164.47119141\n",
      "Train Epoch: 45 ... Batch: 150 ... Loss: 7587.19482422\n",
      "Train Epoch: 45 ... Batch: 160 ... Loss: 8016.07666016\n",
      "Train Epoch: 45 ... Batch: 170 ... Loss: 7158.70800781\n",
      "Train Epoch: 45 ... Batch: 180 ... Loss: 8792.74707031\n",
      "Train Epoch: 45 ... Batch: 190 ... Loss: 7832.44384766\n",
      "Train Epoch: 45 ... Batch: 200 ... Loss: 7887.14550781\n",
      "------------------- Test set: Average loss: 1218602.1607 ... Samples: 809\n",
      "Train Epoch: 46 ... Batch: 0 ... Loss: 7881.56201172\n",
      "Train Epoch: 46 ... Batch: 10 ... Loss: 7653.79003906\n",
      "Train Epoch: 46 ... Batch: 20 ... Loss: 7527.31591797\n",
      "Train Epoch: 46 ... Batch: 30 ... Loss: 7226.98535156\n",
      "Train Epoch: 46 ... Batch: 40 ... Loss: 7174.26171875\n",
      "Train Epoch: 46 ... Batch: 50 ... Loss: 7206.31250000\n",
      "Train Epoch: 46 ... Batch: 60 ... Loss: 8519.23339844\n",
      "Train Epoch: 46 ... Batch: 70 ... Loss: 7855.39550781\n",
      "Train Epoch: 46 ... Batch: 80 ... Loss: 6786.82519531\n",
      "Train Epoch: 46 ... Batch: 90 ... Loss: 8284.37597656\n",
      "Train Epoch: 46 ... Batch: 100 ... Loss: 7089.09472656\n",
      "Train Epoch: 46 ... Batch: 110 ... Loss: 8554.09082031\n",
      "Train Epoch: 46 ... Batch: 120 ... Loss: 8092.08935547\n",
      "Train Epoch: 46 ... Batch: 130 ... Loss: 6636.80322266\n",
      "Train Epoch: 46 ... Batch: 140 ... Loss: 9544.14746094\n",
      "Train Epoch: 46 ... Batch: 150 ... Loss: 6836.05029297\n",
      "Train Epoch: 46 ... Batch: 160 ... Loss: 8499.80664062\n",
      "Train Epoch: 46 ... Batch: 170 ... Loss: 8742.04687500\n",
      "Train Epoch: 46 ... Batch: 180 ... Loss: 7772.14013672\n",
      "Train Epoch: 46 ... Batch: 190 ... Loss: 7906.50634766\n",
      "Train Epoch: 46 ... Batch: 200 ... Loss: 8655.01562500\n",
      "------------------- Test set: Average loss: 1218605.3894 ... Samples: 809\n",
      "Train Epoch: 47 ... Batch: 0 ... Loss: 7898.28125000\n",
      "Train Epoch: 47 ... Batch: 10 ... Loss: 8092.05468750\n",
      "Train Epoch: 47 ... Batch: 20 ... Loss: 8341.13574219\n",
      "Train Epoch: 47 ... Batch: 30 ... Loss: 8163.98144531\n",
      "Train Epoch: 47 ... Batch: 40 ... Loss: 6908.03906250\n",
      "Train Epoch: 47 ... Batch: 50 ... Loss: 8550.67382812\n",
      "Train Epoch: 47 ... Batch: 60 ... Loss: 8214.56738281\n",
      "Train Epoch: 47 ... Batch: 70 ... Loss: 7643.56738281\n",
      "Train Epoch: 47 ... Batch: 80 ... Loss: 7459.44384766\n",
      "Train Epoch: 47 ... Batch: 90 ... Loss: 8538.58496094\n",
      "Train Epoch: 47 ... Batch: 100 ... Loss: 6508.20410156\n",
      "Train Epoch: 47 ... Batch: 110 ... Loss: 8610.82226562\n",
      "Train Epoch: 47 ... Batch: 120 ... Loss: 7628.58447266\n",
      "Train Epoch: 47 ... Batch: 130 ... Loss: 7598.58740234\n",
      "Train Epoch: 47 ... Batch: 140 ... Loss: 7711.56787109\n",
      "Train Epoch: 47 ... Batch: 150 ... Loss: 7868.18212891\n",
      "Train Epoch: 47 ... Batch: 160 ... Loss: 8483.37402344\n",
      "Train Epoch: 47 ... Batch: 170 ... Loss: 7532.03515625\n",
      "Train Epoch: 47 ... Batch: 180 ... Loss: 8228.39843750\n",
      "Train Epoch: 47 ... Batch: 190 ... Loss: 8422.73144531\n",
      "Train Epoch: 47 ... Batch: 200 ... Loss: 7156.98437500\n",
      "------------------- Test set: Average loss: 1218604.3993 ... Samples: 809\n",
      "Train Epoch: 48 ... Batch: 0 ... Loss: 8040.06591797\n",
      "Train Epoch: 48 ... Batch: 10 ... Loss: 7954.47900391\n",
      "Train Epoch: 48 ... Batch: 20 ... Loss: 7871.03125000\n",
      "Train Epoch: 48 ... Batch: 30 ... Loss: 8578.47558594\n",
      "Train Epoch: 48 ... Batch: 40 ... Loss: 7900.89550781\n",
      "Train Epoch: 48 ... Batch: 50 ... Loss: 8117.71728516\n",
      "Train Epoch: 48 ... Batch: 60 ... Loss: 8261.59863281\n",
      "Train Epoch: 48 ... Batch: 70 ... Loss: 7502.09863281\n",
      "Train Epoch: 48 ... Batch: 80 ... Loss: 7073.85644531\n",
      "Train Epoch: 48 ... Batch: 90 ... Loss: 8204.46875000\n",
      "Train Epoch: 48 ... Batch: 100 ... Loss: 8295.59082031\n",
      "Train Epoch: 48 ... Batch: 110 ... Loss: 7360.14990234\n",
      "Train Epoch: 48 ... Batch: 120 ... Loss: 8197.55175781\n",
      "Train Epoch: 48 ... Batch: 130 ... Loss: 7115.63769531\n",
      "Train Epoch: 48 ... Batch: 140 ... Loss: 8262.03613281\n",
      "Train Epoch: 48 ... Batch: 150 ... Loss: 9250.32031250\n",
      "Train Epoch: 48 ... Batch: 160 ... Loss: 6783.18115234\n",
      "Train Epoch: 48 ... Batch: 170 ... Loss: 7627.85888672\n",
      "Train Epoch: 48 ... Batch: 180 ... Loss: 7946.70703125\n",
      "Train Epoch: 48 ... Batch: 190 ... Loss: 8103.78613281\n",
      "Train Epoch: 48 ... Batch: 200 ... Loss: 8204.62792969\n",
      "------------------- Test set: Average loss: 1218603.6712 ... Samples: 809\n",
      "Train Epoch: 49 ... Batch: 0 ... Loss: 6759.49169922\n",
      "Train Epoch: 49 ... Batch: 10 ... Loss: 8359.11230469\n",
      "Train Epoch: 49 ... Batch: 20 ... Loss: 8674.19433594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 ... Batch: 30 ... Loss: 7224.82958984\n",
      "Train Epoch: 49 ... Batch: 40 ... Loss: 6540.15185547\n",
      "Train Epoch: 49 ... Batch: 50 ... Loss: 6776.25634766\n",
      "Train Epoch: 49 ... Batch: 60 ... Loss: 8035.90234375\n",
      "Train Epoch: 49 ... Batch: 70 ... Loss: 7471.69628906\n",
      "Train Epoch: 49 ... Batch: 80 ... Loss: 7619.15771484\n",
      "Train Epoch: 49 ... Batch: 90 ... Loss: 7393.28759766\n",
      "Train Epoch: 49 ... Batch: 100 ... Loss: 8228.04785156\n",
      "Train Epoch: 49 ... Batch: 110 ... Loss: 7641.31884766\n",
      "Train Epoch: 49 ... Batch: 120 ... Loss: 7822.41259766\n",
      "Train Epoch: 49 ... Batch: 130 ... Loss: 8553.10449219\n",
      "Train Epoch: 49 ... Batch: 140 ... Loss: 8154.59082031\n",
      "Train Epoch: 49 ... Batch: 150 ... Loss: 8086.69384766\n",
      "Train Epoch: 49 ... Batch: 160 ... Loss: 7594.45019531\n",
      "Train Epoch: 49 ... Batch: 170 ... Loss: 7138.25097656\n",
      "Train Epoch: 49 ... Batch: 180 ... Loss: 7844.48583984\n",
      "Train Epoch: 49 ... Batch: 190 ... Loss: 7538.97656250\n",
      "Train Epoch: 49 ... Batch: 200 ... Loss: 7243.65478516\n",
      "------------------- Test set: Average loss: 1218602.1261 ... Samples: 809\n",
      "Train Epoch: 50 ... Batch: 0 ... Loss: 9410.86132812\n",
      "Train Epoch: 50 ... Batch: 10 ... Loss: 8327.98925781\n",
      "Train Epoch: 50 ... Batch: 20 ... Loss: 7023.96728516\n",
      "Train Epoch: 50 ... Batch: 30 ... Loss: 8722.96289062\n",
      "Train Epoch: 50 ... Batch: 40 ... Loss: 6971.89404297\n",
      "Train Epoch: 50 ... Batch: 50 ... Loss: 7796.85253906\n",
      "Train Epoch: 50 ... Batch: 60 ... Loss: 7329.23828125\n",
      "Train Epoch: 50 ... Batch: 70 ... Loss: 9161.48437500\n",
      "Train Epoch: 50 ... Batch: 80 ... Loss: 7523.27490234\n",
      "Train Epoch: 50 ... Batch: 90 ... Loss: 8662.74511719\n",
      "Train Epoch: 50 ... Batch: 100 ... Loss: 6348.69482422\n",
      "Train Epoch: 50 ... Batch: 110 ... Loss: 8735.22656250\n",
      "Train Epoch: 50 ... Batch: 120 ... Loss: 8026.88525391\n",
      "Train Epoch: 50 ... Batch: 130 ... Loss: 7955.50146484\n",
      "Train Epoch: 50 ... Batch: 140 ... Loss: 6883.82177734\n",
      "Train Epoch: 50 ... Batch: 150 ... Loss: 7650.29785156\n",
      "Train Epoch: 50 ... Batch: 160 ... Loss: 7715.93994141\n",
      "Train Epoch: 50 ... Batch: 170 ... Loss: 7911.56103516\n",
      "Train Epoch: 50 ... Batch: 180 ... Loss: 7800.45312500\n",
      "Train Epoch: 50 ... Batch: 190 ... Loss: 7774.80468750\n",
      "Train Epoch: 50 ... Batch: 200 ... Loss: 8794.62988281\n",
      "------------------- Test set: Average loss: 1218601.8801 ... Samples: 809\n",
      "Train Epoch: 51 ... Batch: 0 ... Loss: 8259.22070312\n",
      "Train Epoch: 51 ... Batch: 10 ... Loss: 9112.61816406\n",
      "Train Epoch: 51 ... Batch: 20 ... Loss: 6598.22656250\n",
      "Train Epoch: 51 ... Batch: 30 ... Loss: 6596.46826172\n",
      "Train Epoch: 51 ... Batch: 40 ... Loss: 8430.23730469\n",
      "Train Epoch: 51 ... Batch: 50 ... Loss: 8484.00292969\n",
      "Train Epoch: 51 ... Batch: 60 ... Loss: 8550.75000000\n",
      "Train Epoch: 51 ... Batch: 70 ... Loss: 7141.36865234\n",
      "Train Epoch: 51 ... Batch: 80 ... Loss: 7649.49072266\n",
      "Train Epoch: 51 ... Batch: 90 ... Loss: 7452.92285156\n",
      "Train Epoch: 51 ... Batch: 100 ... Loss: 7628.21484375\n",
      "Train Epoch: 51 ... Batch: 110 ... Loss: 8745.66210938\n",
      "Train Epoch: 51 ... Batch: 120 ... Loss: 7481.87353516\n",
      "Train Epoch: 51 ... Batch: 130 ... Loss: 8860.30957031\n",
      "Train Epoch: 51 ... Batch: 140 ... Loss: 6940.76562500\n",
      "Train Epoch: 51 ... Batch: 150 ... Loss: 7753.82519531\n",
      "Train Epoch: 51 ... Batch: 160 ... Loss: 8282.90332031\n",
      "Train Epoch: 51 ... Batch: 170 ... Loss: 8524.91113281\n",
      "Train Epoch: 51 ... Batch: 180 ... Loss: 8223.29589844\n",
      "Train Epoch: 51 ... Batch: 190 ... Loss: 6281.88671875\n",
      "Train Epoch: 51 ... Batch: 200 ... Loss: 7961.41259766\n",
      "------------------- Test set: Average loss: 1218602.8653 ... Samples: 809\n",
      "Train Epoch: 52 ... Batch: 0 ... Loss: 8219.93164062\n",
      "Train Epoch: 52 ... Batch: 10 ... Loss: 8667.28906250\n",
      "Train Epoch: 52 ... Batch: 20 ... Loss: 8492.58593750\n",
      "Train Epoch: 52 ... Batch: 30 ... Loss: 6931.20800781\n",
      "Train Epoch: 52 ... Batch: 40 ... Loss: 8429.10937500\n",
      "Train Epoch: 52 ... Batch: 50 ... Loss: 8772.91308594\n",
      "Train Epoch: 52 ... Batch: 60 ... Loss: 7599.46093750\n",
      "Train Epoch: 52 ... Batch: 70 ... Loss: 7025.92187500\n",
      "Train Epoch: 52 ... Batch: 80 ... Loss: 7489.99853516\n",
      "Train Epoch: 52 ... Batch: 90 ... Loss: 8124.17285156\n",
      "Train Epoch: 52 ... Batch: 100 ... Loss: 7961.66894531\n",
      "Train Epoch: 52 ... Batch: 110 ... Loss: 8522.70703125\n",
      "Train Epoch: 52 ... Batch: 120 ... Loss: 8198.76855469\n",
      "Train Epoch: 52 ... Batch: 130 ... Loss: 7552.78662109\n",
      "Train Epoch: 52 ... Batch: 140 ... Loss: 7529.62353516\n",
      "Train Epoch: 52 ... Batch: 150 ... Loss: 8691.66601562\n",
      "Train Epoch: 52 ... Batch: 160 ... Loss: 8439.86718750\n",
      "Train Epoch: 52 ... Batch: 170 ... Loss: 8695.26074219\n",
      "Train Epoch: 52 ... Batch: 180 ... Loss: 8015.77587891\n",
      "Train Epoch: 52 ... Batch: 190 ... Loss: 7912.01123047\n",
      "Train Epoch: 52 ... Batch: 200 ... Loss: 8138.93603516\n",
      "------------------- Test set: Average loss: 1218600.1965 ... Samples: 809\n",
      "Train Epoch: 53 ... Batch: 0 ... Loss: 8650.16699219\n",
      "Train Epoch: 53 ... Batch: 10 ... Loss: 7540.82031250\n",
      "Train Epoch: 53 ... Batch: 20 ... Loss: 7235.17919922\n",
      "Train Epoch: 53 ... Batch: 30 ... Loss: 8299.20214844\n",
      "Train Epoch: 53 ... Batch: 40 ... Loss: 7326.90576172\n",
      "Train Epoch: 53 ... Batch: 50 ... Loss: 6914.01171875\n",
      "Train Epoch: 53 ... Batch: 60 ... Loss: 8309.62792969\n",
      "Train Epoch: 53 ... Batch: 70 ... Loss: 7525.48974609\n",
      "Train Epoch: 53 ... Batch: 80 ... Loss: 6836.55712891\n",
      "Train Epoch: 53 ... Batch: 90 ... Loss: 8087.74365234\n",
      "Train Epoch: 53 ... Batch: 100 ... Loss: 6706.77050781\n",
      "Train Epoch: 53 ... Batch: 110 ... Loss: 8121.87841797\n",
      "Train Epoch: 53 ... Batch: 120 ... Loss: 7630.35791016\n",
      "Train Epoch: 53 ... Batch: 130 ... Loss: 7724.38916016\n",
      "Train Epoch: 53 ... Batch: 140 ... Loss: 8734.80664062\n",
      "Train Epoch: 53 ... Batch: 150 ... Loss: 7195.91650391\n",
      "Train Epoch: 53 ... Batch: 160 ... Loss: 7996.79833984\n",
      "Train Epoch: 53 ... Batch: 170 ... Loss: 8469.58789062\n",
      "Train Epoch: 53 ... Batch: 180 ... Loss: 8085.66748047\n",
      "Train Epoch: 53 ... Batch: 190 ... Loss: 7654.23681641\n",
      "Train Epoch: 53 ... Batch: 200 ... Loss: 7941.91162109\n",
      "------------------- Test set: Average loss: 1218599.6255 ... Samples: 809\n",
      "Train Epoch: 54 ... Batch: 0 ... Loss: 7679.41894531\n",
      "Train Epoch: 54 ... Batch: 10 ... Loss: 7565.09228516\n",
      "Train Epoch: 54 ... Batch: 20 ... Loss: 8451.60058594\n",
      "Train Epoch: 54 ... Batch: 30 ... Loss: 7923.76416016\n",
      "Train Epoch: 54 ... Batch: 40 ... Loss: 7387.26025391\n",
      "Train Epoch: 54 ... Batch: 50 ... Loss: 9344.87011719\n",
      "Train Epoch: 54 ... Batch: 60 ... Loss: 8151.83349609\n",
      "Train Epoch: 54 ... Batch: 70 ... Loss: 7608.04541016\n",
      "Train Epoch: 54 ... Batch: 80 ... Loss: 9048.78808594\n",
      "Train Epoch: 54 ... Batch: 90 ... Loss: 8477.23535156\n",
      "Train Epoch: 54 ... Batch: 100 ... Loss: 8437.87500000\n",
      "Train Epoch: 54 ... Batch: 110 ... Loss: 7611.02294922\n",
      "Train Epoch: 54 ... Batch: 120 ... Loss: 7767.30322266\n",
      "Train Epoch: 54 ... Batch: 130 ... Loss: 7330.82666016\n",
      "Train Epoch: 54 ... Batch: 140 ... Loss: 8805.83886719\n",
      "Train Epoch: 54 ... Batch: 150 ... Loss: 8211.17285156\n",
      "Train Epoch: 54 ... Batch: 160 ... Loss: 8200.60644531\n",
      "Train Epoch: 54 ... Batch: 170 ... Loss: 7181.35644531\n",
      "Train Epoch: 54 ... Batch: 180 ... Loss: 7534.48583984\n",
      "Train Epoch: 54 ... Batch: 190 ... Loss: 9259.84667969\n",
      "Train Epoch: 54 ... Batch: 200 ... Loss: 7639.06640625\n",
      "------------------- Test set: Average loss: 1218597.3857 ... Samples: 809\n",
      "Train Epoch: 55 ... Batch: 0 ... Loss: 8359.83300781\n",
      "Train Epoch: 55 ... Batch: 10 ... Loss: 8963.69140625\n",
      "Train Epoch: 55 ... Batch: 20 ... Loss: 8455.76269531\n",
      "Train Epoch: 55 ... Batch: 30 ... Loss: 7076.43457031\n",
      "Train Epoch: 55 ... Batch: 40 ... Loss: 7828.04931641\n",
      "Train Epoch: 55 ... Batch: 50 ... Loss: 7377.80957031\n",
      "Train Epoch: 55 ... Batch: 60 ... Loss: 7116.36572266\n",
      "Train Epoch: 55 ... Batch: 70 ... Loss: 7653.70410156\n",
      "Train Epoch: 55 ... Batch: 80 ... Loss: 7239.23291016\n",
      "Train Epoch: 55 ... Batch: 90 ... Loss: 8622.03613281\n",
      "Train Epoch: 55 ... Batch: 100 ... Loss: 8394.17480469\n",
      "Train Epoch: 55 ... Batch: 110 ... Loss: 8342.75195312\n",
      "Train Epoch: 55 ... Batch: 120 ... Loss: 8425.11914062\n",
      "Train Epoch: 55 ... Batch: 130 ... Loss: 7518.31396484\n",
      "Train Epoch: 55 ... Batch: 140 ... Loss: 8455.25097656\n",
      "Train Epoch: 55 ... Batch: 150 ... Loss: 7294.12207031\n",
      "Train Epoch: 55 ... Batch: 160 ... Loss: 6452.99755859\n",
      "Train Epoch: 55 ... Batch: 170 ... Loss: 6532.61572266\n",
      "Train Epoch: 55 ... Batch: 180 ... Loss: 8067.13623047\n",
      "Train Epoch: 55 ... Batch: 190 ... Loss: 7395.79296875\n",
      "Train Epoch: 55 ... Batch: 200 ... Loss: 7158.63378906\n",
      "------------------- Test set: Average loss: 1218597.3226 ... Samples: 809\n",
      "Train Epoch: 56 ... Batch: 0 ... Loss: 7435.24560547\n",
      "Train Epoch: 56 ... Batch: 10 ... Loss: 8392.42187500\n",
      "Train Epoch: 56 ... Batch: 20 ... Loss: 7020.06591797\n",
      "Train Epoch: 56 ... Batch: 30 ... Loss: 8683.91113281\n",
      "Train Epoch: 56 ... Batch: 40 ... Loss: 8198.14355469\n",
      "Train Epoch: 56 ... Batch: 50 ... Loss: 7629.46044922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 56 ... Batch: 60 ... Loss: 10101.72167969\n",
      "Train Epoch: 56 ... Batch: 70 ... Loss: 7524.38623047\n",
      "Train Epoch: 56 ... Batch: 80 ... Loss: 8026.13281250\n",
      "Train Epoch: 56 ... Batch: 90 ... Loss: 8413.14550781\n",
      "Train Epoch: 56 ... Batch: 100 ... Loss: 7455.92333984\n",
      "Train Epoch: 56 ... Batch: 110 ... Loss: 6591.57177734\n",
      "Train Epoch: 56 ... Batch: 120 ... Loss: 6529.67529297\n",
      "Train Epoch: 56 ... Batch: 130 ... Loss: 10300.84570312\n",
      "Train Epoch: 56 ... Batch: 140 ... Loss: 7680.92675781\n",
      "Train Epoch: 56 ... Batch: 150 ... Loss: 8254.27636719\n",
      "Train Epoch: 56 ... Batch: 160 ... Loss: 7768.20800781\n",
      "Train Epoch: 56 ... Batch: 170 ... Loss: 6259.30615234\n",
      "Train Epoch: 56 ... Batch: 180 ... Loss: 8557.51855469\n",
      "Train Epoch: 56 ... Batch: 190 ... Loss: 8091.00390625\n",
      "Train Epoch: 56 ... Batch: 200 ... Loss: 8833.10253906\n",
      "------------------- Test set: Average loss: 1218595.6316 ... Samples: 809\n",
      "Train Epoch: 57 ... Batch: 0 ... Loss: 7337.73291016\n",
      "Train Epoch: 57 ... Batch: 10 ... Loss: 7820.56201172\n",
      "Train Epoch: 57 ... Batch: 20 ... Loss: 6348.59179688\n",
      "Train Epoch: 57 ... Batch: 30 ... Loss: 7498.88916016\n",
      "Train Epoch: 57 ... Batch: 40 ... Loss: 8692.57910156\n",
      "Train Epoch: 57 ... Batch: 50 ... Loss: 7345.89843750\n",
      "Train Epoch: 57 ... Batch: 60 ... Loss: 6947.31884766\n",
      "Train Epoch: 57 ... Batch: 70 ... Loss: 7608.07958984\n",
      "Train Epoch: 57 ... Batch: 80 ... Loss: 8987.95312500\n",
      "Train Epoch: 57 ... Batch: 90 ... Loss: 8559.71875000\n",
      "Train Epoch: 57 ... Batch: 100 ... Loss: 8906.43261719\n",
      "Train Epoch: 57 ... Batch: 110 ... Loss: 7271.09326172\n",
      "Train Epoch: 57 ... Batch: 120 ... Loss: 7919.96630859\n",
      "Train Epoch: 57 ... Batch: 130 ... Loss: 7260.43994141\n",
      "Train Epoch: 57 ... Batch: 140 ... Loss: 8406.64062500\n",
      "Train Epoch: 57 ... Batch: 150 ... Loss: 6626.71728516\n",
      "Train Epoch: 57 ... Batch: 160 ... Loss: 7680.13134766\n",
      "Train Epoch: 57 ... Batch: 170 ... Loss: 7471.22363281\n",
      "Train Epoch: 57 ... Batch: 180 ... Loss: 7316.60791016\n",
      "Train Epoch: 57 ... Batch: 190 ... Loss: 7621.58935547\n",
      "Train Epoch: 57 ... Batch: 200 ... Loss: 7777.35791016\n",
      "------------------- Test set: Average loss: 1218596.6551 ... Samples: 809\n",
      "Train Epoch: 58 ... Batch: 0 ... Loss: 7351.87988281\n",
      "Train Epoch: 58 ... Batch: 10 ... Loss: 7515.42333984\n",
      "Train Epoch: 58 ... Batch: 20 ... Loss: 7755.77050781\n",
      "Train Epoch: 58 ... Batch: 30 ... Loss: 7729.58837891\n",
      "Train Epoch: 58 ... Batch: 40 ... Loss: 6870.00878906\n",
      "Train Epoch: 58 ... Batch: 50 ... Loss: 7009.93750000\n",
      "Train Epoch: 58 ... Batch: 60 ... Loss: 8357.86523438\n",
      "Train Epoch: 58 ... Batch: 70 ... Loss: 7515.41552734\n",
      "Train Epoch: 58 ... Batch: 80 ... Loss: 9034.61914062\n",
      "Train Epoch: 58 ... Batch: 90 ... Loss: 7604.86962891\n",
      "Train Epoch: 58 ... Batch: 100 ... Loss: 7545.83691406\n",
      "Train Epoch: 58 ... Batch: 110 ... Loss: 9427.27246094\n",
      "Train Epoch: 58 ... Batch: 120 ... Loss: 8366.45996094\n",
      "Train Epoch: 58 ... Batch: 130 ... Loss: 6818.20019531\n",
      "Train Epoch: 58 ... Batch: 140 ... Loss: 8207.83105469\n",
      "Train Epoch: 58 ... Batch: 150 ... Loss: 8113.67578125\n",
      "Train Epoch: 58 ... Batch: 160 ... Loss: 8066.14208984\n",
      "Train Epoch: 58 ... Batch: 170 ... Loss: 7665.11572266\n",
      "Train Epoch: 58 ... Batch: 180 ... Loss: 8134.47363281\n",
      "Train Epoch: 58 ... Batch: 190 ... Loss: 6898.70166016\n",
      "Train Epoch: 58 ... Batch: 200 ... Loss: 6075.05615234\n",
      "------------------- Test set: Average loss: 1218597.1805 ... Samples: 809\n",
      "Train Epoch: 59 ... Batch: 0 ... Loss: 7725.42333984\n",
      "Train Epoch: 59 ... Batch: 10 ... Loss: 7762.42529297\n",
      "Train Epoch: 59 ... Batch: 20 ... Loss: 8406.26464844\n",
      "Train Epoch: 59 ... Batch: 30 ... Loss: 7297.02587891\n",
      "Train Epoch: 59 ... Batch: 40 ... Loss: 7036.55712891\n",
      "Train Epoch: 59 ... Batch: 50 ... Loss: 7390.40478516\n",
      "Train Epoch: 59 ... Batch: 60 ... Loss: 7731.09228516\n",
      "Train Epoch: 59 ... Batch: 70 ... Loss: 8531.37597656\n",
      "Train Epoch: 59 ... Batch: 80 ... Loss: 6805.02050781\n",
      "Train Epoch: 59 ... Batch: 90 ... Loss: 8243.28613281\n",
      "Train Epoch: 59 ... Batch: 100 ... Loss: 8345.26855469\n",
      "Train Epoch: 59 ... Batch: 110 ... Loss: 6990.69775391\n",
      "Train Epoch: 59 ... Batch: 120 ... Loss: 6667.30029297\n",
      "Train Epoch: 59 ... Batch: 130 ... Loss: 7927.36083984\n",
      "Train Epoch: 59 ... Batch: 140 ... Loss: 7732.27685547\n",
      "Train Epoch: 59 ... Batch: 150 ... Loss: 6689.47119141\n",
      "Train Epoch: 59 ... Batch: 160 ... Loss: 7694.21093750\n",
      "Train Epoch: 59 ... Batch: 170 ... Loss: 7014.90869141\n",
      "Train Epoch: 59 ... Batch: 180 ... Loss: 7835.43603516\n",
      "Train Epoch: 59 ... Batch: 190 ... Loss: 8567.69238281\n",
      "Train Epoch: 59 ... Batch: 200 ... Loss: 7960.85791016\n",
      "------------------- Test set: Average loss: 1218597.5550 ... Samples: 809\n",
      "Train Epoch: 60 ... Batch: 0 ... Loss: 7944.51904297\n",
      "Train Epoch: 60 ... Batch: 10 ... Loss: 8996.56933594\n",
      "Train Epoch: 60 ... Batch: 20 ... Loss: 8001.97412109\n",
      "Train Epoch: 60 ... Batch: 30 ... Loss: 7922.48144531\n",
      "Train Epoch: 60 ... Batch: 40 ... Loss: 7787.66943359\n",
      "Train Epoch: 60 ... Batch: 50 ... Loss: 7943.14013672\n",
      "Train Epoch: 60 ... Batch: 60 ... Loss: 8563.28320312\n",
      "Train Epoch: 60 ... Batch: 70 ... Loss: 7856.51562500\n",
      "Train Epoch: 60 ... Batch: 80 ... Loss: 6979.25927734\n",
      "Train Epoch: 60 ... Batch: 90 ... Loss: 8051.13281250\n",
      "Train Epoch: 60 ... Batch: 100 ... Loss: 8351.93457031\n",
      "Train Epoch: 60 ... Batch: 110 ... Loss: 7864.15966797\n",
      "Train Epoch: 60 ... Batch: 120 ... Loss: 8024.62060547\n",
      "Train Epoch: 60 ... Batch: 130 ... Loss: 7743.39062500\n",
      "Train Epoch: 60 ... Batch: 140 ... Loss: 8813.69531250\n",
      "Train Epoch: 60 ... Batch: 150 ... Loss: 7065.97412109\n",
      "Train Epoch: 60 ... Batch: 160 ... Loss: 7297.53271484\n",
      "Train Epoch: 60 ... Batch: 170 ... Loss: 8842.53613281\n",
      "Train Epoch: 60 ... Batch: 180 ... Loss: 8124.49707031\n",
      "Train Epoch: 60 ... Batch: 190 ... Loss: 7539.51904297\n",
      "Train Epoch: 60 ... Batch: 200 ... Loss: 7088.70019531\n",
      "------------------- Test set: Average loss: 1218598.7182 ... Samples: 809\n",
      "Train Epoch: 61 ... Batch: 0 ... Loss: 8426.86621094\n",
      "Train Epoch: 61 ... Batch: 10 ... Loss: 8509.07714844\n",
      "Train Epoch: 61 ... Batch: 20 ... Loss: 8172.34765625\n",
      "Train Epoch: 61 ... Batch: 30 ... Loss: 7626.04638672\n",
      "Train Epoch: 61 ... Batch: 40 ... Loss: 7628.31396484\n",
      "Train Epoch: 61 ... Batch: 50 ... Loss: 8252.62207031\n",
      "Train Epoch: 61 ... Batch: 60 ... Loss: 8163.45849609\n",
      "Train Epoch: 61 ... Batch: 70 ... Loss: 7306.91406250\n",
      "Train Epoch: 61 ... Batch: 80 ... Loss: 7687.39404297\n",
      "Train Epoch: 61 ... Batch: 90 ... Loss: 7093.91894531\n",
      "Train Epoch: 61 ... Batch: 100 ... Loss: 7698.50927734\n",
      "Train Epoch: 61 ... Batch: 110 ... Loss: 8022.29248047\n",
      "Train Epoch: 61 ... Batch: 120 ... Loss: 8401.03320312\n",
      "Train Epoch: 61 ... Batch: 130 ... Loss: 6696.64306641\n",
      "Train Epoch: 61 ... Batch: 140 ... Loss: 7137.15234375\n",
      "Train Epoch: 61 ... Batch: 150 ... Loss: 7056.41406250\n",
      "Train Epoch: 61 ... Batch: 160 ... Loss: 8090.20263672\n",
      "Train Epoch: 61 ... Batch: 170 ... Loss: 7307.62744141\n",
      "Train Epoch: 61 ... Batch: 180 ... Loss: 8102.79833984\n",
      "Train Epoch: 61 ... Batch: 190 ... Loss: 7278.97216797\n",
      "Train Epoch: 61 ... Batch: 200 ... Loss: 9458.88183594\n",
      "------------------- Test set: Average loss: 1218596.8269 ... Samples: 809\n",
      "Train Epoch: 62 ... Batch: 0 ... Loss: 6597.14062500\n",
      "Train Epoch: 62 ... Batch: 10 ... Loss: 8554.13574219\n",
      "Train Epoch: 62 ... Batch: 20 ... Loss: 8320.27246094\n",
      "Train Epoch: 62 ... Batch: 30 ... Loss: 7824.92822266\n",
      "Train Epoch: 62 ... Batch: 40 ... Loss: 8055.31396484\n",
      "Train Epoch: 62 ... Batch: 50 ... Loss: 7969.33203125\n",
      "Train Epoch: 62 ... Batch: 60 ... Loss: 8085.34716797\n",
      "Train Epoch: 62 ... Batch: 70 ... Loss: 7993.66113281\n",
      "Train Epoch: 62 ... Batch: 80 ... Loss: 7616.50341797\n",
      "Train Epoch: 62 ... Batch: 90 ... Loss: 8349.00000000\n",
      "Train Epoch: 62 ... Batch: 100 ... Loss: 6675.69238281\n",
      "Train Epoch: 62 ... Batch: 110 ... Loss: 7596.22363281\n",
      "Train Epoch: 62 ... Batch: 120 ... Loss: 7455.07519531\n",
      "Train Epoch: 62 ... Batch: 130 ... Loss: 8583.10644531\n",
      "Train Epoch: 62 ... Batch: 140 ... Loss: 8149.91259766\n",
      "Train Epoch: 62 ... Batch: 150 ... Loss: 8318.14257812\n",
      "Train Epoch: 62 ... Batch: 160 ... Loss: 7392.89550781\n",
      "Train Epoch: 62 ... Batch: 170 ... Loss: 8622.26074219\n",
      "Train Epoch: 62 ... Batch: 180 ... Loss: 8255.81738281\n",
      "Train Epoch: 62 ... Batch: 190 ... Loss: 6959.15332031\n",
      "Train Epoch: 62 ... Batch: 200 ... Loss: 8292.33007812\n",
      "------------------- Test set: Average loss: 1218597.3436 ... Samples: 809\n",
      "Train Epoch: 63 ... Batch: 0 ... Loss: 8725.84472656\n",
      "Train Epoch: 63 ... Batch: 10 ... Loss: 7252.29003906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 63 ... Batch: 20 ... Loss: 7701.35156250\n",
      "Train Epoch: 63 ... Batch: 30 ... Loss: 6540.34228516\n",
      "Train Epoch: 63 ... Batch: 40 ... Loss: 7721.25488281\n",
      "Train Epoch: 63 ... Batch: 50 ... Loss: 7677.81884766\n",
      "Train Epoch: 63 ... Batch: 60 ... Loss: 8284.95312500\n",
      "Train Epoch: 63 ... Batch: 70 ... Loss: 6722.64208984\n",
      "Train Epoch: 63 ... Batch: 80 ... Loss: 8420.89453125\n",
      "Train Epoch: 63 ... Batch: 90 ... Loss: 7509.73779297\n",
      "Train Epoch: 63 ... Batch: 100 ... Loss: 7208.76123047\n",
      "Train Epoch: 63 ... Batch: 110 ... Loss: 7312.08154297\n",
      "Train Epoch: 63 ... Batch: 120 ... Loss: 7387.56738281\n",
      "Train Epoch: 63 ... Batch: 130 ... Loss: 7457.87841797\n",
      "Train Epoch: 63 ... Batch: 140 ... Loss: 7184.14306641\n",
      "Train Epoch: 63 ... Batch: 150 ... Loss: 7547.10937500\n",
      "Train Epoch: 63 ... Batch: 160 ... Loss: 7976.29052734\n",
      "Train Epoch: 63 ... Batch: 170 ... Loss: 7123.32519531\n",
      "Train Epoch: 63 ... Batch: 180 ... Loss: 7363.87988281\n",
      "Train Epoch: 63 ... Batch: 190 ... Loss: 7893.06103516\n",
      "Train Epoch: 63 ... Batch: 200 ... Loss: 7751.12353516\n",
      "------------------- Test set: Average loss: 1218594.0791 ... Samples: 809\n",
      "Train Epoch: 64 ... Batch: 0 ... Loss: 8794.94726562\n",
      "Train Epoch: 64 ... Batch: 10 ... Loss: 8597.36328125\n",
      "Train Epoch: 64 ... Batch: 20 ... Loss: 7613.70800781\n",
      "Train Epoch: 64 ... Batch: 30 ... Loss: 6896.07568359\n",
      "Train Epoch: 64 ... Batch: 40 ... Loss: 8575.11230469\n",
      "Train Epoch: 64 ... Batch: 50 ... Loss: 9308.96875000\n",
      "Train Epoch: 64 ... Batch: 60 ... Loss: 6984.87597656\n",
      "Train Epoch: 64 ... Batch: 70 ... Loss: 7031.21484375\n",
      "Train Epoch: 64 ... Batch: 80 ... Loss: 8901.66699219\n",
      "Train Epoch: 64 ... Batch: 90 ... Loss: 8104.00927734\n",
      "Train Epoch: 64 ... Batch: 100 ... Loss: 7250.88916016\n",
      "Train Epoch: 64 ... Batch: 110 ... Loss: 7028.93457031\n",
      "Train Epoch: 64 ... Batch: 120 ... Loss: 7393.46337891\n",
      "Train Epoch: 64 ... Batch: 130 ... Loss: 7426.69384766\n",
      "Train Epoch: 64 ... Batch: 140 ... Loss: 8810.99707031\n",
      "Train Epoch: 64 ... Batch: 150 ... Loss: 7766.87597656\n",
      "Train Epoch: 64 ... Batch: 160 ... Loss: 7622.54541016\n",
      "Train Epoch: 64 ... Batch: 170 ... Loss: 9182.41796875\n",
      "Train Epoch: 64 ... Batch: 180 ... Loss: 8539.08105469\n",
      "Train Epoch: 64 ... Batch: 190 ... Loss: 8061.43701172\n",
      "Train Epoch: 64 ... Batch: 200 ... Loss: 7911.13769531\n",
      "------------------- Test set: Average loss: 1218592.8344 ... Samples: 809\n",
      "Train Epoch: 65 ... Batch: 0 ... Loss: 8739.88183594\n",
      "Train Epoch: 65 ... Batch: 10 ... Loss: 9059.15722656\n",
      "Train Epoch: 65 ... Batch: 20 ... Loss: 7760.04248047\n",
      "Train Epoch: 65 ... Batch: 30 ... Loss: 7468.66406250\n",
      "Train Epoch: 65 ... Batch: 40 ... Loss: 8109.62841797\n",
      "Train Epoch: 65 ... Batch: 50 ... Loss: 8011.54541016\n",
      "Train Epoch: 65 ... Batch: 60 ... Loss: 8533.10058594\n",
      "Train Epoch: 65 ... Batch: 70 ... Loss: 6464.22607422\n",
      "Train Epoch: 65 ... Batch: 80 ... Loss: 7326.46435547\n",
      "Train Epoch: 65 ... Batch: 90 ... Loss: 8109.20166016\n",
      "Train Epoch: 65 ... Batch: 100 ... Loss: 10452.34375000\n",
      "Train Epoch: 65 ... Batch: 110 ... Loss: 8425.27636719\n",
      "Train Epoch: 65 ... Batch: 120 ... Loss: 8419.94238281\n",
      "Train Epoch: 65 ... Batch: 130 ... Loss: 8719.92089844\n",
      "Train Epoch: 65 ... Batch: 140 ... Loss: 6952.80029297\n",
      "Train Epoch: 65 ... Batch: 150 ... Loss: 8555.10937500\n",
      "Train Epoch: 65 ... Batch: 160 ... Loss: 8324.71093750\n",
      "Train Epoch: 65 ... Batch: 170 ... Loss: 7842.28857422\n",
      "Train Epoch: 65 ... Batch: 180 ... Loss: 8207.78417969\n",
      "Train Epoch: 65 ... Batch: 190 ... Loss: 7701.12646484\n",
      "Train Epoch: 65 ... Batch: 200 ... Loss: 8290.31640625\n",
      "------------------- Test set: Average loss: 1218593.5142 ... Samples: 809\n",
      "Train Epoch: 66 ... Batch: 0 ... Loss: 8511.51562500\n",
      "Train Epoch: 66 ... Batch: 10 ... Loss: 6747.69140625\n",
      "Train Epoch: 66 ... Batch: 20 ... Loss: 8303.10058594\n",
      "Train Epoch: 66 ... Batch: 30 ... Loss: 7950.61718750\n",
      "Train Epoch: 66 ... Batch: 40 ... Loss: 8059.59521484\n",
      "Train Epoch: 66 ... Batch: 50 ... Loss: 7700.30029297\n",
      "Train Epoch: 66 ... Batch: 60 ... Loss: 8111.22802734\n",
      "Train Epoch: 66 ... Batch: 70 ... Loss: 7546.05810547\n",
      "Train Epoch: 66 ... Batch: 80 ... Loss: 8708.74511719\n",
      "Train Epoch: 66 ... Batch: 90 ... Loss: 9199.75097656\n",
      "Train Epoch: 66 ... Batch: 100 ... Loss: 7472.24755859\n",
      "Train Epoch: 66 ... Batch: 110 ... Loss: 7868.85644531\n",
      "Train Epoch: 66 ... Batch: 120 ... Loss: 7331.20019531\n",
      "Train Epoch: 66 ... Batch: 130 ... Loss: 6789.49316406\n",
      "Train Epoch: 66 ... Batch: 140 ... Loss: 7018.39404297\n",
      "Train Epoch: 66 ... Batch: 150 ... Loss: 8564.43750000\n",
      "Train Epoch: 66 ... Batch: 160 ... Loss: 7938.98193359\n",
      "Train Epoch: 66 ... Batch: 170 ... Loss: 6634.08740234\n",
      "Train Epoch: 66 ... Batch: 180 ... Loss: 8042.72363281\n",
      "Train Epoch: 66 ... Batch: 190 ... Loss: 7365.25781250\n",
      "Train Epoch: 66 ... Batch: 200 ... Loss: 7356.24707031\n",
      "------------------- Test set: Average loss: 1218592.5488 ... Samples: 809\n",
      "Train Epoch: 67 ... Batch: 0 ... Loss: 7708.77832031\n",
      "Train Epoch: 67 ... Batch: 10 ... Loss: 9580.36132812\n",
      "Train Epoch: 67 ... Batch: 20 ... Loss: 6841.22265625\n",
      "Train Epoch: 67 ... Batch: 30 ... Loss: 7881.20947266\n",
      "Train Epoch: 67 ... Batch: 40 ... Loss: 7376.92675781\n",
      "Train Epoch: 67 ... Batch: 50 ... Loss: 7438.85107422\n",
      "Train Epoch: 67 ... Batch: 60 ... Loss: 6844.71093750\n",
      "Train Epoch: 67 ... Batch: 70 ... Loss: 7718.49755859\n",
      "Train Epoch: 67 ... Batch: 80 ... Loss: 8038.92041016\n",
      "Train Epoch: 67 ... Batch: 90 ... Loss: 9243.50585938\n",
      "Train Epoch: 67 ... Batch: 100 ... Loss: 7882.33154297\n",
      "Train Epoch: 67 ... Batch: 110 ... Loss: 6687.83935547\n",
      "Train Epoch: 67 ... Batch: 120 ... Loss: 9364.24902344\n",
      "Train Epoch: 67 ... Batch: 130 ... Loss: 7646.66894531\n",
      "Train Epoch: 67 ... Batch: 140 ... Loss: 7664.89550781\n",
      "Train Epoch: 67 ... Batch: 150 ... Loss: 7067.66796875\n",
      "Train Epoch: 67 ... Batch: 160 ... Loss: 7794.11083984\n",
      "Train Epoch: 67 ... Batch: 170 ... Loss: 7257.39990234\n",
      "Train Epoch: 67 ... Batch: 180 ... Loss: 7574.43603516\n",
      "Train Epoch: 67 ... Batch: 190 ... Loss: 7513.98583984\n",
      "Train Epoch: 67 ... Batch: 200 ... Loss: 8297.89160156\n",
      "------------------- Test set: Average loss: 1218590.3956 ... Samples: 809\n",
      "Train Epoch: 68 ... Batch: 0 ... Loss: 6907.11035156\n",
      "Train Epoch: 68 ... Batch: 10 ... Loss: 8358.11425781\n",
      "Train Epoch: 68 ... Batch: 20 ... Loss: 8051.91259766\n",
      "Train Epoch: 68 ... Batch: 30 ... Loss: 8009.50000000\n",
      "Train Epoch: 68 ... Batch: 40 ... Loss: 9513.80761719\n",
      "Train Epoch: 68 ... Batch: 50 ... Loss: 8897.11914062\n",
      "Train Epoch: 68 ... Batch: 60 ... Loss: 8126.21093750\n",
      "Train Epoch: 68 ... Batch: 70 ... Loss: 7617.31347656\n",
      "Train Epoch: 68 ... Batch: 80 ... Loss: 7840.07568359\n",
      "Train Epoch: 68 ... Batch: 90 ... Loss: 7502.62109375\n",
      "Train Epoch: 68 ... Batch: 100 ... Loss: 9589.38183594\n",
      "Train Epoch: 68 ... Batch: 110 ... Loss: 8719.16308594\n",
      "Train Epoch: 68 ... Batch: 120 ... Loss: 7872.00146484\n",
      "Train Epoch: 68 ... Batch: 130 ... Loss: 9095.47070312\n",
      "Train Epoch: 68 ... Batch: 140 ... Loss: 7966.50390625\n",
      "Train Epoch: 68 ... Batch: 150 ... Loss: 7037.79931641\n",
      "Train Epoch: 68 ... Batch: 160 ... Loss: 6644.90478516\n",
      "Train Epoch: 68 ... Batch: 170 ... Loss: 6885.84765625\n",
      "Train Epoch: 68 ... Batch: 180 ... Loss: 7896.42822266\n",
      "Train Epoch: 68 ... Batch: 190 ... Loss: 9207.08105469\n",
      "Train Epoch: 68 ... Batch: 200 ... Loss: 7453.99462891\n",
      "------------------- Test set: Average loss: 1218590.2274 ... Samples: 809\n",
      "Train Epoch: 69 ... Batch: 0 ... Loss: 7350.79785156\n",
      "Train Epoch: 69 ... Batch: 10 ... Loss: 7143.37500000\n",
      "Train Epoch: 69 ... Batch: 20 ... Loss: 7501.10498047\n",
      "Train Epoch: 69 ... Batch: 30 ... Loss: 8589.47656250\n",
      "Train Epoch: 69 ... Batch: 40 ... Loss: 7968.19921875\n",
      "Train Epoch: 69 ... Batch: 50 ... Loss: 7676.94091797\n",
      "Train Epoch: 69 ... Batch: 60 ... Loss: 8999.09765625\n",
      "Train Epoch: 69 ... Batch: 70 ... Loss: 7321.51904297\n",
      "Train Epoch: 69 ... Batch: 80 ... Loss: 8057.86962891\n",
      "Train Epoch: 69 ... Batch: 90 ... Loss: 9167.56250000\n",
      "Train Epoch: 69 ... Batch: 100 ... Loss: 7749.02978516\n",
      "Train Epoch: 69 ... Batch: 110 ... Loss: 7836.69091797\n",
      "Train Epoch: 69 ... Batch: 120 ... Loss: 9127.08496094\n",
      "Train Epoch: 69 ... Batch: 130 ... Loss: 7875.19531250\n",
      "Train Epoch: 69 ... Batch: 140 ... Loss: 7122.67675781\n",
      "Train Epoch: 69 ... Batch: 150 ... Loss: 8360.30175781\n",
      "Train Epoch: 69 ... Batch: 160 ... Loss: 7066.15332031\n",
      "Train Epoch: 69 ... Batch: 170 ... Loss: 7690.63378906\n",
      "Train Epoch: 69 ... Batch: 180 ... Loss: 8227.80664062\n",
      "Train Epoch: 69 ... Batch: 190 ... Loss: 9032.62500000\n",
      "Train Epoch: 69 ... Batch: 200 ... Loss: 7265.44238281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Test set: Average loss: 1218589.2880 ... Samples: 809\n",
      "Train Epoch: 70 ... Batch: 0 ... Loss: 7802.03125000\n",
      "Train Epoch: 70 ... Batch: 10 ... Loss: 7526.74169922\n",
      "Train Epoch: 70 ... Batch: 20 ... Loss: 7311.58349609\n",
      "Train Epoch: 70 ... Batch: 30 ... Loss: 7208.10693359\n",
      "Train Epoch: 70 ... Batch: 40 ... Loss: 6754.04052734\n",
      "Train Epoch: 70 ... Batch: 50 ... Loss: 8392.19824219\n",
      "Train Epoch: 70 ... Batch: 60 ... Loss: 6613.04687500\n",
      "Train Epoch: 70 ... Batch: 70 ... Loss: 7507.75341797\n",
      "Train Epoch: 70 ... Batch: 80 ... Loss: 8323.35449219\n",
      "Train Epoch: 70 ... Batch: 90 ... Loss: 8017.76562500\n",
      "Train Epoch: 70 ... Batch: 100 ... Loss: 6174.18505859\n",
      "Train Epoch: 70 ... Batch: 110 ... Loss: 8024.10253906\n",
      "Train Epoch: 70 ... Batch: 120 ... Loss: 8914.28613281\n",
      "Train Epoch: 70 ... Batch: 130 ... Loss: 8829.53320312\n",
      "Train Epoch: 70 ... Batch: 140 ... Loss: 7691.30468750\n",
      "Train Epoch: 70 ... Batch: 150 ... Loss: 8533.78906250\n",
      "Train Epoch: 70 ... Batch: 160 ... Loss: 7365.09765625\n",
      "Train Epoch: 70 ... Batch: 170 ... Loss: 7071.22021484\n",
      "Train Epoch: 70 ... Batch: 180 ... Loss: 7448.79052734\n",
      "Train Epoch: 70 ... Batch: 190 ... Loss: 7326.01708984\n",
      "Train Epoch: 70 ... Batch: 200 ... Loss: 8023.72802734\n",
      "------------------- Test set: Average loss: 1218590.2744 ... Samples: 809\n",
      "Train Epoch: 71 ... Batch: 0 ... Loss: 7668.71826172\n",
      "Train Epoch: 71 ... Batch: 10 ... Loss: 8811.68457031\n",
      "Train Epoch: 71 ... Batch: 20 ... Loss: 7787.92187500\n",
      "Train Epoch: 71 ... Batch: 30 ... Loss: 7587.16796875\n",
      "Train Epoch: 71 ... Batch: 40 ... Loss: 7577.59863281\n",
      "Train Epoch: 71 ... Batch: 50 ... Loss: 8260.03613281\n",
      "Train Epoch: 71 ... Batch: 60 ... Loss: 7559.84863281\n",
      "Train Epoch: 71 ... Batch: 70 ... Loss: 8140.67578125\n",
      "Train Epoch: 71 ... Batch: 80 ... Loss: 7608.38525391\n",
      "Train Epoch: 71 ... Batch: 90 ... Loss: 7101.57373047\n",
      "Train Epoch: 71 ... Batch: 100 ... Loss: 7378.93359375\n",
      "Train Epoch: 71 ... Batch: 110 ... Loss: 9430.99511719\n",
      "Train Epoch: 71 ... Batch: 120 ... Loss: 7291.47119141\n",
      "Train Epoch: 71 ... Batch: 130 ... Loss: 8763.87597656\n",
      "Train Epoch: 71 ... Batch: 140 ... Loss: 7086.97216797\n",
      "Train Epoch: 71 ... Batch: 150 ... Loss: 9021.84472656\n",
      "Train Epoch: 71 ... Batch: 160 ... Loss: 9088.91699219\n",
      "Train Epoch: 71 ... Batch: 170 ... Loss: 8531.52050781\n",
      "Train Epoch: 71 ... Batch: 180 ... Loss: 6972.21728516\n",
      "Train Epoch: 71 ... Batch: 190 ... Loss: 8017.81494141\n",
      "Train Epoch: 71 ... Batch: 200 ... Loss: 8363.71093750\n",
      "------------------- Test set: Average loss: 1218592.3560 ... Samples: 809\n",
      "Train Epoch: 72 ... Batch: 0 ... Loss: 8808.15917969\n",
      "Train Epoch: 72 ... Batch: 10 ... Loss: 7941.58154297\n",
      "Train Epoch: 72 ... Batch: 20 ... Loss: 8728.96093750\n",
      "Train Epoch: 72 ... Batch: 30 ... Loss: 7993.15087891\n",
      "Train Epoch: 72 ... Batch: 40 ... Loss: 8295.58007812\n",
      "Train Epoch: 72 ... Batch: 50 ... Loss: 7067.67822266\n",
      "Train Epoch: 72 ... Batch: 60 ... Loss: 8687.26074219\n",
      "Train Epoch: 72 ... Batch: 70 ... Loss: 8325.95214844\n",
      "Train Epoch: 72 ... Batch: 80 ... Loss: 7246.42187500\n",
      "Train Epoch: 72 ... Batch: 90 ... Loss: 8753.21582031\n",
      "Train Epoch: 72 ... Batch: 100 ... Loss: 8198.21777344\n",
      "Train Epoch: 72 ... Batch: 110 ... Loss: 7756.17285156\n",
      "Train Epoch: 72 ... Batch: 120 ... Loss: 10568.15917969\n",
      "Train Epoch: 72 ... Batch: 130 ... Loss: 7848.67041016\n",
      "Train Epoch: 72 ... Batch: 140 ... Loss: 6937.70458984\n",
      "Train Epoch: 72 ... Batch: 150 ... Loss: 8300.33886719\n",
      "Train Epoch: 72 ... Batch: 160 ... Loss: 7982.68212891\n",
      "Train Epoch: 72 ... Batch: 170 ... Loss: 7989.58935547\n",
      "Train Epoch: 72 ... Batch: 180 ... Loss: 7889.79443359\n",
      "Train Epoch: 72 ... Batch: 190 ... Loss: 7752.90869141\n",
      "Train Epoch: 72 ... Batch: 200 ... Loss: 7316.88427734\n",
      "------------------- Test set: Average loss: 1218590.0878 ... Samples: 809\n",
      "Train Epoch: 73 ... Batch: 0 ... Loss: 7396.87500000\n",
      "Train Epoch: 73 ... Batch: 10 ... Loss: 8481.63476562\n",
      "Train Epoch: 73 ... Batch: 20 ... Loss: 7619.82519531\n",
      "Train Epoch: 73 ... Batch: 30 ... Loss: 6898.56738281\n",
      "Train Epoch: 73 ... Batch: 40 ... Loss: 8803.01855469\n",
      "Train Epoch: 73 ... Batch: 50 ... Loss: 7669.60937500\n",
      "Train Epoch: 73 ... Batch: 60 ... Loss: 8946.04199219\n",
      "Train Epoch: 73 ... Batch: 70 ... Loss: 7906.79541016\n",
      "Train Epoch: 73 ... Batch: 80 ... Loss: 7067.65332031\n",
      "Train Epoch: 73 ... Batch: 90 ... Loss: 7854.27343750\n",
      "Train Epoch: 73 ... Batch: 100 ... Loss: 7782.55175781\n",
      "Train Epoch: 73 ... Batch: 110 ... Loss: 8998.03125000\n",
      "Train Epoch: 73 ... Batch: 120 ... Loss: 8426.83105469\n",
      "Train Epoch: 73 ... Batch: 130 ... Loss: 8236.33300781\n",
      "Train Epoch: 73 ... Batch: 140 ... Loss: 7138.86865234\n",
      "Train Epoch: 73 ... Batch: 150 ... Loss: 7681.82031250\n",
      "Train Epoch: 73 ... Batch: 160 ... Loss: 7969.93115234\n",
      "Train Epoch: 73 ... Batch: 170 ... Loss: 8040.92041016\n",
      "Train Epoch: 73 ... Batch: 180 ... Loss: 7577.25000000\n",
      "Train Epoch: 73 ... Batch: 190 ... Loss: 7712.79052734\n",
      "Train Epoch: 73 ... Batch: 200 ... Loss: 7761.25390625\n",
      "------------------- Test set: Average loss: 1218591.6885 ... Samples: 809\n",
      "Train Epoch: 74 ... Batch: 0 ... Loss: 8015.10644531\n",
      "Train Epoch: 74 ... Batch: 10 ... Loss: 9469.96386719\n",
      "Train Epoch: 74 ... Batch: 20 ... Loss: 7967.88427734\n",
      "Train Epoch: 74 ... Batch: 30 ... Loss: 8065.31005859\n",
      "Train Epoch: 74 ... Batch: 40 ... Loss: 8367.36132812\n",
      "Train Epoch: 74 ... Batch: 50 ... Loss: 8964.40527344\n",
      "Train Epoch: 74 ... Batch: 60 ... Loss: 9023.77148438\n",
      "Train Epoch: 74 ... Batch: 70 ... Loss: 8263.71386719\n",
      "Train Epoch: 74 ... Batch: 80 ... Loss: 8472.19140625\n",
      "Train Epoch: 74 ... Batch: 90 ... Loss: 7257.02832031\n",
      "Train Epoch: 74 ... Batch: 100 ... Loss: 8732.62402344\n",
      "Train Epoch: 74 ... Batch: 110 ... Loss: 7858.74072266\n",
      "Train Epoch: 74 ... Batch: 120 ... Loss: 7716.90332031\n",
      "Train Epoch: 74 ... Batch: 130 ... Loss: 8107.13281250\n",
      "Train Epoch: 74 ... Batch: 140 ... Loss: 7306.67333984\n",
      "Train Epoch: 74 ... Batch: 150 ... Loss: 8173.28076172\n",
      "Train Epoch: 74 ... Batch: 160 ... Loss: 7088.42041016\n",
      "Train Epoch: 74 ... Batch: 170 ... Loss: 8060.78906250\n",
      "Train Epoch: 74 ... Batch: 180 ... Loss: 8215.49707031\n",
      "Train Epoch: 74 ... Batch: 190 ... Loss: 7536.45849609\n",
      "Train Epoch: 74 ... Batch: 200 ... Loss: 6614.72998047\n",
      "------------------- Test set: Average loss: 1218589.1471 ... Samples: 809\n",
      "Train Epoch: 75 ... Batch: 0 ... Loss: 7970.87988281\n",
      "Train Epoch: 75 ... Batch: 10 ... Loss: 9015.11132812\n",
      "Train Epoch: 75 ... Batch: 20 ... Loss: 8739.11425781\n",
      "Train Epoch: 75 ... Batch: 30 ... Loss: 7061.37207031\n",
      "Train Epoch: 75 ... Batch: 40 ... Loss: 7619.90869141\n",
      "Train Epoch: 75 ... Batch: 50 ... Loss: 7554.21435547\n",
      "Train Epoch: 75 ... Batch: 60 ... Loss: 7523.98974609\n",
      "Train Epoch: 75 ... Batch: 70 ... Loss: 7052.00732422\n",
      "Train Epoch: 75 ... Batch: 80 ... Loss: 7998.22998047\n",
      "Train Epoch: 75 ... Batch: 90 ... Loss: 8431.33105469\n",
      "Train Epoch: 75 ... Batch: 100 ... Loss: 8090.27050781\n",
      "Train Epoch: 75 ... Batch: 110 ... Loss: 6854.02050781\n",
      "Train Epoch: 75 ... Batch: 120 ... Loss: 7431.12353516\n",
      "Train Epoch: 75 ... Batch: 130 ... Loss: 9097.06738281\n",
      "Train Epoch: 75 ... Batch: 140 ... Loss: 6283.98925781\n",
      "Train Epoch: 75 ... Batch: 150 ... Loss: 9026.06738281\n",
      "Train Epoch: 75 ... Batch: 160 ... Loss: 6515.09082031\n",
      "Train Epoch: 75 ... Batch: 170 ... Loss: 7780.25146484\n",
      "Train Epoch: 75 ... Batch: 180 ... Loss: 7904.29394531\n",
      "Train Epoch: 75 ... Batch: 190 ... Loss: 8291.29687500\n",
      "Train Epoch: 75 ... Batch: 200 ... Loss: 7073.32519531\n",
      "------------------- Test set: Average loss: 1218590.5958 ... Samples: 809\n",
      "Train Epoch: 76 ... Batch: 0 ... Loss: 7572.09863281\n",
      "Train Epoch: 76 ... Batch: 10 ... Loss: 7527.07910156\n",
      "Train Epoch: 76 ... Batch: 20 ... Loss: 9204.89843750\n",
      "Train Epoch: 76 ... Batch: 30 ... Loss: 8073.81787109\n",
      "Train Epoch: 76 ... Batch: 40 ... Loss: 8018.39697266\n",
      "Train Epoch: 76 ... Batch: 50 ... Loss: 7719.92333984\n",
      "Train Epoch: 76 ... Batch: 60 ... Loss: 8387.85058594\n",
      "Train Epoch: 76 ... Batch: 70 ... Loss: 8289.93164062\n",
      "Train Epoch: 76 ... Batch: 80 ... Loss: 8122.20312500\n",
      "Train Epoch: 76 ... Batch: 90 ... Loss: 7818.73437500\n",
      "Train Epoch: 76 ... Batch: 100 ... Loss: 7218.72412109\n",
      "Train Epoch: 76 ... Batch: 110 ... Loss: 7602.01416016\n",
      "Train Epoch: 76 ... Batch: 120 ... Loss: 7379.60107422\n",
      "Train Epoch: 76 ... Batch: 130 ... Loss: 8102.40966797\n",
      "Train Epoch: 76 ... Batch: 140 ... Loss: 7130.94091797\n",
      "Train Epoch: 76 ... Batch: 150 ... Loss: 7870.12500000\n",
      "Train Epoch: 76 ... Batch: 160 ... Loss: 7935.29248047\n",
      "Train Epoch: 76 ... Batch: 170 ... Loss: 8145.12060547\n",
      "Train Epoch: 76 ... Batch: 180 ... Loss: 8068.80468750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 76 ... Batch: 190 ... Loss: 7140.09375000\n",
      "Train Epoch: 76 ... Batch: 200 ... Loss: 8166.90478516\n",
      "------------------- Test set: Average loss: 1218589.9172 ... Samples: 809\n",
      "Train Epoch: 77 ... Batch: 0 ... Loss: 7244.70019531\n",
      "Train Epoch: 77 ... Batch: 10 ... Loss: 7309.28906250\n",
      "Train Epoch: 77 ... Batch: 20 ... Loss: 6587.58203125\n",
      "Train Epoch: 77 ... Batch: 30 ... Loss: 7585.24072266\n",
      "Train Epoch: 77 ... Batch: 40 ... Loss: 8999.35351562\n",
      "Train Epoch: 77 ... Batch: 50 ... Loss: 8001.02343750\n",
      "Train Epoch: 77 ... Batch: 60 ... Loss: 7147.15332031\n",
      "Train Epoch: 77 ... Batch: 70 ... Loss: 7775.35009766\n",
      "Train Epoch: 77 ... Batch: 80 ... Loss: 6612.90185547\n",
      "Train Epoch: 77 ... Batch: 90 ... Loss: 7846.79687500\n",
      "Train Epoch: 77 ... Batch: 100 ... Loss: 7583.17285156\n",
      "Train Epoch: 77 ... Batch: 110 ... Loss: 7700.50488281\n",
      "Train Epoch: 77 ... Batch: 120 ... Loss: 6741.53271484\n",
      "Train Epoch: 77 ... Batch: 130 ... Loss: 8041.05810547\n",
      "Train Epoch: 77 ... Batch: 140 ... Loss: 8387.77636719\n",
      "Train Epoch: 77 ... Batch: 150 ... Loss: 8472.90136719\n",
      "Train Epoch: 77 ... Batch: 160 ... Loss: 8598.21386719\n",
      "Train Epoch: 77 ... Batch: 170 ... Loss: 7609.88818359\n",
      "Train Epoch: 77 ... Batch: 180 ... Loss: 8118.93603516\n",
      "Train Epoch: 77 ... Batch: 190 ... Loss: 7466.44091797\n",
      "Train Epoch: 77 ... Batch: 200 ... Loss: 7703.05029297\n",
      "------------------- Test set: Average loss: 1218589.0470 ... Samples: 809\n",
      "Train Epoch: 78 ... Batch: 0 ... Loss: 7516.27197266\n",
      "Train Epoch: 78 ... Batch: 10 ... Loss: 7427.35937500\n",
      "Train Epoch: 78 ... Batch: 20 ... Loss: 8121.59765625\n",
      "Train Epoch: 78 ... Batch: 30 ... Loss: 8377.67480469\n",
      "Train Epoch: 78 ... Batch: 40 ... Loss: 7249.17041016\n",
      "Train Epoch: 78 ... Batch: 50 ... Loss: 6352.08203125\n",
      "Train Epoch: 78 ... Batch: 60 ... Loss: 8178.02197266\n",
      "Train Epoch: 78 ... Batch: 70 ... Loss: 7955.40087891\n",
      "Train Epoch: 78 ... Batch: 80 ... Loss: 7680.46728516\n",
      "Train Epoch: 78 ... Batch: 90 ... Loss: 7458.46875000\n",
      "Train Epoch: 78 ... Batch: 100 ... Loss: 7515.14013672\n",
      "Train Epoch: 78 ... Batch: 110 ... Loss: 8668.87011719\n",
      "Train Epoch: 78 ... Batch: 120 ... Loss: 9605.74902344\n",
      "Train Epoch: 78 ... Batch: 130 ... Loss: 7931.40771484\n",
      "Train Epoch: 78 ... Batch: 140 ... Loss: 8677.46875000\n",
      "Train Epoch: 78 ... Batch: 150 ... Loss: 7142.69482422\n",
      "Train Epoch: 78 ... Batch: 160 ... Loss: 8307.40917969\n",
      "Train Epoch: 78 ... Batch: 170 ... Loss: 7398.60791016\n",
      "Train Epoch: 78 ... Batch: 180 ... Loss: 7946.06396484\n",
      "Train Epoch: 78 ... Batch: 190 ... Loss: 7290.06787109\n",
      "Train Epoch: 78 ... Batch: 200 ... Loss: 7874.54052734\n",
      "------------------- Test set: Average loss: 1218589.0247 ... Samples: 809\n",
      "Train Epoch: 79 ... Batch: 0 ... Loss: 8123.69384766\n",
      "Train Epoch: 79 ... Batch: 10 ... Loss: 6980.95312500\n",
      "Train Epoch: 79 ... Batch: 20 ... Loss: 7127.21337891\n",
      "Train Epoch: 79 ... Batch: 30 ... Loss: 9683.24511719\n",
      "Train Epoch: 79 ... Batch: 40 ... Loss: 8018.43750000\n",
      "Train Epoch: 79 ... Batch: 50 ... Loss: 7576.04052734\n",
      "Train Epoch: 79 ... Batch: 60 ... Loss: 8707.25976562\n",
      "Train Epoch: 79 ... Batch: 70 ... Loss: 8791.13476562\n",
      "Train Epoch: 79 ... Batch: 80 ... Loss: 9040.86914062\n",
      "Train Epoch: 79 ... Batch: 90 ... Loss: 7602.43603516\n",
      "Train Epoch: 79 ... Batch: 100 ... Loss: 8116.75488281\n",
      "Train Epoch: 79 ... Batch: 110 ... Loss: 7900.69091797\n",
      "Train Epoch: 79 ... Batch: 120 ... Loss: 7643.33935547\n",
      "Train Epoch: 79 ... Batch: 130 ... Loss: 7292.29052734\n",
      "Train Epoch: 79 ... Batch: 140 ... Loss: 7419.06396484\n",
      "Train Epoch: 79 ... Batch: 150 ... Loss: 9592.62890625\n",
      "Train Epoch: 79 ... Batch: 160 ... Loss: 7305.59521484\n",
      "Train Epoch: 79 ... Batch: 170 ... Loss: 9284.92187500\n",
      "Train Epoch: 79 ... Batch: 180 ... Loss: 6427.56591797\n",
      "Train Epoch: 79 ... Batch: 190 ... Loss: 7310.56396484\n",
      "Train Epoch: 79 ... Batch: 200 ... Loss: 6431.17138672\n",
      "------------------- Test set: Average loss: 1218589.7033 ... Samples: 809\n",
      "Train Epoch: 80 ... Batch: 0 ... Loss: 8489.88574219\n",
      "Train Epoch: 80 ... Batch: 10 ... Loss: 8288.18945312\n",
      "Train Epoch: 80 ... Batch: 20 ... Loss: 8301.33300781\n",
      "Train Epoch: 80 ... Batch: 30 ... Loss: 7503.07958984\n",
      "Train Epoch: 80 ... Batch: 40 ... Loss: 8279.10839844\n",
      "Train Epoch: 80 ... Batch: 50 ... Loss: 6604.47656250\n",
      "Train Epoch: 80 ... Batch: 60 ... Loss: 8178.23681641\n",
      "Train Epoch: 80 ... Batch: 70 ... Loss: 9622.06445312\n",
      "Train Epoch: 80 ... Batch: 80 ... Loss: 8416.91699219\n",
      "Train Epoch: 80 ... Batch: 90 ... Loss: 8206.44042969\n",
      "Train Epoch: 80 ... Batch: 100 ... Loss: 7243.26269531\n",
      "Train Epoch: 80 ... Batch: 110 ... Loss: 7684.23925781\n",
      "Train Epoch: 80 ... Batch: 120 ... Loss: 7807.15380859\n",
      "Train Epoch: 80 ... Batch: 130 ... Loss: 8012.82031250\n",
      "Train Epoch: 80 ... Batch: 140 ... Loss: 7866.47265625\n",
      "Train Epoch: 80 ... Batch: 150 ... Loss: 7743.95166016\n",
      "Train Epoch: 80 ... Batch: 160 ... Loss: 8760.76269531\n",
      "Train Epoch: 80 ... Batch: 170 ... Loss: 6932.57958984\n",
      "Train Epoch: 80 ... Batch: 180 ... Loss: 7749.35009766\n",
      "Train Epoch: 80 ... Batch: 190 ... Loss: 8390.00683594\n",
      "Train Epoch: 80 ... Batch: 200 ... Loss: 7811.30029297\n",
      "------------------- Test set: Average loss: 1218588.0939 ... Samples: 809\n",
      "Train Epoch: 81 ... Batch: 0 ... Loss: 7315.11962891\n",
      "Train Epoch: 81 ... Batch: 10 ... Loss: 7936.45458984\n",
      "Train Epoch: 81 ... Batch: 20 ... Loss: 7957.74560547\n",
      "Train Epoch: 81 ... Batch: 30 ... Loss: 6611.02587891\n",
      "Train Epoch: 81 ... Batch: 40 ... Loss: 8414.64746094\n",
      "Train Epoch: 81 ... Batch: 50 ... Loss: 9081.72851562\n",
      "Train Epoch: 81 ... Batch: 60 ... Loss: 7685.85009766\n",
      "Train Epoch: 81 ... Batch: 70 ... Loss: 7843.70849609\n",
      "Train Epoch: 81 ... Batch: 80 ... Loss: 8617.33984375\n",
      "Train Epoch: 81 ... Batch: 90 ... Loss: 8529.78320312\n",
      "Train Epoch: 81 ... Batch: 100 ... Loss: 8060.98681641\n",
      "Train Epoch: 81 ... Batch: 110 ... Loss: 7130.10644531\n",
      "Train Epoch: 81 ... Batch: 120 ... Loss: 7226.54248047\n",
      "Train Epoch: 81 ... Batch: 130 ... Loss: 6890.62109375\n",
      "Train Epoch: 81 ... Batch: 140 ... Loss: 8699.46289062\n",
      "Train Epoch: 81 ... Batch: 150 ... Loss: 7610.71337891\n",
      "Train Epoch: 81 ... Batch: 160 ... Loss: 7345.57666016\n",
      "Train Epoch: 81 ... Batch: 170 ... Loss: 8917.13281250\n",
      "Train Epoch: 81 ... Batch: 180 ... Loss: 7818.73681641\n",
      "Train Epoch: 81 ... Batch: 190 ... Loss: 8094.60302734\n",
      "Train Epoch: 81 ... Batch: 200 ... Loss: 8300.96093750\n",
      "------------------- Test set: Average loss: 1218591.2040 ... Samples: 809\n",
      "Train Epoch: 82 ... Batch: 0 ... Loss: 7408.00000000\n",
      "Train Epoch: 82 ... Batch: 10 ... Loss: 8027.50781250\n",
      "Train Epoch: 82 ... Batch: 20 ... Loss: 7195.58935547\n",
      "Train Epoch: 82 ... Batch: 30 ... Loss: 7690.92968750\n",
      "Train Epoch: 82 ... Batch: 40 ... Loss: 8313.80078125\n",
      "Train Epoch: 82 ... Batch: 50 ... Loss: 8933.74707031\n",
      "Train Epoch: 82 ... Batch: 60 ... Loss: 8473.98828125\n",
      "Train Epoch: 82 ... Batch: 70 ... Loss: 7463.96582031\n",
      "Train Epoch: 82 ... Batch: 80 ... Loss: 9607.72070312\n",
      "Train Epoch: 82 ... Batch: 90 ... Loss: 8145.07177734\n",
      "Train Epoch: 82 ... Batch: 100 ... Loss: 8325.11523438\n",
      "Train Epoch: 82 ... Batch: 110 ... Loss: 7892.28613281\n",
      "Train Epoch: 82 ... Batch: 120 ... Loss: 7596.06347656\n",
      "Train Epoch: 82 ... Batch: 130 ... Loss: 7146.88037109\n",
      "Train Epoch: 82 ... Batch: 140 ... Loss: 7670.86572266\n",
      "Train Epoch: 82 ... Batch: 150 ... Loss: 8604.86718750\n",
      "Train Epoch: 82 ... Batch: 160 ... Loss: 7682.33056641\n",
      "Train Epoch: 82 ... Batch: 170 ... Loss: 6685.18115234\n",
      "Train Epoch: 82 ... Batch: 180 ... Loss: 7072.83935547\n",
      "Train Epoch: 82 ... Batch: 190 ... Loss: 7891.64062500\n",
      "Train Epoch: 82 ... Batch: 200 ... Loss: 7175.82812500\n",
      "------------------- Test set: Average loss: 1218588.2460 ... Samples: 809\n",
      "Train Epoch: 83 ... Batch: 0 ... Loss: 7840.41796875\n",
      "Train Epoch: 83 ... Batch: 10 ... Loss: 7912.16650391\n",
      "Train Epoch: 83 ... Batch: 20 ... Loss: 8208.39550781\n",
      "Train Epoch: 83 ... Batch: 30 ... Loss: 7168.47021484\n",
      "Train Epoch: 83 ... Batch: 40 ... Loss: 7318.08154297\n",
      "Train Epoch: 83 ... Batch: 50 ... Loss: 7896.04394531\n",
      "Train Epoch: 83 ... Batch: 60 ... Loss: 7416.35253906\n",
      "Train Epoch: 83 ... Batch: 70 ... Loss: 7700.10009766\n",
      "Train Epoch: 83 ... Batch: 80 ... Loss: 9387.10937500\n",
      "Train Epoch: 83 ... Batch: 90 ... Loss: 7204.50537109\n",
      "Train Epoch: 83 ... Batch: 100 ... Loss: 6503.11572266\n",
      "Train Epoch: 83 ... Batch: 110 ... Loss: 7638.37207031\n",
      "Train Epoch: 83 ... Batch: 120 ... Loss: 8508.75683594\n",
      "Train Epoch: 83 ... Batch: 130 ... Loss: 8363.52539062\n",
      "Train Epoch: 83 ... Batch: 140 ... Loss: 7793.18896484\n",
      "Train Epoch: 83 ... Batch: 150 ... Loss: 7785.40576172\n",
      "Train Epoch: 83 ... Batch: 160 ... Loss: 7430.80810547\n",
      "Train Epoch: 83 ... Batch: 170 ... Loss: 8086.16748047\n",
      "Train Epoch: 83 ... Batch: 180 ... Loss: 8315.32031250\n",
      "Train Epoch: 83 ... Batch: 190 ... Loss: 7552.81005859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 83 ... Batch: 200 ... Loss: 6941.96582031\n",
      "------------------- Test set: Average loss: 1218590.6069 ... Samples: 809\n",
      "Train Epoch: 84 ... Batch: 0 ... Loss: 7861.55029297\n",
      "Train Epoch: 84 ... Batch: 10 ... Loss: 7447.61425781\n",
      "Train Epoch: 84 ... Batch: 20 ... Loss: 8089.04248047\n",
      "Train Epoch: 84 ... Batch: 30 ... Loss: 7989.55957031\n",
      "Train Epoch: 84 ... Batch: 40 ... Loss: 7309.88134766\n",
      "Train Epoch: 84 ... Batch: 50 ... Loss: 7462.34716797\n",
      "Train Epoch: 84 ... Batch: 60 ... Loss: 7143.26318359\n",
      "Train Epoch: 84 ... Batch: 70 ... Loss: 8801.14550781\n",
      "Train Epoch: 84 ... Batch: 80 ... Loss: 8419.05273438\n",
      "Train Epoch: 84 ... Batch: 90 ... Loss: 8169.89306641\n",
      "Train Epoch: 84 ... Batch: 100 ... Loss: 9148.53613281\n",
      "Train Epoch: 84 ... Batch: 110 ... Loss: 8688.53710938\n",
      "Train Epoch: 84 ... Batch: 120 ... Loss: 7855.86083984\n",
      "Train Epoch: 84 ... Batch: 130 ... Loss: 8083.97363281\n",
      "Train Epoch: 84 ... Batch: 140 ... Loss: 6728.73779297\n",
      "Train Epoch: 84 ... Batch: 150 ... Loss: 8012.22656250\n",
      "Train Epoch: 84 ... Batch: 160 ... Loss: 8431.29882812\n",
      "Train Epoch: 84 ... Batch: 170 ... Loss: 9259.21093750\n",
      "Train Epoch: 84 ... Batch: 180 ... Loss: 7919.78613281\n",
      "Train Epoch: 84 ... Batch: 190 ... Loss: 7263.32421875\n",
      "Train Epoch: 84 ... Batch: 200 ... Loss: 7357.86083984\n",
      "------------------- Test set: Average loss: 1218588.5995 ... Samples: 809\n",
      "Train Epoch: 85 ... Batch: 0 ... Loss: 7210.84228516\n",
      "Train Epoch: 85 ... Batch: 10 ... Loss: 8301.27636719\n",
      "Train Epoch: 85 ... Batch: 20 ... Loss: 8988.40625000\n",
      "Train Epoch: 85 ... Batch: 30 ... Loss: 8492.10058594\n",
      "Train Epoch: 85 ... Batch: 40 ... Loss: 7399.91650391\n",
      "Train Epoch: 85 ... Batch: 50 ... Loss: 8330.38476562\n",
      "Train Epoch: 85 ... Batch: 60 ... Loss: 8359.49902344\n",
      "Train Epoch: 85 ... Batch: 70 ... Loss: 7715.18505859\n",
      "Train Epoch: 85 ... Batch: 80 ... Loss: 6793.69531250\n",
      "Train Epoch: 85 ... Batch: 90 ... Loss: 7786.61669922\n",
      "Train Epoch: 85 ... Batch: 100 ... Loss: 7290.83691406\n",
      "Train Epoch: 85 ... Batch: 110 ... Loss: 8273.42089844\n",
      "Train Epoch: 85 ... Batch: 120 ... Loss: 7400.30224609\n",
      "Train Epoch: 85 ... Batch: 130 ... Loss: 7358.06250000\n",
      "Train Epoch: 85 ... Batch: 140 ... Loss: 8427.06933594\n",
      "Train Epoch: 85 ... Batch: 150 ... Loss: 7707.75488281\n",
      "Train Epoch: 85 ... Batch: 160 ... Loss: 7565.09130859\n",
      "Train Epoch: 85 ... Batch: 170 ... Loss: 8201.59277344\n",
      "Train Epoch: 85 ... Batch: 180 ... Loss: 8703.23632812\n",
      "Train Epoch: 85 ... Batch: 190 ... Loss: 7249.09228516\n",
      "Train Epoch: 85 ... Batch: 200 ... Loss: 8005.97753906\n",
      "------------------- Test set: Average loss: 1218588.6452 ... Samples: 809\n",
      "Train Epoch: 86 ... Batch: 0 ... Loss: 7460.48193359\n",
      "Train Epoch: 86 ... Batch: 10 ... Loss: 7994.25000000\n",
      "Train Epoch: 86 ... Batch: 20 ... Loss: 7755.83935547\n",
      "Train Epoch: 86 ... Batch: 30 ... Loss: 7721.22216797\n",
      "Train Epoch: 86 ... Batch: 40 ... Loss: 6257.58837891\n",
      "Train Epoch: 86 ... Batch: 50 ... Loss: 8889.73437500\n",
      "Train Epoch: 86 ... Batch: 60 ... Loss: 8644.74121094\n",
      "Train Epoch: 86 ... Batch: 70 ... Loss: 8228.29687500\n",
      "Train Epoch: 86 ... Batch: 80 ... Loss: 8704.06933594\n",
      "Train Epoch: 86 ... Batch: 90 ... Loss: 8643.62792969\n",
      "Train Epoch: 86 ... Batch: 100 ... Loss: 7682.17968750\n",
      "Train Epoch: 86 ... Batch: 110 ... Loss: 7896.58447266\n",
      "Train Epoch: 86 ... Batch: 120 ... Loss: 6528.43115234\n",
      "Train Epoch: 86 ... Batch: 130 ... Loss: 8551.56933594\n",
      "Train Epoch: 86 ... Batch: 140 ... Loss: 9646.41601562\n",
      "Train Epoch: 86 ... Batch: 150 ... Loss: 6913.89697266\n",
      "Train Epoch: 86 ... Batch: 160 ... Loss: 7781.10156250\n",
      "Train Epoch: 86 ... Batch: 170 ... Loss: 8100.39843750\n",
      "Train Epoch: 86 ... Batch: 180 ... Loss: 6875.25927734\n",
      "Train Epoch: 86 ... Batch: 190 ... Loss: 7357.33740234\n",
      "Train Epoch: 86 ... Batch: 200 ... Loss: 8608.87207031\n",
      "------------------- Test set: Average loss: 1218587.8418 ... Samples: 809\n",
      "Train Epoch: 87 ... Batch: 0 ... Loss: 6716.20703125\n",
      "Train Epoch: 87 ... Batch: 10 ... Loss: 7754.37353516\n",
      "Train Epoch: 87 ... Batch: 20 ... Loss: 7339.17968750\n",
      "Train Epoch: 87 ... Batch: 30 ... Loss: 8045.14550781\n",
      "Train Epoch: 87 ... Batch: 40 ... Loss: 8815.79199219\n",
      "Train Epoch: 87 ... Batch: 50 ... Loss: 7269.33056641\n",
      "Train Epoch: 87 ... Batch: 60 ... Loss: 9307.50781250\n",
      "Train Epoch: 87 ... Batch: 70 ... Loss: 7772.92968750\n",
      "Train Epoch: 87 ... Batch: 80 ... Loss: 7739.54833984\n",
      "Train Epoch: 87 ... Batch: 90 ... Loss: 8133.25537109\n",
      "Train Epoch: 87 ... Batch: 100 ... Loss: 8012.22998047\n",
      "Train Epoch: 87 ... Batch: 110 ... Loss: 7189.02343750\n",
      "Train Epoch: 87 ... Batch: 120 ... Loss: 8308.65917969\n",
      "Train Epoch: 87 ... Batch: 130 ... Loss: 8662.53710938\n",
      "Train Epoch: 87 ... Batch: 140 ... Loss: 8439.04980469\n",
      "Train Epoch: 87 ... Batch: 150 ... Loss: 7774.72802734\n",
      "Train Epoch: 87 ... Batch: 160 ... Loss: 8392.35351562\n",
      "Train Epoch: 87 ... Batch: 170 ... Loss: 6537.81396484\n",
      "Train Epoch: 87 ... Batch: 180 ... Loss: 7367.19238281\n",
      "Train Epoch: 87 ... Batch: 190 ... Loss: 9480.88085938\n",
      "Train Epoch: 87 ... Batch: 200 ... Loss: 8300.11621094\n",
      "------------------- Test set: Average loss: 1218588.9666 ... Samples: 809\n",
      "Train Epoch: 88 ... Batch: 0 ... Loss: 6842.56640625\n",
      "Train Epoch: 88 ... Batch: 10 ... Loss: 8097.69287109\n",
      "Train Epoch: 88 ... Batch: 20 ... Loss: 7738.65478516\n",
      "Train Epoch: 88 ... Batch: 30 ... Loss: 8140.32373047\n",
      "Train Epoch: 88 ... Batch: 40 ... Loss: 7157.19140625\n",
      "Train Epoch: 88 ... Batch: 50 ... Loss: 8128.22656250\n",
      "Train Epoch: 88 ... Batch: 60 ... Loss: 7340.42529297\n",
      "Train Epoch: 88 ... Batch: 70 ... Loss: 6774.36083984\n",
      "Train Epoch: 88 ... Batch: 80 ... Loss: 8298.41210938\n",
      "Train Epoch: 88 ... Batch: 90 ... Loss: 8296.65917969\n",
      "Train Epoch: 88 ... Batch: 100 ... Loss: 8621.01855469\n",
      "Train Epoch: 88 ... Batch: 110 ... Loss: 8946.86132812\n",
      "Train Epoch: 88 ... Batch: 120 ... Loss: 8139.51123047\n",
      "Train Epoch: 88 ... Batch: 130 ... Loss: 7711.99218750\n",
      "Train Epoch: 88 ... Batch: 140 ... Loss: 8197.68457031\n",
      "Train Epoch: 88 ... Batch: 150 ... Loss: 8247.94824219\n",
      "Train Epoch: 88 ... Batch: 160 ... Loss: 9482.78613281\n",
      "Train Epoch: 88 ... Batch: 170 ... Loss: 7266.14550781\n",
      "Train Epoch: 88 ... Batch: 180 ... Loss: 7484.04541016\n",
      "Train Epoch: 88 ... Batch: 190 ... Loss: 7809.99755859\n",
      "Train Epoch: 88 ... Batch: 200 ... Loss: 7222.05322266\n",
      "------------------- Test set: Average loss: 1218586.5834 ... Samples: 809\n",
      "Train Epoch: 89 ... Batch: 0 ... Loss: 8568.15039062\n",
      "Train Epoch: 89 ... Batch: 10 ... Loss: 9262.80468750\n",
      "Train Epoch: 89 ... Batch: 20 ... Loss: 8774.09472656\n",
      "Train Epoch: 89 ... Batch: 30 ... Loss: 8495.07031250\n",
      "Train Epoch: 89 ... Batch: 40 ... Loss: 8262.99902344\n",
      "Train Epoch: 89 ... Batch: 50 ... Loss: 8070.69873047\n",
      "Train Epoch: 89 ... Batch: 60 ... Loss: 6617.97998047\n",
      "Train Epoch: 89 ... Batch: 70 ... Loss: 8152.85498047\n",
      "Train Epoch: 89 ... Batch: 80 ... Loss: 7246.31250000\n",
      "Train Epoch: 89 ... Batch: 90 ... Loss: 8892.58886719\n",
      "Train Epoch: 89 ... Batch: 100 ... Loss: 7490.55078125\n",
      "Train Epoch: 89 ... Batch: 110 ... Loss: 7092.90722656\n",
      "Train Epoch: 89 ... Batch: 120 ... Loss: 8537.93945312\n",
      "Train Epoch: 89 ... Batch: 130 ... Loss: 7826.10937500\n",
      "Train Epoch: 89 ... Batch: 140 ... Loss: 8758.37011719\n",
      "Train Epoch: 89 ... Batch: 150 ... Loss: 7836.28125000\n",
      "Train Epoch: 89 ... Batch: 160 ... Loss: 8074.53759766\n",
      "Train Epoch: 89 ... Batch: 170 ... Loss: 7486.58154297\n",
      "Train Epoch: 89 ... Batch: 180 ... Loss: 8183.20947266\n",
      "Train Epoch: 89 ... Batch: 190 ... Loss: 8269.23437500\n",
      "Train Epoch: 89 ... Batch: 200 ... Loss: 8515.79589844\n",
      "------------------- Test set: Average loss: 1218587.0284 ... Samples: 809\n",
      "Train Epoch: 90 ... Batch: 0 ... Loss: 7021.35302734\n",
      "Train Epoch: 90 ... Batch: 10 ... Loss: 9512.56445312\n",
      "Train Epoch: 90 ... Batch: 20 ... Loss: 8170.11669922\n",
      "Train Epoch: 90 ... Batch: 30 ... Loss: 7899.39550781\n",
      "Train Epoch: 90 ... Batch: 40 ... Loss: 8646.14746094\n",
      "Train Epoch: 90 ... Batch: 50 ... Loss: 7832.60009766\n",
      "Train Epoch: 90 ... Batch: 60 ... Loss: 7117.82177734\n",
      "Train Epoch: 90 ... Batch: 70 ... Loss: 7351.72753906\n",
      "Train Epoch: 90 ... Batch: 80 ... Loss: 7522.93457031\n",
      "Train Epoch: 90 ... Batch: 90 ... Loss: 7107.32373047\n",
      "Train Epoch: 90 ... Batch: 100 ... Loss: 8506.54980469\n",
      "Train Epoch: 90 ... Batch: 110 ... Loss: 7635.27880859\n",
      "Train Epoch: 90 ... Batch: 120 ... Loss: 9001.28613281\n",
      "Train Epoch: 90 ... Batch: 130 ... Loss: 9590.28906250\n",
      "Train Epoch: 90 ... Batch: 140 ... Loss: 6640.97802734\n",
      "Train Epoch: 90 ... Batch: 150 ... Loss: 8833.18359375\n",
      "Train Epoch: 90 ... Batch: 160 ... Loss: 7086.18115234\n",
      "Train Epoch: 90 ... Batch: 170 ... Loss: 7360.38623047\n",
      "Train Epoch: 90 ... Batch: 180 ... Loss: 7358.58349609\n",
      "Train Epoch: 90 ... Batch: 190 ... Loss: 7792.83300781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 90 ... Batch: 200 ... Loss: 7913.73925781\n",
      "------------------- Test set: Average loss: 1218586.0643 ... Samples: 809\n",
      "Train Epoch: 91 ... Batch: 0 ... Loss: 8068.35937500\n",
      "Train Epoch: 91 ... Batch: 10 ... Loss: 8065.48535156\n",
      "Train Epoch: 91 ... Batch: 20 ... Loss: 8813.37207031\n",
      "Train Epoch: 91 ... Batch: 30 ... Loss: 8704.62988281\n",
      "Train Epoch: 91 ... Batch: 40 ... Loss: 8416.86425781\n",
      "Train Epoch: 91 ... Batch: 50 ... Loss: 7696.29541016\n",
      "Train Epoch: 91 ... Batch: 60 ... Loss: 8340.49121094\n",
      "Train Epoch: 91 ... Batch: 70 ... Loss: 7786.51562500\n",
      "Train Epoch: 91 ... Batch: 80 ... Loss: 7678.86718750\n",
      "Train Epoch: 91 ... Batch: 90 ... Loss: 6962.84765625\n",
      "Train Epoch: 91 ... Batch: 100 ... Loss: 7562.48583984\n",
      "Train Epoch: 91 ... Batch: 110 ... Loss: 7359.86962891\n",
      "Train Epoch: 91 ... Batch: 120 ... Loss: 8022.27294922\n",
      "Train Epoch: 91 ... Batch: 130 ... Loss: 8844.78417969\n",
      "Train Epoch: 91 ... Batch: 140 ... Loss: 7530.49365234\n",
      "Train Epoch: 91 ... Batch: 150 ... Loss: 8228.26757812\n",
      "Train Epoch: 91 ... Batch: 160 ... Loss: 9103.84960938\n",
      "Train Epoch: 91 ... Batch: 170 ... Loss: 8015.09863281\n",
      "Train Epoch: 91 ... Batch: 180 ... Loss: 9025.94042969\n",
      "Train Epoch: 91 ... Batch: 190 ... Loss: 8415.09179688\n",
      "Train Epoch: 91 ... Batch: 200 ... Loss: 7277.11279297\n",
      "------------------- Test set: Average loss: 1218586.2818 ... Samples: 809\n",
      "Train Epoch: 92 ... Batch: 0 ... Loss: 8687.65527344\n",
      "Train Epoch: 92 ... Batch: 10 ... Loss: 7922.43750000\n",
      "Train Epoch: 92 ... Batch: 20 ... Loss: 8172.45458984\n",
      "Train Epoch: 92 ... Batch: 30 ... Loss: 8032.51416016\n",
      "Train Epoch: 92 ... Batch: 40 ... Loss: 6697.63427734\n",
      "Train Epoch: 92 ... Batch: 50 ... Loss: 9031.26074219\n",
      "Train Epoch: 92 ... Batch: 60 ... Loss: 8973.04394531\n",
      "Train Epoch: 92 ... Batch: 70 ... Loss: 7007.97363281\n",
      "Train Epoch: 92 ... Batch: 80 ... Loss: 7702.30810547\n",
      "Train Epoch: 92 ... Batch: 90 ... Loss: 8824.99902344\n",
      "Train Epoch: 92 ... Batch: 100 ... Loss: 8213.09863281\n",
      "Train Epoch: 92 ... Batch: 110 ... Loss: 7593.68115234\n",
      "Train Epoch: 92 ... Batch: 120 ... Loss: 7853.45800781\n",
      "Train Epoch: 92 ... Batch: 130 ... Loss: 7806.99755859\n",
      "Train Epoch: 92 ... Batch: 140 ... Loss: 8152.48437500\n",
      "Train Epoch: 92 ... Batch: 150 ... Loss: 7600.52197266\n",
      "Train Epoch: 92 ... Batch: 160 ... Loss: 7722.74853516\n",
      "Train Epoch: 92 ... Batch: 170 ... Loss: 7710.02343750\n",
      "Train Epoch: 92 ... Batch: 180 ... Loss: 8060.76708984\n",
      "Train Epoch: 92 ... Batch: 190 ... Loss: 8479.45312500\n",
      "Train Epoch: 92 ... Batch: 200 ... Loss: 8380.51855469\n",
      "------------------- Test set: Average loss: 1218590.8467 ... Samples: 809\n",
      "Train Epoch: 93 ... Batch: 0 ... Loss: 7918.42529297\n",
      "Train Epoch: 93 ... Batch: 10 ... Loss: 6449.04052734\n",
      "Train Epoch: 93 ... Batch: 20 ... Loss: 7097.39697266\n",
      "Train Epoch: 93 ... Batch: 30 ... Loss: 6278.31250000\n",
      "Train Epoch: 93 ... Batch: 40 ... Loss: 7932.13134766\n",
      "Train Epoch: 93 ... Batch: 50 ... Loss: 7529.01562500\n",
      "Train Epoch: 93 ... Batch: 60 ... Loss: 9067.45605469\n",
      "Train Epoch: 93 ... Batch: 70 ... Loss: 7815.09863281\n",
      "Train Epoch: 93 ... Batch: 80 ... Loss: 7567.70166016\n",
      "Train Epoch: 93 ... Batch: 90 ... Loss: 8186.83837891\n",
      "Train Epoch: 93 ... Batch: 100 ... Loss: 7585.10644531\n",
      "Train Epoch: 93 ... Batch: 110 ... Loss: 7402.87500000\n",
      "Train Epoch: 93 ... Batch: 120 ... Loss: 7808.54150391\n",
      "Train Epoch: 93 ... Batch: 130 ... Loss: 7309.17675781\n",
      "Train Epoch: 93 ... Batch: 140 ... Loss: 7790.23437500\n",
      "Train Epoch: 93 ... Batch: 150 ... Loss: 6764.30029297\n",
      "Train Epoch: 93 ... Batch: 160 ... Loss: 9485.39453125\n",
      "Train Epoch: 93 ... Batch: 170 ... Loss: 7264.02050781\n",
      "Train Epoch: 93 ... Batch: 180 ... Loss: 7870.12451172\n",
      "Train Epoch: 93 ... Batch: 190 ... Loss: 7809.91650391\n",
      "Train Epoch: 93 ... Batch: 200 ... Loss: 6994.49707031\n",
      "------------------- Test set: Average loss: 1218587.6279 ... Samples: 809\n",
      "Train Epoch: 94 ... Batch: 0 ... Loss: 8330.30078125\n",
      "Train Epoch: 94 ... Batch: 10 ... Loss: 7718.53466797\n",
      "Train Epoch: 94 ... Batch: 20 ... Loss: 7689.34375000\n",
      "Train Epoch: 94 ... Batch: 30 ... Loss: 8827.44140625\n",
      "Train Epoch: 94 ... Batch: 40 ... Loss: 8219.09667969\n",
      "Train Epoch: 94 ... Batch: 50 ... Loss: 8917.30371094\n",
      "Train Epoch: 94 ... Batch: 60 ... Loss: 8089.83154297\n",
      "Train Epoch: 94 ... Batch: 70 ... Loss: 6800.92041016\n",
      "Train Epoch: 94 ... Batch: 80 ... Loss: 7774.79833984\n",
      "Train Epoch: 94 ... Batch: 90 ... Loss: 7715.31738281\n",
      "Train Epoch: 94 ... Batch: 100 ... Loss: 8467.28417969\n",
      "Train Epoch: 94 ... Batch: 110 ... Loss: 7583.28222656\n",
      "Train Epoch: 94 ... Batch: 120 ... Loss: 7137.12646484\n",
      "Train Epoch: 94 ... Batch: 130 ... Loss: 7614.97656250\n",
      "Train Epoch: 94 ... Batch: 140 ... Loss: 7208.02099609\n",
      "Train Epoch: 94 ... Batch: 150 ... Loss: 7628.59716797\n",
      "Train Epoch: 94 ... Batch: 160 ... Loss: 8223.29003906\n",
      "Train Epoch: 94 ... Batch: 170 ... Loss: 7855.28515625\n",
      "Train Epoch: 94 ... Batch: 180 ... Loss: 7650.58544922\n",
      "Train Epoch: 94 ... Batch: 190 ... Loss: 7258.45458984\n",
      "Train Epoch: 94 ... Batch: 200 ... Loss: 7556.11865234\n",
      "------------------- Test set: Average loss: 1218586.9691 ... Samples: 809\n",
      "Train Epoch: 95 ... Batch: 0 ... Loss: 7488.61425781\n",
      "Train Epoch: 95 ... Batch: 10 ... Loss: 7406.34228516\n",
      "Train Epoch: 95 ... Batch: 20 ... Loss: 8107.69775391\n",
      "Train Epoch: 95 ... Batch: 30 ... Loss: 8271.72265625\n",
      "Train Epoch: 95 ... Batch: 40 ... Loss: 7995.73925781\n",
      "Train Epoch: 95 ... Batch: 50 ... Loss: 7593.25537109\n",
      "Train Epoch: 95 ... Batch: 60 ... Loss: 7443.45410156\n",
      "Train Epoch: 95 ... Batch: 70 ... Loss: 7835.91894531\n",
      "Train Epoch: 95 ... Batch: 80 ... Loss: 7396.08056641\n",
      "Train Epoch: 95 ... Batch: 90 ... Loss: 9331.33300781\n",
      "Train Epoch: 95 ... Batch: 100 ... Loss: 9009.81250000\n",
      "Train Epoch: 95 ... Batch: 110 ... Loss: 8245.35644531\n",
      "Train Epoch: 95 ... Batch: 120 ... Loss: 7385.13232422\n",
      "Train Epoch: 95 ... Batch: 130 ... Loss: 7829.34765625\n",
      "Train Epoch: 95 ... Batch: 140 ... Loss: 8692.33593750\n",
      "Train Epoch: 95 ... Batch: 150 ... Loss: 7979.79003906\n",
      "Train Epoch: 95 ... Batch: 160 ... Loss: 7980.44384766\n",
      "Train Epoch: 95 ... Batch: 170 ... Loss: 7824.96337891\n",
      "Train Epoch: 95 ... Batch: 180 ... Loss: 7902.42529297\n",
      "Train Epoch: 95 ... Batch: 190 ... Loss: 8264.60644531\n",
      "Train Epoch: 95 ... Batch: 200 ... Loss: 8882.24511719\n",
      "------------------- Test set: Average loss: 1218586.6354 ... Samples: 809\n",
      "Train Epoch: 96 ... Batch: 0 ... Loss: 7326.13427734\n",
      "Train Epoch: 96 ... Batch: 10 ... Loss: 7333.19873047\n",
      "Train Epoch: 96 ... Batch: 20 ... Loss: 7112.28271484\n",
      "Train Epoch: 96 ... Batch: 30 ... Loss: 5870.74218750\n",
      "Train Epoch: 96 ... Batch: 40 ... Loss: 8385.45605469\n",
      "Train Epoch: 96 ... Batch: 50 ... Loss: 8825.69042969\n",
      "Train Epoch: 96 ... Batch: 60 ... Loss: 8055.14404297\n",
      "Train Epoch: 96 ... Batch: 70 ... Loss: 8032.03613281\n",
      "Train Epoch: 96 ... Batch: 80 ... Loss: 6729.15966797\n",
      "Train Epoch: 96 ... Batch: 90 ... Loss: 8375.99609375\n",
      "Train Epoch: 96 ... Batch: 100 ... Loss: 8319.04687500\n",
      "Train Epoch: 96 ... Batch: 110 ... Loss: 9481.47753906\n",
      "Train Epoch: 96 ... Batch: 120 ... Loss: 7774.65478516\n",
      "Train Epoch: 96 ... Batch: 130 ... Loss: 9658.10449219\n",
      "Train Epoch: 96 ... Batch: 140 ... Loss: 7871.74560547\n",
      "Train Epoch: 96 ... Batch: 150 ... Loss: 8267.49707031\n",
      "Train Epoch: 96 ... Batch: 160 ... Loss: 8770.67675781\n",
      "Train Epoch: 96 ... Batch: 170 ... Loss: 8131.21093750\n",
      "Train Epoch: 96 ... Batch: 180 ... Loss: 8158.67041016\n",
      "Train Epoch: 96 ... Batch: 190 ... Loss: 7559.51416016\n",
      "Train Epoch: 96 ... Batch: 200 ... Loss: 9746.16113281\n",
      "------------------- Test set: Average loss: 1218586.1904 ... Samples: 809\n",
      "Train Epoch: 97 ... Batch: 0 ... Loss: 9078.32226562\n",
      "Train Epoch: 97 ... Batch: 10 ... Loss: 8061.29638672\n",
      "Train Epoch: 97 ... Batch: 20 ... Loss: 8867.72167969\n",
      "Train Epoch: 97 ... Batch: 30 ... Loss: 7381.31250000\n",
      "Train Epoch: 97 ... Batch: 40 ... Loss: 7966.97412109\n",
      "Train Epoch: 97 ... Batch: 50 ... Loss: 8158.04687500\n",
      "Train Epoch: 97 ... Batch: 60 ... Loss: 7978.18066406\n",
      "Train Epoch: 97 ... Batch: 70 ... Loss: 8243.91894531\n",
      "Train Epoch: 97 ... Batch: 80 ... Loss: 6743.28759766\n",
      "Train Epoch: 97 ... Batch: 90 ... Loss: 8349.63281250\n",
      "Train Epoch: 97 ... Batch: 100 ... Loss: 6891.58447266\n",
      "Train Epoch: 97 ... Batch: 110 ... Loss: 7643.50341797\n",
      "Train Epoch: 97 ... Batch: 120 ... Loss: 8076.66552734\n",
      "Train Epoch: 97 ... Batch: 130 ... Loss: 7051.34472656\n",
      "Train Epoch: 97 ... Batch: 140 ... Loss: 7812.45166016\n",
      "Train Epoch: 97 ... Batch: 150 ... Loss: 7941.88671875\n",
      "Train Epoch: 97 ... Batch: 160 ... Loss: 8723.92968750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 97 ... Batch: 170 ... Loss: 7728.28125000\n",
      "Train Epoch: 97 ... Batch: 180 ... Loss: 8484.27929688\n",
      "Train Epoch: 97 ... Batch: 190 ... Loss: 8276.03417969\n",
      "Train Epoch: 97 ... Batch: 200 ... Loss: 9013.59765625\n",
      "------------------- Test set: Average loss: 1218586.6910 ... Samples: 809\n",
      "Train Epoch: 98 ... Batch: 0 ... Loss: 7658.89990234\n",
      "Train Epoch: 98 ... Batch: 10 ... Loss: 7781.05615234\n",
      "Train Epoch: 98 ... Batch: 20 ... Loss: 6918.75097656\n",
      "Train Epoch: 98 ... Batch: 30 ... Loss: 8286.45117188\n",
      "Train Epoch: 98 ... Batch: 40 ... Loss: 7934.43847656\n",
      "Train Epoch: 98 ... Batch: 50 ... Loss: 8167.10937500\n",
      "Train Epoch: 98 ... Batch: 60 ... Loss: 8233.76953125\n",
      "Train Epoch: 98 ... Batch: 70 ... Loss: 8827.56054688\n",
      "Train Epoch: 98 ... Batch: 80 ... Loss: 8188.55175781\n",
      "Train Epoch: 98 ... Batch: 90 ... Loss: 7265.00341797\n",
      "Train Epoch: 98 ... Batch: 100 ... Loss: 8671.03125000\n",
      "Train Epoch: 98 ... Batch: 110 ... Loss: 6732.02490234\n",
      "Train Epoch: 98 ... Batch: 120 ... Loss: 7397.98291016\n",
      "Train Epoch: 98 ... Batch: 130 ... Loss: 7819.30615234\n",
      "Train Epoch: 98 ... Batch: 140 ... Loss: 8366.73144531\n",
      "Train Epoch: 98 ... Batch: 150 ... Loss: 8559.22558594\n",
      "Train Epoch: 98 ... Batch: 160 ... Loss: 7500.01025391\n",
      "Train Epoch: 98 ... Batch: 170 ... Loss: 7855.71044922\n",
      "Train Epoch: 98 ... Batch: 180 ... Loss: 6697.60546875\n",
      "Train Epoch: 98 ... Batch: 190 ... Loss: 8281.52246094\n",
      "Train Epoch: 98 ... Batch: 200 ... Loss: 8093.73535156\n",
      "------------------- Test set: Average loss: 1218587.7058 ... Samples: 809\n",
      "Train Epoch: 99 ... Batch: 0 ... Loss: 9113.02246094\n",
      "Train Epoch: 99 ... Batch: 10 ... Loss: 7868.73437500\n",
      "Train Epoch: 99 ... Batch: 20 ... Loss: 7027.73779297\n",
      "Train Epoch: 99 ... Batch: 30 ... Loss: 7545.93603516\n",
      "Train Epoch: 99 ... Batch: 40 ... Loss: 8535.34863281\n",
      "Train Epoch: 99 ... Batch: 50 ... Loss: 9193.60351562\n",
      "Train Epoch: 99 ... Batch: 60 ... Loss: 7837.22265625\n",
      "Train Epoch: 99 ... Batch: 70 ... Loss: 6766.97363281\n",
      "Train Epoch: 99 ... Batch: 80 ... Loss: 7601.71582031\n",
      "Train Epoch: 99 ... Batch: 90 ... Loss: 8139.13525391\n",
      "Train Epoch: 99 ... Batch: 100 ... Loss: 7282.04541016\n",
      "Train Epoch: 99 ... Batch: 110 ... Loss: 7204.19677734\n",
      "Train Epoch: 99 ... Batch: 120 ... Loss: 8585.40625000\n",
      "Train Epoch: 99 ... Batch: 130 ... Loss: 8923.36035156\n",
      "Train Epoch: 99 ... Batch: 140 ... Loss: 7442.23925781\n",
      "Train Epoch: 99 ... Batch: 150 ... Loss: 7902.03466797\n",
      "Train Epoch: 99 ... Batch: 160 ... Loss: 7750.08984375\n",
      "Train Epoch: 99 ... Batch: 170 ... Loss: 9501.34082031\n",
      "Train Epoch: 99 ... Batch: 180 ... Loss: 8883.01269531\n",
      "Train Epoch: 99 ... Batch: 190 ... Loss: 7750.66552734\n",
      "Train Epoch: 99 ... Batch: 200 ... Loss: 8292.51757812\n",
      "------------------- Test set: Average loss: 1218586.6823 ... Samples: 809\n",
      "Train Epoch: 100 ... Batch: 0 ... Loss: 6869.88769531\n",
      "Train Epoch: 100 ... Batch: 10 ... Loss: 7161.27880859\n",
      "Train Epoch: 100 ... Batch: 20 ... Loss: 8103.39453125\n",
      "Train Epoch: 100 ... Batch: 30 ... Loss: 7280.85302734\n",
      "Train Epoch: 100 ... Batch: 40 ... Loss: 8212.68261719\n",
      "Train Epoch: 100 ... Batch: 50 ... Loss: 8346.69140625\n",
      "Train Epoch: 100 ... Batch: 60 ... Loss: 7807.14599609\n",
      "Train Epoch: 100 ... Batch: 70 ... Loss: 8083.80615234\n",
      "Train Epoch: 100 ... Batch: 80 ... Loss: 6883.88916016\n",
      "Train Epoch: 100 ... Batch: 90 ... Loss: 7743.79931641\n",
      "Train Epoch: 100 ... Batch: 100 ... Loss: 8991.89648438\n",
      "Train Epoch: 100 ... Batch: 110 ... Loss: 8194.34667969\n",
      "Train Epoch: 100 ... Batch: 120 ... Loss: 7438.48828125\n",
      "Train Epoch: 100 ... Batch: 130 ... Loss: 8616.85156250\n",
      "Train Epoch: 100 ... Batch: 140 ... Loss: 7790.85791016\n",
      "Train Epoch: 100 ... Batch: 150 ... Loss: 7598.05175781\n",
      "Train Epoch: 100 ... Batch: 160 ... Loss: 8395.74023438\n",
      "Train Epoch: 100 ... Batch: 170 ... Loss: 7246.20312500\n",
      "Train Epoch: 100 ... Batch: 180 ... Loss: 8227.09277344\n",
      "Train Epoch: 100 ... Batch: 190 ... Loss: 8685.95703125\n",
      "Train Epoch: 100 ... Batch: 200 ... Loss: 7695.76904297\n",
      "------------------- Test set: Average loss: 1218586.8220 ... Samples: 809\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "545ce9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_input[0][:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a91b5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## torch.flip(data_input[0], dims=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f687825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_input[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5fb8f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    data = data_input[0:1].to(device)\n",
    "    output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1cf95f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.9983e-01,  9.9991e-01,  9.9991e-01, -9.9991e-01,  9.9990e-01,\n",
       "           9.9984e-01,  9.9988e-01, -9.9991e-01,  9.9988e-01,  9.9990e-01,\n",
       "          -9.9991e-01, -9.9991e-01, -9.9991e-01,  9.9985e-01,  9.9991e-01,\n",
       "          -9.9984e-01, -9.9991e-01,  9.9991e-01,  9.9991e-01,  9.9985e-01,\n",
       "          -9.9991e-01,  9.9991e-01, -9.9982e-01,  9.9991e-01,  9.9991e-01,\n",
       "          -9.9991e-01, -9.9991e-01, -9.9983e-01,  9.9991e-01,  4.1983e-13,\n",
       "          -9.9991e-01, -9.9991e-01],\n",
       "         [-9.9877e-01,  9.9933e-01,  9.9933e-01, -9.9933e-01,  9.9928e-01,\n",
       "           9.9894e-01,  9.9915e-01, -9.9933e-01,  9.9914e-01,  9.9927e-01,\n",
       "          -9.9931e-01, -9.9933e-01, -9.9933e-01,  9.9888e-01,  9.9933e-01,\n",
       "          -9.9880e-01, -9.9933e-01,  9.9933e-01,  9.9933e-01,  9.9896e-01,\n",
       "          -9.9933e-01,  9.9932e-01, -9.9868e-01,  9.9933e-01,  9.9933e-01,\n",
       "          -9.9933e-01, -9.9931e-01, -9.9880e-01,  9.9931e-01,  4.7505e-13,\n",
       "          -9.9932e-01, -9.9933e-01],\n",
       "         [-9.9106e-01,  9.9505e-01,  9.9505e-01, -9.9505e-01,  9.9478e-01,\n",
       "           9.9281e-01,  9.9392e-01, -9.9505e-01,  9.9370e-01,  9.9472e-01,\n",
       "          -9.9495e-01, -9.9505e-01, -9.9505e-01,  9.9178e-01,  9.9505e-01,\n",
       "          -9.9121e-01, -9.9505e-01,  9.9505e-01,  9.9505e-01,  9.9295e-01,\n",
       "          -9.9505e-01,  9.9496e-01, -9.9037e-01,  9.9505e-01,  9.9505e-01,\n",
       "          -9.9505e-01, -9.9495e-01, -9.9132e-01,  9.9494e-01,  1.1291e-12,\n",
       "          -9.9496e-01, -9.9505e-01],\n",
       "         [-9.3711e-01,  9.6403e-01,  9.6403e-01, -9.6403e-01,  9.6262e-01,\n",
       "           9.5058e-01,  9.5718e-01, -9.6403e-01,  9.5436e-01,  9.6231e-01,\n",
       "          -9.6325e-01, -9.6402e-01, -9.6403e-01,  9.4122e-01,  9.6399e-01,\n",
       "          -9.3741e-01, -9.6403e-01,  9.6403e-01,  9.6400e-01,  9.5290e-01,\n",
       "          -9.6400e-01,  9.6332e-01, -9.3143e-01,  9.6399e-01,  9.6403e-01,\n",
       "          -9.6403e-01, -9.6357e-01, -9.3873e-01,  9.6324e-01,  3.0303e-10,\n",
       "          -9.6337e-01, -9.6402e-01],\n",
       "         [-6.4270e-01,  7.6159e-01,  7.6159e-01, -7.6159e-01,  7.5662e-01,\n",
       "           6.9631e-01,  7.2901e-01, -7.6159e-01,  7.0569e-01,  7.5510e-01,\n",
       "          -7.5698e-01, -7.6153e-01, -7.6159e-01,  6.4732e-01,  7.6139e-01,\n",
       "          -6.3195e-01, -7.6159e-01,  7.6159e-01,  7.6145e-01,  7.1823e-01,\n",
       "          -7.6133e-01,  7.5741e-01, -6.0011e-01,  7.6139e-01,  7.6159e-01,\n",
       "          -7.6159e-01, -7.5994e-01, -6.3253e-01,  7.5690e-01,  9.9596e-04,\n",
       "          -7.5772e-01, -7.6156e-01]]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c1b5c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -45.8218,   57.4789,   48.6242,  -82.7458,   90.2192,    2.5085,\n",
       "            18.7802, -135.9315,   29.0594,   66.9606,  -36.3427,  -45.6037,\n",
       "           -26.2268,   59.3037,  114.4326,  -42.4477, -107.8410,   74.5253,\n",
       "            41.5111,   25.1215,    7.0498,   36.8380,   16.7537,   67.9338,\n",
       "            82.8892, -139.4555,  -37.5287,   12.5863,    3.8364,  -35.1793,\n",
       "           -78.3844,  -91.5303],\n",
       "         [ -44.9507,   63.4614,   35.0839,  -89.5835,   98.5496,   10.8374,\n",
       "            24.1075, -142.6458,   29.8852,   66.4066,  -49.6138,  -44.9583,\n",
       "           -20.1390,   55.7010,  121.8717,  -43.1314, -112.7467,   72.6779,\n",
       "            43.1624,   24.1007,   -0.3647,   50.5636,    3.0370,   77.0706,\n",
       "            86.3826, -137.6309,  -33.2479,    6.2407,    8.1310,  -41.9642,\n",
       "           -78.9873,  -92.0102],\n",
       "         [ -44.7154,   67.8981,   23.0046,  -95.5193,  106.9143,   17.9277,\n",
       "            28.2278, -147.4190,   29.9477,   65.4966,  -60.8076,  -44.1688,\n",
       "           -13.7817,   51.8209,  128.2737,  -44.1821, -115.7059,   69.0041,\n",
       "            44.6621,   22.8283,   -6.6413,   63.2664,   -7.7435,   84.3840,\n",
       "            89.3393, -134.9127,  -29.0668,    0.5037,   10.5727,  -47.7640,\n",
       "           -79.4777,  -92.8328],\n",
       "         [ -43.7503,   70.3785,   17.2225,  -99.0394,  110.8966,   21.8248,\n",
       "            30.2069, -149.6685,   29.4585,   64.3995,  -66.4518,  -43.5891,\n",
       "           -10.6423,   48.9018,  130.8311,  -44.4366, -116.3673,   67.0173,\n",
       "            45.6187,   21.8431,   -9.9524,   69.9450,  -12.7828,   88.1587,\n",
       "            91.3532, -133.2392,  -26.9647,   -2.7604,   11.8986,  -49.8120,\n",
       "           -79.1265,  -92.8279],\n",
       "         [ -41.8207,   72.7958,   12.5172, -102.9736,  114.1274,   25.5242,\n",
       "            31.8028, -151.2463,   28.0864,   61.8309,  -71.4926,  -43.1019,\n",
       "            -7.4713,   44.8370,  132.2467,  -44.9262, -115.4060,   64.6330,\n",
       "            47.2207,   20.4984,  -12.7613,   76.7467,  -15.9062,   91.0650,\n",
       "            94.3694, -130.9608,  -24.5994,   -6.4535,   13.1130,  -49.6811,\n",
       "           -77.9692,  -92.5359]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839a506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
